--- patches/v32_base/suite.py	2026-01-10 16:41:55
+++ atos_core/suite.py	2026-01-10 16:44:53
@@ -3,7 +3,7 @@
 import json
 import re
 from collections import Counter
-from typing import Any, Dict, Iterable, List, Sequence, Tuple
+from typing import Any, Callable, Dict, Iterable, List, Optional, Sequence, Tuple
 
 from .metrics import distinct_n, loop_rate, repeat_ngram_rate, tokenize_text
 
@@ -43,6 +43,305 @@
 _WS_RE = re.compile(r"\s+", flags=re.UNICODE)
 _LAST_USER_RE = re.compile(r"(?:^|\n)User:\s*(.*?)\nSystem:", flags=re.UNICODE | re.DOTALL)
 _ALL_USERS_RE = re.compile(r"(?:^|\n)User:\s*([^\n]*)\nSystem:", flags=re.UNICODE)
+
+
+# Utility / skill suite (deterministic validators, no world knowledge).
+# Each task is validated on a single chosen turn (default: last turn).
+SKILL_DIALOGUES_V0: Tuple[Dict[str, Any], ...] = (
+    # Instruction following (exact output).
+    {
+        "task_id": "instr_ok",
+        "dialogue": ("Responda exatamente: OK",),
+        "validator_id": "exact_match",
+        "expected_spec": {"text": "OK", "collapse_ws": True, "case_sensitive": True},
+        "tags": ["instruction"],
+    },
+    {
+        "task_id": "instr_exact_code_1",
+        "dialogue": ("Retorne exatamente a string: PATO-123",),
+        "validator_id": "exact_match",
+        "expected_spec": {"text": "PATO-123", "collapse_ws": True, "case_sensitive": True},
+        "tags": ["instruction"],
+    },
+    {
+        "task_id": "instr_exact_code_2",
+        "dialogue": ("Retorne exatamente a string: X_Y-Z",),
+        "validator_id": "exact_match",
+        "expected_spec": {"text": "X_Y-Z", "collapse_ws": True, "case_sensitive": True},
+        "tags": ["instruction"],
+    },
+    # Format / JSON (strict, parseable, minimal schema).
+    {
+        "task_id": "json_ab_1",
+        "dialogue": (
+            'Responda com APENAS um JSON válido (sem texto extra) com chaves "a" e "b". '
+            '"a" deve ser a string "x" e "b" deve ser o número 7.',
+        ),
+        "validator_id": "json_parse_keys",
+        "expected_spec": {
+            "required_keys": ["a", "b"],
+            "types": {"a": "str", "b": "int"},
+            "expected_values": {"a": "x", "b": 7},
+        },
+        "tags": ["format", "json", "instruction"],
+    },
+    {
+        "task_id": "json_bool_1",
+        "dialogue": (
+            'Responda com APENAS um JSON válido (sem texto extra) com chaves "ok" e "n". '
+            '"ok" deve ser true e "n" deve ser 3.',
+        ),
+        "validator_id": "json_parse_keys",
+        "expected_spec": {
+            "required_keys": ["ok", "n"],
+            "types": {"ok": "bool", "n": "int"},
+            "expected_values": {"ok": True, "n": 3},
+        },
+        "tags": ["format", "json", "instruction"],
+    },
+    # Arithmetic (no world knowledge; strict integer output only).
+    {
+        "task_id": "math_add_17_25",
+        "dialogue": ("17 + 25 = ? (responda APENAS com o número inteiro)",),
+        "validator_id": "int_exact",
+        "expected_spec": {"value": 42, "strict": True},
+        "tags": ["math", "format", "instruction"],
+    },
+    {
+        "task_id": "math_mul_9_7",
+        "dialogue": ("9 * 7 = ? (responda APENAS com o número inteiro)",),
+        "validator_id": "int_exact",
+        "expected_spec": {"value": 63, "strict": True},
+        "tags": ["math", "format", "instruction"],
+    },
+    {
+        "task_id": "math_sub_100_58",
+        "dialogue": ("100 - 58 = ? (responda APENAS com o número inteiro)",),
+        "validator_id": "int_exact",
+        "expected_spec": {"value": 42, "strict": True},
+        "tags": ["math", "format", "instruction"],
+    },
+    # Context factual (fact provided in-context; deterministic target).
+    {
+        "task_id": "ctx_password",
+        "dialogue": (
+            "Contexto: a senha é PINEAPPLE. Apenas memorize.",
+            "Qual é a senha? Responda exatamente com a senha.",
+        ),
+        "validate_turn": 1,
+        "validator_id": "exact_match",
+        "expected_spec": {"text": "PINEAPPLE", "collapse_ws": True, "case_sensitive": True},
+        "tags": ["factual", "state", "instruction"],
+    },
+    {
+        "task_id": "ctx_year",
+        "dialogue": (
+            "Contexto: Joana nasceu em 1980. Apenas memorize.",
+            "Em que ano Joana nasceu? Responda APENAS com o número inteiro.",
+        ),
+        "validate_turn": 1,
+        "validator_id": "int_exact",
+        "expected_spec": {"value": 1980, "strict": True},
+        "tags": ["factual", "state", "math", "instruction"],
+    },
+    # State multi-turn (must carry state across turns).
+    {
+        "task_id": "state_code_alfa9",
+        "dialogue": (
+            "Memorize a palavra-código: ALFA9.",
+            "Repita a palavra-código exatamente.",
+        ),
+        "validate_turn": 1,
+        "validator_id": "exact_match",
+        "expected_spec": {"text": "ALFA9", "collapse_ws": True, "case_sensitive": True},
+        "tags": ["state", "instruction"],
+    },
+    {
+        "task_id": "state_code_beta7",
+        "dialogue": (
+            "Memorize a palavra-código: BETA7.",
+            "Repita a palavra-código exatamente.",
+        ),
+        "validate_turn": 1,
+        "validator_id": "exact_match",
+        "expected_spec": {"text": "BETA7", "collapse_ws": True, "case_sensitive": True},
+        "tags": ["state", "instruction"],
+    },
+    # Contains token (allows extra text but must include exact token).
+    {
+        "task_id": "contains_token_foobar",
+        "dialogue": (
+            "A palavra-chave é FOOBAR. Responda em uma frase curta, mas inclua FOOBAR.",
+        ),
+        "validator_id": "contains_exact_token",
+        "expected_spec": {"token": "FOOBAR", "case_sensitive": True},
+        "tags": ["state", "instruction"],
+    },
+)
+
+
+def _normalize_output(text: str, *, collapse_ws: bool) -> str:
+    s = str(text or "").strip()
+    if collapse_ws:
+        s = _WS_RE.sub(" ", s)
+    return s
+
+
+def _short_snip(text: str, *, limit: int = 160) -> str:
+    s = _normalize_output(text, collapse_ws=True)
+    if len(s) <= int(limit):
+        return s
+    return s[: int(limit) - 1] + "…"
+
+
+def _validate_exact_match(output: str, spec: Dict[str, Any]) -> Tuple[bool, str]:
+    expected = str(spec.get("text", ""))
+    collapse_ws = bool(spec.get("collapse_ws", True))
+    case_sensitive = bool(spec.get("case_sensitive", True))
+    out_n = _normalize_output(output, collapse_ws=collapse_ws)
+    exp_n = _normalize_output(expected, collapse_ws=collapse_ws)
+    if not case_sensitive:
+        out_n = out_n.lower()
+        exp_n = exp_n.lower()
+    ok = bool(out_n == exp_n)
+    return ok, "" if ok else f"exact_mismatch got={_short_snip(out_n)} expected={_short_snip(exp_n)}"
+
+
+_INT_FULL_RE = re.compile(r"[-+]?\d+", flags=re.UNICODE)
+
+
+def _validate_int_exact(output: str, spec: Dict[str, Any]) -> Tuple[bool, str]:
+    expected = int(spec.get("value", 0) or 0)
+    strict = bool(spec.get("strict", True))
+    s = _normalize_output(output, collapse_ws=True)
+    if strict:
+        if re.fullmatch(_INT_FULL_RE, s) is None:
+            return False, "int_not_strict"
+        got = int(s)
+    else:
+        m = _INT_FULL_RE.search(s)
+        if not m:
+            return False, "int_missing"
+        got = int(m.group(0))
+    ok = bool(got == expected)
+    return ok, "" if ok else f"int_mismatch got={got} expected={expected}"
+
+
+def _validate_json_parse_keys(output: str, spec: Dict[str, Any]) -> Tuple[bool, str]:
+    s = str(output or "").strip()
+    required_keys = spec.get("required_keys") or []
+    types = spec.get("types") or {}
+    expected_values = spec.get("expected_values") or {}
+    if not isinstance(required_keys, list) or not isinstance(types, dict) or not isinstance(expected_values, dict):
+        return False, "invalid_spec"
+    try:
+        obj = json.loads(s)
+    except Exception:
+        return False, "json_parse_error"
+    if not isinstance(obj, dict):
+        return False, "json_not_object"
+    for k in required_keys:
+        if not isinstance(k, str):
+            continue
+        if k not in obj:
+            return False, f"json_missing_key:{k}"
+    for k, tname in types.items():
+        if not isinstance(k, str) or not isinstance(tname, str):
+            continue
+        v = obj.get(k)
+        if tname == "str":
+            if not isinstance(v, str):
+                return False, f"json_type_mismatch:{k}:str"
+        elif tname == "int":
+            if not isinstance(v, int) or isinstance(v, bool):
+                return False, f"json_type_mismatch:{k}:int"
+        elif tname == "bool":
+            if not isinstance(v, bool):
+                return False, f"json_type_mismatch:{k}:bool"
+        else:
+            return False, f"json_unknown_type:{k}:{tname}"
+    for k, exp in expected_values.items():
+        if not isinstance(k, str):
+            continue
+        if obj.get(k) != exp:
+            return False, f"json_value_mismatch:{k}"
+    return True, ""
+
+
+def _validate_regex_fullmatch(output: str, spec: Dict[str, Any]) -> Tuple[bool, str]:
+    pattern = str(spec.get("pattern") or "")
+    flags = int(spec.get("flags", 0) or 0)
+    s = str(output or "").strip()
+    try:
+        ok = re.fullmatch(pattern, s, flags=flags) is not None
+    except re.error:
+        return False, "regex_error"
+    return ok, "" if ok else "regex_no_match"
+
+
+def _validate_contains_exact_token(output: str, spec: Dict[str, Any]) -> Tuple[bool, str]:
+    token = str(spec.get("token") or "")
+    case_sensitive = bool(spec.get("case_sensitive", True))
+    s = str(output or "")
+    if not case_sensitive:
+        s = s.lower()
+        token = token.lower()
+    ok = bool(token and (token in s))
+    return ok, "" if ok else "token_missing"
+
+
+_VALIDATORS: Dict[str, Callable[[str, Dict[str, Any]], Tuple[bool, str]]] = {
+    "exact_match": _validate_exact_match,
+    "int_exact": _validate_int_exact,
+    "json_parse_keys": _validate_json_parse_keys,
+    "regex_fullmatch": _validate_regex_fullmatch,
+    "contains_exact_token": _validate_contains_exact_token,
+}
+
+
+def _plan_trace_for_task(task: Dict[str, Any], *, turn_idx: int) -> Dict[str, Any]:
+    task_id = str(task.get("task_id") or "")
+    dialogue = task.get("dialogue") or ()
+    validate_turn = int(task.get("validate_turn", max(0, len(dialogue) - 1)) or 0)
+    validator_id = str(task.get("validator_id") or "") if int(turn_idx) == int(validate_turn) else ""
+    expected_spec = task.get("expected_spec") or {}
+
+    expected_format = ""
+    constraints: List[str] = []
+    if validator_id == "exact_match":
+        expected_format = "exact"
+        constraints = ["exact_match", "no_extra_tokens"]
+    elif validator_id == "int_exact":
+        expected_format = "int"
+        strict = bool(expected_spec.get("strict", True)) if isinstance(expected_spec, dict) else True
+        constraints = ["int_exact", "digits_only" if strict else "int_extract"]
+    elif validator_id == "json_parse_keys":
+        expected_format = "json"
+        keys = []
+        if isinstance(expected_spec, dict):
+            rk = expected_spec.get("required_keys") or []
+            if isinstance(rk, list):
+                keys = [str(k) for k in rk if isinstance(k, str)]
+        if keys:
+            constraints = ["json", "keys:" + ",".join(keys)]
+        else:
+            constraints = ["json"]
+    elif validator_id == "regex_fullmatch":
+        expected_format = "regex"
+        constraints = ["regex_fullmatch"]
+    elif validator_id == "contains_exact_token":
+        expected_format = "contains"
+        tok = ""
+        if isinstance(expected_spec, dict):
+            tok = str(expected_spec.get("token") or "")
+        constraints = ["contains_exact_token"] + ([f"token:{tok}"] if tok else [])
+
+    return {
+        "task_id": task_id,
+        "validator_id": validator_id,
+        "expected_format": expected_format,
+        "constraints": constraints,
+    }
 
 
 def reply_signature(text: str) -> str:
@@ -403,6 +702,131 @@
         metrics["trace_active_set_size"] = int(trace_active_set_size)
         metrics["trace_rewrite_rules_total"] = int(trace_rewrite_rules_total)
         metrics["trace_selector_present"] = int(trace_selector_present)
+    return transcripts, metrics
+
+
+def run_skill_suite(
+    engine,
+    *,
+    tasks: Sequence[Dict[str, Any]] = SKILL_DIALOGUES_V0,
+    max_new_tokens: int = 200,
+) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
+    transcripts: List[Dict[str, Any]] = []
+    failures: List[Dict[str, Any]] = []
+
+    total_tasks = 0
+    pass_count = 0
+
+    cat_total: Counter = Counter()
+    cat_pass: Counter = Counter()
+
+    plan_turns_total = 0
+    plan_turns_missing = 0
+
+    for i, task in enumerate(tasks):
+        if not isinstance(task, dict):
+            continue
+        task_id = str(task.get("task_id") or f"task_{i}")
+        turns = task.get("dialogue") or ()
+        if not isinstance(turns, (list, tuple)) or not turns:
+            continue
+        validate_turn = int(task.get("validate_turn", max(0, len(turns) - 1)) or 0)
+        validate_turn = max(0, min(validate_turn, len(turns) - 1))
+        validator_id = str(task.get("validator_id") or "")
+        expected_spec = task.get("expected_spec") or {}
+        tags = task.get("tags") or []
+        if not isinstance(tags, list):
+            tags = []
+
+        total_tasks += 1
+        for cat in ("instruction", "json", "math", "state"):
+            if cat in tags:
+                cat_total[cat] += 1
+
+        history: List[Dict[str, Any]] = []
+        for j, user_msg in enumerate(turns):
+            history.append({"user": str(user_msg), "system": ""})
+            prompt = build_chat_prompt(history)
+            out = engine.generate(
+                prompt=prompt,
+                max_new_tokens=max_new_tokens,
+                mode="greedy",
+                dialogue_id=int(i),
+                turn=int(j),
+            )
+            resp = out["text"][len(prompt) :]
+            history[-1]["system"] = resp
+            history[-1]["mode"] = str(out.get("mode") or "default")
+            history[-1]["mode_source"] = str(out.get("mode_source") or "router")
+            history[-1]["mode_policy_action"] = str(out.get("mode_policy_action") or "")
+            history[-1]["policy_coverage"] = float(out.get("policy_coverage") or 0.0)
+            history[-1]["user_sig"] = str(out.get("user_sig") or "")
+
+            tr = dict(out.get("trace") or {})
+            plan_trace = _plan_trace_for_task(task, turn_idx=int(j))
+            tr["plan_trace"] = plan_trace
+            history[-1]["trace"] = tr
+
+            plan_turns_total += 1
+            if "plan_trace" not in tr:
+                plan_turns_missing += 1
+
+        full_text = build_chat_prompt(history)
+        transcripts.append(
+            {"prompt_id": i, "task_id": task_id, "turns": history, "full_text": full_text}
+        )
+
+        # Validate a single, deterministic turn per task.
+        turn_rec = history[validate_turn] if validate_turn < len(history) else history[-1]
+        out_text = str(turn_rec.get("system", ""))
+        fn = _VALIDATORS.get(validator_id)
+        if fn is None or not isinstance(expected_spec, dict):
+            ok = False
+            reason = "unknown_validator_or_spec"
+        else:
+            ok, reason = fn(out_text, expected_spec)
+
+        if ok:
+            pass_count += 1
+            for cat in ("instruction", "json", "math", "state"):
+                if cat in tags:
+                    cat_pass[cat] += 1
+        else:
+            if len(failures) < 5:
+                try:
+                    exp_short = _short_snip(
+                        json.dumps(expected_spec, ensure_ascii=False, sort_keys=True)
+                    )
+                except Exception:
+                    exp_short = _short_snip(str(expected_spec))
+                failures.append(
+                    {
+                        "task_id": task_id,
+                        "turn": int(validate_turn),
+                        "validator_id": validator_id,
+                        "expected_spec": exp_short,
+                        "output_snippet": _short_snip(out_text),
+                        "reason": str(reason or ""),
+                    }
+                )
+
+    def _rate(ok: int, total: int) -> float:
+        return float(ok / total) if int(total) > 0 else 0.0
+
+    metrics: Dict[str, Any] = {
+        "total_tasks": int(total_tasks),
+        "pass_count": int(pass_count),
+        "pass_rate": _rate(pass_count, total_tasks),
+        "instruction_pass_rate": _rate(
+            int(cat_pass.get("instruction", 0)), int(cat_total.get("instruction", 0))
+        ),
+        "json_pass_rate": _rate(int(cat_pass.get("json", 0)), int(cat_total.get("json", 0))),
+        "math_pass_rate": _rate(int(cat_pass.get("math", 0)), int(cat_total.get("math", 0))),
+        "state_pass_rate": _rate(int(cat_pass.get("state", 0)), int(cat_total.get("state", 0))),
+        "failures": list(failures),
+        "plan_trace_turns_total": int(plan_turns_total),
+        "plan_trace_missing_turns": int(plan_turns_missing),
+    }
     return transcripts, metrics
 
 
--- patches/v32_base/learn.py	2026-01-10 16:41:55
+++ atos_core/learn.py	2026-01-10 16:49:01
@@ -34,6 +34,7 @@
     prefix_k_signature,
     reply_signature,
     run_chat_suite,
+    run_skill_suite,
     user_signature,
 )
 from .store import ActStore
@@ -745,6 +746,8 @@
     suite_prefix_k: int = 8
     suite_template_ngram_n: int = 6
     suite_template_prefix_window: int = 32
+    # Utility suite weight in patch score (shadow default: 0.0). Set >0 to optimize pass_rate.
+    utility_weight: float = 0.0
 
 
 class KAAbsoluteTrainer:
@@ -874,6 +877,13 @@
             template_prefix_window=self.config.suite_template_prefix_window,
         )
         return metrics
+
+    def _eval_skill_suite_metrics(self, engine: Engine) -> Dict[str, Any]:
+        _transcripts, metrics = run_skill_suite(
+            engine,
+            max_new_tokens=self.config.fluency_gen_tokens,
+        )
+        return dict(metrics)
 
     def _get_mode_policy_act(self) -> Optional[Act]:
         acts = self.store.by_kind("mode_policy")
@@ -2152,7 +2162,10 @@
             ctx = ctx[-(engine.config.max_order - 1) :]
 
         cost_bits = sum(estimate_act_cost_bits(a) for a in store_copy.active())
-        gen = self._eval_chat_harness_metrics(engine)
+        gen: Dict[str, Any] = dict(self._eval_chat_harness_metrics(engine))
+        util = self._eval_skill_suite_metrics(engine)
+        for k, v in util.items():
+            gen[f"utility_{k}"] = v
         return {"nll_bits": nll_bits, "cost_bits": cost_bits, "gen": gen}
 
     def _apply_patch(self, patch: Patch, *, count: bool) -> Dict[str, Any]:
@@ -2616,9 +2629,21 @@
                 + float(gen.get("cross_turn_signature_repeat_rate", 0.0))
             )
 
+        def utility_penalty(gen: Dict[str, Any]) -> float:
+            try:
+                pass_rate = float(gen.get("utility_pass_rate", 0.0) or 0.0)
+            except Exception:
+                pass_rate = 0.0
+            if pass_rate != pass_rate:
+                pass_rate = 0.0
+            pass_rate = max(0.0, min(1.0, pass_rate))
+            return 1.0 - pass_rate
+
         base_pen = fluency_penalty(base_gen)
+        base_util_pen = utility_penalty(base_gen)
         lam = float(self.config.fluency_lambda)
-        base_score = -lam * base_pen
+        util_w = float(self.config.utility_weight)
+        base_score = (-lam * base_pen) - (util_w * base_util_pen)
 
         for patch in patches:
             cand_eval = self._eval_online_window(
@@ -2642,7 +2667,8 @@
             gain = data_gain_bits - cost_delta_bits
 
             cand_pen = fluency_penalty(cand_gen)
-            score = gain - lam * cand_pen
+            cand_util_pen = utility_penalty(cand_gen)
+            score = gain - lam * cand_pen - (util_w * cand_util_pen)
 
             # Accept criteria (v0.2.1):
             # - Non-merge: must improve the *relative* objective vs. baseline slice
@@ -2676,8 +2702,11 @@
                         "base_score": base_score,
                         "score_improvement": score - base_score,
                         "fluency_lambda": lam,
+                        "utility_weight": util_w,
                         "base_penalty_sum": base_pen,
                         "cand_penalty_sum": cand_pen,
+                        "base_utility_penalty": base_util_pen,
+                        "cand_utility_penalty": cand_util_pen,
                         "horizon_tokens": horizon,
                         "base": {"nll_bits": base_nll, "cost_bits": base_cost, **base_gen},
                         "cand": {"nll_bits": cand_nll, "cost_bits": cand_cost, **cand_gen},
@@ -2795,6 +2824,16 @@
                     }
                     gen = post_gen
 
+                _skill_transcripts, util_metrics = run_skill_suite(
+                    engine,
+                    max_new_tokens=self.config.fluency_gen_tokens,
+                )
+                util_log = dict(util_metrics)
+                if step != int(self.config.steps):
+                    util_log.pop("failures", None)
+                for k, v in util_log.items():
+                    gen[f"utility_{k}"] = v
+
                 memory_meta = self._update_fact_memory_from_transcripts(transcripts, step=step)
                 if memory_meta and memory_meta.get("enabled"):
                     if patch_meta is None:
@@ -2876,6 +2915,17 @@
                     "policy_explore_rate": gen.get("policy_explore_rate"),
                     "policy_exploit_rate": gen.get("policy_exploit_rate"),
                     "policy_coverage_mean": gen.get("policy_coverage_mean"),
+                    "utility_weight": float(self.config.utility_weight),
+                    "utility_total_tasks": gen.get("utility_total_tasks"),
+                    "utility_pass_count": gen.get("utility_pass_count"),
+                    "utility_pass_rate": gen.get("utility_pass_rate"),
+                    "utility_instruction_pass_rate": gen.get("utility_instruction_pass_rate"),
+                    "utility_json_pass_rate": gen.get("utility_json_pass_rate"),
+                    "utility_math_pass_rate": gen.get("utility_math_pass_rate"),
+                    "utility_state_pass_rate": gen.get("utility_state_pass_rate"),
+                    "utility_plan_trace_missing_turns": gen.get(
+                        "utility_plan_trace_missing_turns"
+                    ),
                     "mode_policy_table_size": policy_meta.get("table_size") if policy_meta else None,
                     "mode_policy_k_user": policy_meta.get("k_user") if policy_meta else None,
                     "memory_total_facts": memory_meta.get("total_facts") if memory_meta else None,
--- /dev/null	2026-01-10 16:50:51
+++ scripts/eval_skill_suite.py	2026-01-10 16:45:32
@@ -0,0 +1,71 @@
+#!/usr/bin/env python3
+from __future__ import annotations
+
+import argparse
+import hashlib
+import json
+import os
+import sys
+from typing import Any, Dict, List, Optional, Sequence
+
+sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))
+
+from atos_core.engine import Engine
+from atos_core.store import ActStore
+from atos_core.suite import SKILL_DIALOGUES_V0, run_skill_suite
+
+
+def sha256_text(s: str) -> str:
+    return hashlib.sha256(s.encode("utf-8")).hexdigest()
+
+
+def transcripts_text(transcripts: Sequence[Dict[str, Any]]) -> str:
+    return "\n".join(str(r.get("full_text", "")) for r in transcripts)
+
+
+def first_plan_trace(transcripts: Sequence[Dict[str, Any]]) -> Optional[Dict[str, Any]]:
+    for rec in transcripts:
+        turns = rec.get("turns", [])
+        if not isinstance(turns, list):
+            continue
+        for t in turns:
+            if not isinstance(t, dict):
+                continue
+            tr = t.get("trace") or {}
+            if not isinstance(tr, dict):
+                continue
+            pt = tr.get("plan_trace")
+            if isinstance(pt, dict):
+                return pt
+    return None
+
+
+def main() -> None:
+    ap = argparse.ArgumentParser()
+    ap.add_argument("--run", required=True, help="Run dir containing acts.jsonl (read-only)")
+    ap.add_argument("--seed", type=int, default=0)
+    ap.add_argument("--max_new_tokens", type=int, default=200)
+    args = ap.parse_args()
+
+    acts_path = os.path.join(args.run, "acts.jsonl")
+    store = ActStore.load_jsonl(acts_path)
+    engine = Engine(store, seed=args.seed)
+
+    transcripts, metrics = run_skill_suite(
+        engine, tasks=SKILL_DIALOGUES_V0, max_new_tokens=args.max_new_tokens
+    )
+    txt = transcripts_text(transcripts)
+    out: Dict[str, Any] = {
+        "run": str(args.run),
+        "seed": int(args.seed),
+        "max_new_tokens": int(args.max_new_tokens),
+        "sha256_transcript_text": sha256_text(txt),
+        "plan_trace_sample": first_plan_trace(transcripts),
+        **dict(metrics),
+    }
+    print(json.dumps(out, ensure_ascii=False, indent=2))
+
+
+if __name__ == "__main__":
+    main()
+
