--- /dev/null	2026-01-15 03:48:47
+++ scripts/canonicalize_chatgpt_export_v113.py	2026-01-15 03:37:58
@@ -0,0 +1,343 @@
+#!/usr/bin/env python3
+from __future__ import annotations
+
+import argparse
+import datetime as _dt
+import hashlib
+import json
+import os
+import shutil
+import sys
+from typing import Any, Dict, Iterator, List, Optional, Tuple
+
+# Ensure repo root is on sys.path (scripts/ is sys.path[0] when invoked directly).
+sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))
+
+from atos_core.act import canonical_json_dumps, sha256_hex
+
+
+CHUNK_CHARS = 1024 * 1024
+
+
+def _fail(msg: str, *, code: int = 2) -> None:
+    print(msg, file=sys.stderr)
+    raise SystemExit(code)
+
+
+def _sha256_file(path: str) -> str:
+    h = hashlib.sha256()
+    with open(path, "rb") as f:
+        while True:
+            b = f.read(1024 * 1024)
+            if not b:
+                break
+            h.update(b)
+    return h.hexdigest()
+
+
+def _iso_utc_from_epoch_seconds(sec: int) -> str:
+    dt = _dt.datetime.fromtimestamp(int(sec), tz=_dt.timezone.utc)
+    dt = dt.replace(microsecond=0)
+    return dt.isoformat().replace("+00:00", "Z")
+
+
+def _iter_json_array(path: str) -> Iterator[Any]:
+    """
+    Streaming JSON array parser using stdlib json.JSONDecoder.raw_decode.
+    Handles very large arrays without loading the whole file.
+    """
+    dec = json.JSONDecoder()
+    with open(path, "r", encoding="utf-8") as f:
+        # Seek to start of array.
+        while True:
+            ch = f.read(1)
+            if ch == "":
+                _fail("json_array_missing_open_bracket")
+            if ch.isspace():
+                continue
+            if ch != "[":
+                _fail(f"json_array_expected_open_bracket_got:{repr(ch)}")
+            break
+
+        buf = ""
+        while True:
+            if not buf:
+                chunk = f.read(CHUNK_CHARS)
+                if chunk == "":
+                    _fail("json_array_unexpected_eof")
+                buf += chunk
+
+            # Skip whitespace and commas.
+            i = 0
+            while i < len(buf) and buf[i].isspace():
+                i += 1
+            if i:
+                buf = buf[i:]
+            if buf.startswith(","):
+                buf = buf[1:]
+                continue
+            if buf.startswith("]"):
+                return
+
+            try:
+                obj, idx = dec.raw_decode(buf)
+            except json.JSONDecodeError:
+                chunk = f.read(CHUNK_CHARS)
+                if chunk == "":
+                    _fail("json_array_decode_error_eof")
+                buf += chunk
+                continue
+
+            yield obj
+            buf = buf[idx:]
+
+
+def _safe_int(x: Any) -> Optional[int]:
+    try:
+        if x is None:
+            return None
+        # Export uses float seconds; cast via int for determinism.
+        return int(float(x))
+    except Exception:
+        return None
+
+
+def _canon_role(role: Any) -> str:
+    r = str(role or "")
+    if r in ("user", "assistant", "system", "tool"):
+        return r
+    return "unknown"
+
+
+def _normalize_text(text: str) -> str:
+    # Minimal and deterministic: normalize line endings only.
+    t = str(text or "")
+    t = t.replace("\r\n", "\n").replace("\r", "\n")
+    return t
+
+
+def _extract_text_from_message_content(content: Any) -> str:
+    if not isinstance(content, dict):
+        return ""
+    ctype = str(content.get("content_type") or "")
+    if ctype == "text":
+        parts = content.get("parts")
+        if isinstance(parts, list):
+            out_parts: List[str] = []
+            for p in parts:
+                if isinstance(p, str):
+                    out_parts.append(p)
+                else:
+                    out_parts.append(canonical_json_dumps(p))
+            return "\n".join(out_parts)
+        return canonical_json_dumps(content)
+    return canonical_json_dumps(content)
+
+
+def _resolve_input_path(user_supplied: str) -> Tuple[str, List[str]]:
+    tried: List[str] = []
+
+    def _try(p: str) -> Optional[str]:
+        pp = os.path.expanduser(str(p))
+        tried.append(pp)
+        if os.path.exists(pp):
+            return pp
+        return None
+
+    if user_supplied:
+        got = _try(user_supplied)
+        if got:
+            return got, tried
+
+    # Known default locations (accented/unaccented).
+    defaults = [
+        "/Users/danielpenin/Desktop/HISTÓRICO/conversations.json",
+        "/Users/danielpenin/Desktop/HISTORICO/conversations.json",
+        os.path.expanduser("~/Desktop/HISTÓRICO/conversations.json"),
+        os.path.expanduser("~/Desktop/HISTORICO/conversations.json"),
+    ]
+    for p in defaults:
+        got = _try(p)
+        if got:
+            return got, tried
+
+    # Deterministic fallback search (sorted).
+    desktop = os.path.expanduser("~/Desktop")
+    tried.append(desktop + "/* (search)")
+    candidates: List[str] = []
+    if os.path.isdir(desktop):
+        for root, dirs, files in os.walk(desktop):
+            dirs.sort()
+            files.sort()
+            if "conversations.json" in files:
+                candidates.append(os.path.join(root, "conversations.json"))
+    candidates = sorted(set(candidates))
+    for p in candidates:
+        got = _try(p)
+        if got:
+            return got, tried
+
+    _fail("missing_input_conversations_json_tried:" + json.dumps(tried, ensure_ascii=False, sort_keys=True))
+    raise AssertionError("unreachable")
+
+
+def main() -> None:
+    ap = argparse.ArgumentParser()
+    ap.add_argument("--input", default="", help="Path to conversations.json (ChatGPT export).")
+    ap.add_argument("--out", required=True, help="External world output root (e.g., external_world).")
+    args = ap.parse_args()
+
+    input_path, tried = _resolve_input_path(str(args.input or ""))
+    out_root = str(args.out)
+    os.makedirs(out_root, exist_ok=True)
+
+    input_sha = _sha256_file(input_path)
+
+    raw_dir = os.path.join(out_root, "dialogue_history_raw")
+    os.makedirs(raw_dir, exist_ok=True)
+    raw_copy_name = "conversations_v113_{h}.json".format(h=input_sha[:16])
+    raw_copy_path = os.path.join(raw_dir, raw_copy_name)
+    if not os.path.exists(raw_copy_path):
+        tmp = raw_copy_path + ".tmp"
+        if os.path.exists(tmp):
+            _fail(f"tmp_exists:{tmp}")
+        shutil.copyfile(input_path, tmp)
+        os.replace(tmp, raw_copy_path)
+    else:
+        if _sha256_file(raw_copy_path) != input_sha:
+            _fail(f"raw_copy_sha_mismatch:{raw_copy_path}")
+
+    canon_path = os.path.join(out_root, "dialogue_history_canonical_v113.jsonl")
+    manifest_path = os.path.join(out_root, "dialogue_history_canonical_v113_manifest.json")
+    if os.path.exists(canon_path) or os.path.exists(manifest_path):
+        _fail("worm_exists_output_paths")
+
+    # Collect all message records for global temporal ordering.
+    messages: List[Dict[str, Any]] = []
+    conversation_ids: set = set()
+    missing_time = 0
+    unknown_role = 0
+
+    # Dedup guard: (conversation_id, message_id) must not conflict.
+    seen_mid: Dict[Tuple[str, str], Tuple[int, str]] = {}
+    dup_dropped = 0
+
+    for conv in _iter_json_array(input_path):
+        if not isinstance(conv, dict):
+            continue
+        conv_id = str(conv.get("id") or "")
+        if not conv_id:
+            continue
+        conversation_ids.add(conv_id)
+        mapping = conv.get("mapping")
+        if not isinstance(mapping, dict):
+            continue
+        for mid, node in mapping.items():
+            if not isinstance(mid, str) or not mid:
+                continue
+            if not isinstance(node, dict):
+                continue
+            msg = node.get("message")
+            if not isinstance(msg, dict):
+                continue
+            author = msg.get("author")
+            author = author if isinstance(author, dict) else {}
+            role = _canon_role(author.get("role"))
+            if role == "unknown":
+                unknown_role += 1
+            content = msg.get("content")
+            text = _normalize_text(_extract_text_from_message_content(content))
+
+            create_time = _safe_int(msg.get("create_time"))
+            if create_time is None:
+                missing_time += 1
+            ct_sort = int(create_time) if create_time is not None else -1
+            ts = _iso_utc_from_epoch_seconds(ct_sort) if create_time is not None else ""
+
+            key = (conv_id, mid)
+            text_hash = sha256_hex(text.encode("utf-8"))
+            if key in seen_mid:
+                prev_ct, prev_th = seen_mid[key]
+                if prev_ct == ct_sort and prev_th == text_hash:
+                    dup_dropped += 1
+                    continue
+                _fail("duplicate_message_id_conflict:" + canonical_json_dumps({"conversation_id": conv_id, "message_id": mid}))
+            seen_mid[key] = (ct_sort, text_hash)
+
+            messages.append(
+                {
+                    "create_time": int(ct_sort),
+                    "conversation_id": str(conv_id),
+                    "message_id": str(mid),
+                    "timestamp": str(ts),
+                    "role": str(role),
+                    "text": str(text),
+                }
+            )
+
+    messages.sort(key=lambda d: (int(d.get("create_time") or -1), str(d.get("conversation_id") or ""), str(d.get("message_id") or "")))
+
+    # Write canonical JSONL + compute offsets in-memory for internal checks only.
+    turns_total = 0
+    with open(canon_path, "xb") as f:
+        for idx, m in enumerate(messages):
+            line_obj = {
+                "global_turn_index": int(idx),
+                "conversation_id": str(m.get("conversation_id") or ""),
+                "message_id": str(m.get("message_id") or ""),
+                "timestamp": str(m.get("timestamp") or ""),
+                "role": str(m.get("role") or "unknown"),
+                "text": str(m.get("text") or ""),
+                "source": "chatgpt_export_v113",
+            }
+            f.write((canonical_json_dumps(line_obj) + "\n").encode("utf-8"))
+            turns_total += 1
+
+    out_sha = _sha256_file(canon_path)
+    raw_copy_sha = _sha256_file(raw_copy_path)
+    if raw_copy_sha != input_sha:
+        _fail("raw_copy_sha_mismatch_postcopy")
+
+    manifest = {
+        "schema_version": 113,
+        "kind": "chatgpt_export_canonical_v113",
+        "source": {
+            "input_path": str(input_path),
+            "input_sha256": str(input_sha),
+            "paths_tried": list(tried),
+        },
+        "paths": {
+            "raw_copy": os.path.relpath(raw_copy_path, out_root).replace(os.sep, "/"),
+            "canonical_jsonl": os.path.relpath(canon_path, out_root).replace(os.sep, "/"),
+        },
+        "sha256": {"input": str(input_sha), "raw_copy": str(raw_copy_sha), "canonical_jsonl": str(out_sha)},
+        "counts": {
+            "turns_total": int(turns_total),
+            "conversations_total": int(len(conversation_ids)),
+            "missing_timestamp_total": int(missing_time),
+            "unknown_role_total": int(unknown_role),
+            "duplicates_dropped_total": int(dup_dropped),
+        },
+    }
+    with open(manifest_path, "x", encoding="utf-8") as f:
+        f.write(json.dumps(manifest, ensure_ascii=False, indent=2, sort_keys=True) + "\n")
+
+    print(
+        json.dumps(
+            {
+                "ok": True,
+                "reason": "built",
+                "out": {"canonical_jsonl": canon_path, "manifest": manifest_path},
+                "sha256": dict(manifest["sha256"]),
+                "counts": dict(manifest["counts"]),
+            },
+            ensure_ascii=False,
+            indent=2,
+            sort_keys=True,
+        )
+    )
+
+
+if __name__ == "__main__":
+    main()
+
--- /dev/null	2026-01-15 03:48:47
+++ atos_core/external_dialogue_world_v113.py	2026-01-15 03:37:58
@@ -0,0 +1,163 @@
+from __future__ import annotations
+
+import json
+import os
+from dataclasses import dataclass
+from typing import Any, Dict, List, Optional, Sequence
+
+
+def _canon_path(p: str) -> str:
+    return str(p).replace(os.sep, "/")
+
+
+@dataclass(frozen=True)
+class ExternalWorldTurnV113:
+    global_turn_index: int
+    conversation_id: str
+    message_id: str
+    timestamp: str
+    role: str
+    text: str
+    source: str
+
+
+def _turn_from_obj(obj: Dict[str, Any]) -> ExternalWorldTurnV113:
+    return ExternalWorldTurnV113(
+        global_turn_index=int(obj.get("global_turn_index") or 0),
+        conversation_id=str(obj.get("conversation_id") or ""),
+        message_id=str(obj.get("message_id") or ""),
+        timestamp=str(obj.get("timestamp") or ""),
+        role=str(obj.get("role") or "unknown"),
+        text=str(obj.get("text") or ""),
+        source=str(obj.get("source") or ""),
+    )
+
+
+class ExternalDialogueWorldV113:
+    """
+    Deterministic read-only API over dialogue_history_canonical_v113.jsonl.
+
+    Indexing is deterministic and built in-memory (byte offsets per turn_id).
+    No embeddings, no clustering, no global summarization.
+    """
+
+    def __init__(self, *, canonical_jsonl_path: str, manifest: Dict[str, Any]) -> None:
+        self.canonical_jsonl_path = str(canonical_jsonl_path)
+        self.manifest = dict(manifest)
+        if not os.path.exists(self.canonical_jsonl_path):
+            raise FileNotFoundError("external_world_v113_missing_canonical_jsonl")
+        self.offsets: List[int] = []
+        self.turns_total = 0
+        self._build_offsets()
+
+    def _build_offsets(self) -> None:
+        self.offsets = []
+        idx = 0
+        with open(self.canonical_jsonl_path, "rb") as f:
+            while True:
+                off = f.tell()
+                line = f.readline()
+                if not line:
+                    break
+                self.offsets.append(int(off))
+                try:
+                    obj = json.loads(line.decode("utf-8"))
+                except Exception:
+                    raise ValueError("external_world_v113_json_decode_error")
+                if int(obj.get("global_turn_index", -1)) != int(idx):
+                    raise ValueError("external_world_v113_global_turn_index_mismatch")
+                idx += 1
+        self.turns_total = int(idx)
+
+    def fetch_turn(self, turn_id: int) -> ExternalWorldTurnV113:
+        idx = int(turn_id)
+        if idx < 0 or idx >= len(self.offsets):
+            raise IndexError("turn_id_out_of_range")
+        off = int(self.offsets[idx])
+        with open(self.canonical_jsonl_path, "rb") as f:
+            f.seek(off)
+            line = f.readline()
+        obj = json.loads(line.decode("utf-8"))
+        if int(obj.get("global_turn_index", -1)) != idx:
+            raise ValueError("turn_index_mismatch")
+        return _turn_from_obj(obj)
+
+    def observe_range(
+        self,
+        *,
+        start_turn: int,
+        end_turn: int,
+        roles: Optional[Sequence[str]] = None,
+        limit: Optional[int] = None,
+    ) -> List[ExternalWorldTurnV113]:
+        s = int(start_turn)
+        e = int(end_turn)
+        if s < 0:
+            s = 0
+        if e >= len(self.offsets):
+            e = len(self.offsets) - 1
+        if e < s:
+            return []
+        role_set = set([str(r) for r in (roles or []) if isinstance(r, str) and r])
+        out: List[ExternalWorldTurnV113] = []
+        max_n = int(limit) if limit is not None else None
+        for idx in range(s, e + 1):
+            t = self.fetch_turn(idx)
+            if role_set and t.role not in role_set:
+                continue
+            out.append(t)
+            if max_n is not None and len(out) >= max_n:
+                break
+        return list(out)
+
+    def search(
+        self,
+        *,
+        query: str,
+        limit: int,
+        roles: Optional[Sequence[str]] = None,
+    ) -> List[Dict[str, Any]]:
+        q = str(query or "")
+        if not q:
+            return []
+        role_set = set([str(r) for r in (roles or []) if isinstance(r, str) and r])
+        out: List[Dict[str, Any]] = []
+        with open(self.canonical_jsonl_path, "r", encoding="utf-8") as f:
+            for line in f:
+                obj = json.loads(line)
+                role = str(obj.get("role") or "unknown")
+                if role_set and role not in role_set:
+                    continue
+                text = str(obj.get("text") or "")
+                if q in text:
+                    snippet = text[:120]
+                    out.append(
+                        {
+                            "global_turn_index": int(obj.get("global_turn_index") or 0),
+                            "conversation_id": str(obj.get("conversation_id") or ""),
+                            "message_id": str(obj.get("message_id") or ""),
+                            "role": role,
+                            "snippet": snippet,
+                        }
+                    )
+                    if len(out) >= int(limit):
+                        break
+        out.sort(key=lambda d: (int(d.get("global_turn_index") or 0), str(d.get("conversation_id") or "")))
+        return list(out)
+
+
+def load_world_v113(*, manifest_path: str) -> ExternalDialogueWorldV113:
+    mp = str(manifest_path)
+    if not os.path.exists(mp):
+        raise FileNotFoundError("missing_world_manifest")
+    with open(mp, "r", encoding="utf-8") as f:
+        manifest = json.load(f)
+    root = os.path.dirname(os.path.abspath(mp))
+    paths = manifest.get("paths") if isinstance(manifest.get("paths"), dict) else {}
+    canon_rel = str(paths.get("canonical_jsonl") or "")
+    if not canon_rel:
+        # Default to sibling file name if manifest is minimal.
+        canon_rel = "dialogue_history_canonical_v113.jsonl"
+    canon_path = os.path.normpath(os.path.join(root, canon_rel))
+    return ExternalDialogueWorldV113(canonical_jsonl_path=canon_path, manifest=dict(manifest))
+
--- /dev/null	2026-01-15 03:48:47
+++ atos_core/external_world_gating_v113.py	2026-01-15 03:37:58
@@ -0,0 +1,98 @@
+from __future__ import annotations
+
+from typing import Any, Dict, List, Tuple
+
+from .act import sha256_hex
+from .external_dialogue_world_v113 import load_world_v113
+from .external_world_ledger_v111 import (
+    EXTERNAL_WORLD_ACTION_FETCH_V111,
+    EXTERNAL_WORLD_ACTION_OBSERVE_V111,
+    EXTERNAL_WORLD_ACTION_SEARCH_V111,
+    EXTERNAL_WORLD_REASON_CODES_V111,
+    external_world_event_to_dict_v111,
+    make_external_world_event_v111,
+)
+
+
+def external_world_access_v113(
+    *,
+    allowed: bool,
+    world_manifest: str,
+    action: str,
+    reason_code: str,
+    args: Dict[str, Any],
+    seed: int,
+    turn_index: int,
+    prev_event_sig: str,
+) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
+    """
+    Deterministic gating wrapper for the V113 ExternalDialogueWorld:
+      - If not allowed -> fail-closed with reason external_world_access_not_allowed.
+      - If reason_code invalid -> fail-closed with reason invalid_reason_code.
+
+    Returns: (events_jsonl_payloads, result_summary).
+    """
+    if not bool(allowed):
+        raise ValueError("external_world_access_not_allowed")
+    if str(reason_code) not in EXTERNAL_WORLD_REASON_CODES_V111:
+        raise ValueError("invalid_reason_code")
+
+    world = load_world_v113(manifest_path=str(world_manifest))
+
+    result_summary: Dict[str, Any] = {"seed": int(seed)}
+    if str(action) == EXTERNAL_WORLD_ACTION_SEARCH_V111:
+        q = str((args or {}).get("query") or "")
+        limit = int((args or {}).get("limit") or 3)
+        roles = (args or {}).get("roles") if isinstance((args or {}).get("roles"), list) else ["user"]
+        matches = world.search(query=str(q), limit=int(limit), roles=[str(r) for r in roles if isinstance(r, str)])
+        result_summary.update(
+            {
+                "query": str(q),
+                "matches_total": int(len(matches)),
+                "matches": [
+                    {
+                        "global_turn_index": int(m.get("global_turn_index") or 0),
+                        "conversation_id": str(m.get("conversation_id") or ""),
+                        "role": str(m.get("role") or ""),
+                    }
+                    for m in matches
+                    if isinstance(m, dict)
+                ],
+            }
+        )
+    elif str(action) == EXTERNAL_WORLD_ACTION_OBSERVE_V111:
+        s = int((args or {}).get("start_turn") or 0)
+        e = int((args or {}).get("end_turn") or 0)
+        roles = (args or {}).get("roles") if isinstance((args or {}).get("roles"), list) else ["user"]
+        turns = world.observe_range(start_turn=int(s), end_turn=int(e), roles=[str(r) for r in roles if isinstance(r, str)], limit=10)
+        result_summary.update(
+            {
+                "observed_total": int(len(turns)),
+                "observed_hash": sha256_hex(("\n".join([str(t.text) for t in turns]) if turns else "").encode("utf-8")),
+            }
+        )
+    elif str(action) == EXTERNAL_WORLD_ACTION_FETCH_V111:
+        tid = int((args or {}).get("turn_id") or 0)
+        t = world.fetch_turn(int(tid))
+        result_summary.update(
+            {
+                "fetched_turn_id": int(t.global_turn_index),
+                "conversation_id": str(t.conversation_id),
+                "role": str(t.role),
+                "text_hash": sha256_hex(str(t.text).encode("utf-8")),
+            }
+        )
+    else:
+        raise ValueError("invalid_external_world_action")
+
+    ev = make_external_world_event_v111(
+        event_index=0,
+        turn_index=int(turn_index),
+        action=str(action),
+        reason_code=str(reason_code),
+        args=dict(args),
+        result_summary=dict(result_summary),
+        prev_event_sig=str(prev_event_sig or ""),
+    )
+    return [external_world_event_to_dict_v111(ev)], dict(result_summary)
+
--- /dev/null	2026-01-15 03:48:47
+++ scripts/gen_family7_dla_from_history_v113.py	2026-01-15 03:44:30
@@ -0,0 +1,358 @@
+#!/usr/bin/env python3
+from __future__ import annotations
+
+import argparse
+import hashlib
+import json
+import os
+import sys
+from pathlib import Path
+from typing import Any, Dict, List, Sequence, Tuple
+
+sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))
+
+from atos_core.act import canonical_json_dumps, sha256_hex
+
+
+def _fail(msg: str, *, code: int = 2) -> None:
+    print(msg, file=sys.stderr)
+    raise SystemExit(code)
+
+
+def _sha256_file(path: Path) -> str:
+    h = hashlib.sha256()
+    with open(path, "rb") as f:
+        while True:
+            b = f.read(1024 * 1024)
+            if not b:
+                break
+            h.update(b)
+    return h.hexdigest()
+
+
+def _ensure_absent(path: Path) -> None:
+    if path.exists():
+        _fail(f"worm_exists:{path}")
+
+
+def _load_world_manifest(path: Path) -> Dict[str, Any]:
+    if not path.exists():
+        _fail(f"missing_world_manifest:{path}")
+    return json.loads(path.read_text(encoding="utf-8"))
+
+
+def _world_jsonl_path_from_manifest(manifest_path: Path) -> Path:
+    m = _load_world_manifest(manifest_path)
+    paths = m.get("paths") if isinstance(m.get("paths"), dict) else {}
+    rel = str(paths.get("canonical_jsonl") or "dialogue_history_canonical_v113.jsonl")
+    return (manifest_path.parent / rel).resolve()
+
+
+def _is_safe_user_turn_text_v113(text: str) -> bool:
+    t = str(text or "").strip()
+    if not t:
+        return False
+    if len(t) > 800:
+        return False
+    t0 = t.lstrip()
+    if t0.startswith("{") or t0.startswith("["):
+        return False
+    bad_substrings = [
+        "content_type",
+        "asset_pointer",
+        "file-service://",
+        "multimodal_text",
+        "image_asset_pointer",
+    ]
+    for s in bad_substrings:
+        if s in t0:
+            return False
+    if t0.count("\n") > 10:
+        return False
+    return True
+
+
+def _make_task(task_kind: str, payload: Dict[str, Any]) -> Dict[str, Any]:
+    body = dict(payload)
+    body["schema_version"] = 113
+    body["task_kind"] = str(task_kind)
+    task_id = sha256_hex(canonical_json_dumps(body).encode("utf-8"))
+    return dict(body, task_id=f"family7_dla_v113_{task_id}")
+
+
+def _injection_plan_for_task_v113(*, task_index: int, total_turns: int) -> List[Dict[str, Any]]:
+    plan: List[Tuple[str, int, str]] = []
+    plan.append(("dont_invent", 12, "não invente; se não souber diga não sei e pergunte o dado"))
+    cycle = int(task_index) % 7
+    if cycle == 0:
+        plan.append(("late_reference", min(total_turns - 5, 90), "como eu disse antes, faz isso do mesmo jeito"))
+    elif cycle == 1:
+        plan.append(("soft_contradiction", min(total_turns - 5, 45), "na verdade era X, não Y"))
+    elif cycle == 2:
+        plan.append(("implicit_goal_shift", min(total_turns - 5, 70), "agora muda o objetivo sem avisar e vê se você percebe"))
+    elif cycle == 3:
+        plan.append(("irony", min(total_turns - 5, 55), "claro... sqn"))
+    elif cycle == 4:
+        plan.append(("late_reference_far", min(total_turns - 5, 140), "isso que eu falei lá atrás continua valendo"))
+    elif cycle == 5:
+        plan.append(("hostile_confused", min(total_turns - 5, 30), "?? você não entendeu nada"))
+    elif cycle == 6:
+        plan.append(("minimalist_trap", min(total_turns - 5, 80), "ok"))
+    plan.sort(key=lambda t: (int(t[1]), str(t[0])))
+    out: List[Dict[str, Any]] = []
+    for kind, pos, text in plan:
+        out.append({"kind": str(kind), "pos": int(pos), "text": str(text)})
+    return out
+
+
+def _apply_injection_plan(turns: List[str], plan: Sequence[Dict[str, Any]]) -> List[str]:
+    out = list(turns)
+    for inj in sorted(plan, key=lambda d: (int(d.get("pos") or 0), str(d.get("kind") or ""))):
+        pos = int(inj.get("pos") or 0)
+        txt = str(inj.get("text") or "")
+        if not txt:
+            continue
+        if pos < 0:
+            pos = 0
+        if pos > len(out):
+            pos = len(out)
+        out.insert(pos, txt)
+    return list(out)
+
+
+def main() -> None:
+    ap = argparse.ArgumentParser()
+    ap.add_argument("--world_manifest", required=True)
+    ap.add_argument("--seed", required=True, type=int)
+    ap.add_argument("--out", required=True)
+    ap.add_argument("--tasks_total", type=int, default=20)
+    ap.add_argument("--stress_200", type=int, default=2)
+    ap.add_argument("--stress_500", type=int, default=1)
+    args = ap.parse_args()
+
+    seed = int(args.seed)
+    out_path = Path(str(args.out))
+    _ensure_absent(out_path)
+
+    world_manifest = Path(str(args.world_manifest))
+    canon_path = _world_jsonl_path_from_manifest(world_manifest)
+    if not canon_path.exists():
+        _fail(f"missing_world_canonical_jsonl:{canon_path}")
+
+    tasks_total = int(args.tasks_total)
+    stress_200 = int(args.stress_200)
+    stress_500 = int(args.stress_500)
+    if tasks_total < 20:
+        _fail("tasks_total_too_small")
+    if stress_200 < 2:
+        _fail("stress_200_too_small")
+    if stress_500 < 1:
+        _fail("stress_500_too_small")
+
+    # Build per-conversation user-turn buffers (deterministic, bounded).
+    conv_meta: Dict[str, Dict[str, Any]] = {}
+    conv_user_turns: Dict[str, List[str]] = {}
+    with open(canon_path, "r", encoding="utf-8") as f:
+        for line in f:
+            line = line.strip()
+            if not line:
+                continue
+            obj = json.loads(line)
+            cid = str(obj.get("conversation_id") or "")
+            if not cid:
+                continue
+            idx = int(obj.get("global_turn_index") or 0)
+            role = str(obj.get("role") or "unknown")
+            txt = str(obj.get("text") or "")
+            m = conv_meta.get(cid)
+            if m is None:
+                m = {"conversation_id": cid, "start_turn": idx, "end_turn": idx, "turns_total": 0, "user_turns_total": 0}
+                conv_meta[cid] = m
+                conv_user_turns[cid] = []
+            m["turns_total"] = int(m.get("turns_total") or 0) + 1
+            if idx < int(m.get("start_turn") or idx):
+                m["start_turn"] = int(idx)
+            if idx > int(m.get("end_turn") or idx):
+                m["end_turn"] = int(idx)
+            if role == "user" and _is_safe_user_turn_text_v113(txt):
+                m["user_turns_total"] = int(m.get("user_turns_total") or 0) + 1
+                # Cap to avoid memory blowup; still enough for STRESS_500 sampling.
+                buf = conv_user_turns[cid]
+                if len(buf) < 800:
+                    buf.append(str(txt))
+
+    # Candidate conversations by user_turns_total DESC then conversation_id ASC.
+    convs: List[Dict[str, Any]] = list(conv_meta.values())
+    convs.sort(key=lambda d: (-int(d.get("user_turns_total") or 0), str(d.get("conversation_id") or "")))
+
+    # Select conversations deterministically for stress tiers.
+    used_conv_ids: set = set()
+    tasks: List[Dict[str, Any]] = []
+    external_allocated = False
+
+    def _pick_conv(min_user_turns: int) -> Optional[Dict[str, Any]]:
+        for c in convs:
+            cid0 = str(c.get("conversation_id") or "")
+            if cid0 in used_conv_ids:
+                continue
+            if int(c.get("user_turns_total") or 0) < int(min_user_turns):
+                continue
+            used_conv_ids.add(cid0)
+            return dict(c)
+        return None
+
+    # Build STRESS_500 (or fallback STRESS_300 if not enough user turns).
+    stress_long_kind = "STRESS_500"
+    stress_long_turns = 500
+    conv_for_500 = _pick_conv(200)
+    if conv_for_500 is None:
+        _fail("not_enough_conversations_for_stress_long")
+    safe_turns_500 = conv_user_turns.get(str(conv_for_500.get("conversation_id") or ""), [])
+    if len(safe_turns_500) < 60:
+        # If the dataset doesn't support long realistic windows, fallback to STRESS_300.
+        stress_long_kind = "STRESS_300"
+        stress_long_turns = 300
+
+    for j in range(stress_500):
+        if conv_for_500 is None:
+            break
+        cid = str(conv_for_500.get("conversation_id") or "")
+        safe_turns = conv_user_turns.get(cid, [])
+        real_sample_n = min(120, len(safe_turns))
+        if real_sample_n < 20:
+            _fail("not_enough_user_turns_for_stress_long_sample")
+        base_turns = safe_turns[:real_sample_n]
+        plan = _injection_plan_for_task_v113(task_index=len(tasks), total_turns=int(stress_long_turns))
+        goal_turn = "goal: family7_v113 outcome=complete constraints=deterministic deadline={n}".format(n=int(stress_long_turns))
+        user_turns = [goal_turn] + list(base_turns)
+        user_turns = _apply_injection_plan(user_turns, plan)
+        while len(user_turns) < int(stress_long_turns):
+            user_turns.append("ok")
+        allow_external = bool(not external_allocated)
+        if allow_external:
+            external_allocated = True
+        tasks.append(
+            _make_task(
+                "family7_dla_task_v113",
+                {
+                    "seed": int(seed),
+                    "world_manifest": str(world_manifest.as_posix()),
+                    "conversation_id": str(cid),
+                    "window": {"start_turn": int(conv_for_500.get("start_turn") or 0), "end_turn": int(conv_for_500.get("end_turn") or 0)},
+                    "stress_kind": str(stress_long_kind),
+                    "minimal_ok_turns": int(stress_long_turns),
+                    "real_user_sample_turns": int(real_sample_n),
+                    "injection_plan": list(plan),
+                    "allow_external_world_once": bool(allow_external),
+                    "external_world_probe_reason_code": "validator_failed_fluency_contract",
+                    "expected_validators": ["fluency_survival_v112", "binding_unresolved_reference_zero", "semantic_contradiction_zero"],
+                    "user_turns": list(user_turns),
+                },
+            )
+        )
+
+    # Build STRESS_200 tasks.
+    stress_built = 0
+    while stress_built < stress_200:
+        c = _pick_conv(80)
+        if c is None:
+            _fail("not_enough_conversations_for_stress_200")
+        cid = str(c.get("conversation_id") or "")
+        safe_turns = conv_user_turns.get(cid, [])
+        real_sample_n = min(60, len(safe_turns))
+        if real_sample_n < 12:
+            continue
+        base_turns = safe_turns[:real_sample_n]
+        plan = _injection_plan_for_task_v113(task_index=len(tasks), total_turns=200)
+        goal_turn = "goal: family7_v113 outcome=complete constraints=deterministic deadline=200"
+        user_turns = [goal_turn] + list(base_turns)
+        user_turns = _apply_injection_plan(user_turns, plan)
+        while len(user_turns) < 200:
+            user_turns.append("ok")
+        tasks.append(
+            _make_task(
+                "family7_dla_task_v113",
+                {
+                    "seed": int(seed),
+                    "world_manifest": str(world_manifest.as_posix()),
+                    "conversation_id": str(cid),
+                    "window": {"start_turn": int(c.get("start_turn") or 0), "end_turn": int(c.get("end_turn") or 0)},
+                    "stress_kind": "STRESS_200",
+                    "minimal_ok_turns": 200,
+                    "real_user_sample_turns": int(real_sample_n),
+                    "injection_plan": list(plan),
+                    "allow_external_world_once": False,
+                    "external_world_probe_reason_code": "validator_failed_fluency_contract",
+                    "expected_validators": ["fluency_survival_v112", "binding_unresolved_reference_zero", "semantic_contradiction_zero"],
+                    "user_turns": list(user_turns),
+                },
+            )
+        )
+        stress_built += 1
+
+    # Fill remaining tasks (medium horizon).
+    while len(tasks) < tasks_total:
+        c = _pick_conv(20)
+        if c is None:
+            _fail("not_enough_conversations_for_medium_tasks")
+        cid = str(c.get("conversation_id") or "")
+        safe_turns = conv_user_turns.get(cid, [])
+        real_sample_n = min(30, len(safe_turns))
+        if real_sample_n < 8:
+            continue
+        base_turns = safe_turns[:real_sample_n]
+        plan = _injection_plan_for_task_v113(task_index=len(tasks), total_turns=120)
+        goal_turn = "goal: family7_v113 outcome=complete constraints=deterministic deadline=120"
+        user_turns = [goal_turn] + list(base_turns)
+        user_turns = _apply_injection_plan(user_turns, plan)
+        while len(user_turns) < 120:
+            user_turns.append("ok")
+        tasks.append(
+            _make_task(
+                "family7_dla_task_v113",
+                {
+                    "seed": int(seed),
+                    "world_manifest": str(world_manifest.as_posix()),
+                    "conversation_id": str(cid),
+                    "window": {"start_turn": int(c.get("start_turn") or 0), "end_turn": int(c.get("end_turn") or 0)},
+                    "stress_kind": "MEDIUM_120",
+                    "minimal_ok_turns": 120,
+                    "real_user_sample_turns": int(real_sample_n),
+                    "injection_plan": list(plan),
+                    "allow_external_world_once": False,
+                    "external_world_probe_reason_code": "validator_failed_fluency_contract",
+                    "expected_validators": ["fluency_survival_v112", "binding_unresolved_reference_zero", "semantic_contradiction_zero"],
+                    "user_turns": list(user_turns),
+                },
+            )
+        )
+
+    # Write tasks JSONL (WORM).
+    out_path.parent.mkdir(parents=True, exist_ok=True)
+    with open(out_path, "x", encoding="utf-8") as f:
+        for t in tasks:
+            f.write(canonical_json_dumps(t))
+            f.write("\n")
+
+    # Write manifest (WORM).
+    manifest_path = out_path.with_suffix(out_path.suffix + "_manifest.json")
+    _ensure_absent(manifest_path)
+    manifest = {
+        "schema_version": 113,
+        "kind": "family7_dla_tasks_v113",
+        "seed": int(seed),
+        "world_manifest": str(world_manifest.as_posix()),
+        "world_canonical_sha256": _sha256_file(canon_path),
+        "tasks_total": int(len(tasks)),
+        "stress_200_total": int(sum(1 for t in tasks if str(t.get("stress_kind") or "") == "STRESS_200")),
+        "stress_500_total": int(sum(1 for t in tasks if str(t.get("stress_kind") or "") == "STRESS_500")),
+        "stress_300_total": int(sum(1 for t in tasks if str(t.get("stress_kind") or "") == "STRESS_300")),
+        "tasks_sha256": _sha256_file(out_path),
+    }
+    manifest_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2, sort_keys=True) + "\n", encoding="utf-8")
+
+    print(json.dumps({"ok": True, "out": str(out_path), "manifest": str(manifest_path), "manifest_obj": manifest}, ensure_ascii=False, indent=2, sort_keys=True))
+
+
+if __name__ == "__main__":
+    main()
--- /dev/null	2026-01-15 03:48:47
+++ scripts/run_family7_dla_v113.py	2026-01-15 03:37:58
@@ -0,0 +1,437 @@
+#!/usr/bin/env python3
+from __future__ import annotations
+
+import argparse
+import hashlib
+import json
+import os
+import sys
+from pathlib import Path
+from typing import Any, Dict, List, Sequence, Tuple
+
+sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))
+
+from atos_core.act import canonical_json_dumps, sha256_hex
+from atos_core.conversation_loop_v110 import run_conversation_v110
+from atos_core.external_world_ledger_v111 import (
+    EXTERNAL_WORLD_REASON_CODES_V111,
+    compute_external_world_chain_hash_v111,
+    verify_external_world_event_sig_chain_v111,
+)
+from atos_core.external_world_gating_v113 import external_world_access_v113
+from atos_core.external_world_ledger_v111 import EXTERNAL_WORLD_ACTION_SEARCH_V111
+from atos_core.fluency_survival_v112 import fluency_contract_v112, fluency_survival_plan_v112, summarize_fluency_fail_code_v112
+
+
+def _fail(msg: str, *, code: int = 2) -> None:
+    print(msg, file=sys.stderr)
+    raise SystemExit(code)
+
+
+def _sha256_file(path: Path) -> str:
+    h = hashlib.sha256()
+    with open(path, "rb") as f:
+        while True:
+            b = f.read(1024 * 1024)
+            if not b:
+                break
+            h.update(b)
+    return h.hexdigest()
+
+
+def _ensure_absent(path: Path) -> None:
+    if path.exists():
+        _fail(f"worm_exists:{path}")
+
+
+ACK_TO_CHOICE_LABEL_V112 = {
+    "ok",
+    "okay",
+    "certo",
+    "beleza",
+    "blz",
+    "continua",
+    "continue",
+    "segue",
+    "vai",
+    "faz",
+    "pode",
+    "sim",
+}
+
+
+def _canon_ack_token_v112(s: str) -> str:
+    t = str(s or "").strip().lower()
+    t = " ".join([x for x in t.split() if x])
+    return t
+
+
+def _choiceify_minimal_ack_v112(user_turn_texts: Sequence[str]) -> List[str]:
+    out: List[str] = []
+    for s in user_turn_texts:
+        cs = _canon_ack_token_v112(str(s))
+        if cs in ACK_TO_CHOICE_LABEL_V112:
+            out.append("A")
+        else:
+            out.append(str(s))
+    return out
+
+
+def _load_jsonl_payload_view(path: Path) -> List[Dict[str, Any]]:
+    out: List[Dict[str, Any]] = []
+    if not path.exists():
+        return out
+    with open(path, "r", encoding="utf-8") as f:
+        for line in f:
+            line = line.strip()
+            if not line:
+                continue
+            obj = json.loads(line)
+            if not isinstance(obj, dict):
+                continue
+            payload = obj.get("payload")
+            if not isinstance(payload, dict):
+                continue
+            out.append(dict(payload))
+    return out
+
+
+def _load_jsonl(path: Path) -> List[Dict[str, Any]]:
+    out: List[Dict[str, Any]] = []
+    if not path.exists():
+        return out
+    with open(path, "r", encoding="utf-8") as f:
+        for line in f:
+            line = line.strip()
+            if not line:
+                continue
+            out.append(json.loads(line))
+    return out
+
+
+def _write_once_json(path: Path, obj: Any) -> None:
+    _ensure_absent(path)
+    tmp = path.with_suffix(path.suffix + ".tmp")
+    if tmp.exists():
+        _fail(f"tmp_exists:{tmp}")
+    tmp.write_text(json.dumps(obj, ensure_ascii=False, indent=2, sort_keys=True) + "\n", encoding="utf-8")
+    os.replace(str(tmp), str(path))
+
+
+def _compute_external_world_access_once_v113(
+    *,
+    world_manifest: str,
+    reason_code: str,
+    query: str,
+    seed: int,
+) -> List[Dict[str, Any]]:
+    try:
+        evs, _ = external_world_access_v113(
+            allowed=True,
+            world_manifest=str(world_manifest),
+            action=EXTERNAL_WORLD_ACTION_SEARCH_V111,
+            reason_code=str(reason_code),
+            args={"query": str(query), "limit": 3, "roles": ["user"]},
+            seed=int(seed),
+            turn_index=0,
+            prev_event_sig="",
+        )
+        return list(evs)
+    except ValueError as e:
+        _fail(str(e))
+    return []
+
+
+def _count_unresolved_reference_events(binding_events: Sequence[Dict[str, Any]]) -> int:
+    bad = 0
+    for ev in binding_events:
+        if not isinstance(ev, dict):
+            continue
+        t = str(ev.get("type") or "")
+        if t in {"BIND_MISS", "BIND_AMBIGUOUS"}:
+            bad += 1
+    return int(bad)
+
+
+def _unresolved_reference_final_from_flow(flow_events: Sequence[Dict[str, Any]]) -> int:
+    if not flow_events:
+        return 0
+    last = flow_events[-1] if isinstance(flow_events[-1], dict) else {}
+    flags = last.get("flow_flags_v108")
+    if not isinstance(flags, dict):
+        return 0
+    return 1 if bool(flags.get("UNRESOLVED_REFERENCE")) else 0
+
+
+def _count_semantic_contradiction_flags(semantic_events: Sequence[Dict[str, Any]]) -> int:
+    cnt = 0
+    for ev in semantic_events:
+        if not isinstance(ev, dict):
+            continue
+        flags = ev.get("flags_v109")
+        if not isinstance(flags, dict):
+            continue
+        if bool(flags.get("CONTRADICTION_UNREPAIRED")):
+            cnt += 1
+    return int(cnt)
+
+
+def _write_external_world_ledger(*, task_dir: Path, events: Sequence[Dict[str, Any]]) -> Dict[str, Any]:
+    events_path = task_dir / "external_world_events.jsonl"
+    _ensure_absent(events_path)
+    if events:
+        with open(events_path, "x", encoding="utf-8") as f:
+            for e in events:
+                f.write(canonical_json_dumps(e))
+                f.write("\n")
+    else:
+        events_path.write_text("", encoding="utf-8")
+
+    ok_sig, reason_sig, _ = verify_external_world_event_sig_chain_v111(list(events))
+    if not ok_sig:
+        _fail(f"external_world_sig_chain_fail:{reason_sig}")
+    chain_hash = compute_external_world_chain_hash_v111(list(events))
+    snap = {
+        "schema_version": 111,
+        "kind": "external_world_registry_snapshot_v111",
+        "events_total": int(len(events)),
+        "external_world_chain_hash_v111": str(chain_hash),
+    }
+    snap_path = task_dir / "external_world_registry_snapshot_v111.json"
+    _write_once_json(snap_path, snap)
+    return {
+        "events_total": int(len(events)),
+        "external_world_chain_hash_v111": str(chain_hash),
+        "external_world_events_jsonl": str(events_path),
+        "external_world_registry_snapshot_v111_json": str(snap_path),
+    }
+
+
+def _compute_freeze_manifest_v113(*, task_dir: Path, sha256_paths: Dict[str, str]) -> Dict[str, Any]:
+    sha256: Dict[str, str] = {}
+    rel_paths: Dict[str, str] = {}
+    for k, p in sorted(sha256_paths.items(), key=lambda kv: str(kv[0])):
+        fp = Path(p)
+        try:
+            rel_paths[str(k)] = str(fp.relative_to(task_dir))
+        except Exception:
+            rel_paths[str(k)] = str(fp.name)
+        if fp.exists():
+            sha256[str(k)] = _sha256_file(fp)
+    return {"schema_version": 113, "kind": "freeze_manifest_v113", "sha256": sha256, "sha256_paths": dict(rel_paths)}
+
+
+def main() -> None:
+    ap = argparse.ArgumentParser()
+    ap.add_argument("--tasks", required=True)
+    ap.add_argument("--out", required=True)
+    ap.add_argument("--seed", required=True, type=int)
+    ap.add_argument("--max_tasks", type=int, default=9999)
+    ap.add_argument("--max_rewrites", type=int, default=4)
+    args = ap.parse_args()
+
+    seed = int(args.seed)
+    tasks_path = Path(str(args.tasks))
+    out_dir = Path(str(args.out))
+    _ensure_absent(out_dir)
+    out_dir.mkdir(parents=True, exist_ok=False)
+
+    tasks: List[Dict[str, Any]] = []
+    with open(tasks_path, "r", encoding="utf-8") as f:
+        for line in f:
+            line = line.strip()
+            if not line:
+                continue
+            tasks.append(json.loads(line))
+    if not tasks:
+        _fail("empty_tasks")
+
+    tasks = tasks[: min(int(args.max_tasks), len(tasks))]
+
+    results: List[Dict[str, Any]] = []
+    failures: List[Dict[str, Any]] = []
+
+    for i, task in enumerate(tasks):
+        task_id = str(task.get("task_id") or f"task_{i}")
+        user_turns = task.get("user_turns") if isinstance(task.get("user_turns"), list) else []
+        user_turn_texts = [str(x) for x in user_turns if isinstance(x, str)]
+        user_turn_texts_engine = _choiceify_minimal_ack_v112(user_turn_texts)
+        task_subdir = out_dir / f"task_{i:03d}"
+        _ensure_absent(task_subdir)
+        task_subdir.mkdir(parents=True, exist_ok=False)
+
+        world_manifest = str(task.get("world_manifest") or "")
+        allow_external = bool(task.get("allow_external_world_once"))
+        reason_code = str(task.get("external_world_probe_reason_code") or "validator_failed_fluency_contract")
+        if reason_code and reason_code not in EXTERNAL_WORLD_REASON_CODES_V111:
+            _fail(f"invalid_reason_code_in_task:{reason_code}")
+
+        attempt_seeds = fluency_survival_plan_v112(base_seed=int(seed), max_attempts=int(args.max_rewrites))
+        attempts: List[Dict[str, Any]] = []
+        chosen_attempt = -1
+        ext_events_final: List[Dict[str, Any]] = []
+        ext_used = False
+        ext_used_reason = ""
+
+        for a, seed_used in enumerate(attempt_seeds):
+            attempt_dir = task_subdir / f"attempt_{a:03d}"
+            _ensure_absent(attempt_dir)
+            run_conversation_v110(user_turn_texts=user_turn_texts_engine, out_dir=str(attempt_dir), seed=int(seed_used))
+
+            transcript_rows = _load_jsonl_payload_view(attempt_dir / "transcript.jsonl")
+            user_i = 0
+            transcript_view: List[Dict[str, Any]] = []
+            for r in transcript_rows:
+                role = str(r.get("role") or "")
+                text = str(r.get("text") or "")
+                if role == "user" and user_i < len(user_turn_texts):
+                    text = str(user_turn_texts[user_i])
+                    user_i += 1
+                transcript_view.append({"role": role, "text": text})
+
+            ok_fc, reason_fc, details_fc = fluency_contract_v112(transcript_view=transcript_view)
+            if allow_external and (not ext_used) and a == 0:
+                ok_fc = False
+                reason_fc = "forced_external_world_probe"
+
+            binding_events = _load_jsonl(attempt_dir / "binding_events.jsonl")
+            unresolved_refs_total = _count_unresolved_reference_events(binding_events)
+            flow_events = _load_jsonl(attempt_dir / "flow_events.jsonl")
+            unresolved_refs_final = _unresolved_reference_final_from_flow(flow_events)
+            semantic_events = _load_jsonl(attempt_dir / "semantic_events.jsonl")
+            contradiction_flags = _count_semantic_contradiction_flags(semantic_events)
+
+            attempts.append(
+                {
+                    "attempt_index": int(a),
+                    "seed_used": int(seed_used),
+                    "ok_fluency": bool(ok_fc),
+                    "reason_fluency": str(reason_fc),
+                    "unresolved_reference_events_total": int(unresolved_refs_total),
+                    "unresolved_reference_final": int(unresolved_refs_final),
+                    "semantic_contradiction_flags": int(contradiction_flags),
+                    "fluency_details": dict(details_fc),
+                }
+            )
+
+            if allow_external and (not ext_used) and (not ok_fc):
+                ext_events_final = _compute_external_world_access_once_v113(
+                    world_manifest=world_manifest,
+                    reason_code=str(reason_code),
+                    query="não invente",
+                    seed=int(seed),
+                )
+                ext_used = True
+                ext_used_reason = str(reason_code)
+
+            if ok_fc and unresolved_refs_final == 0 and contradiction_flags == 0:
+                chosen_attempt = int(a)
+                break
+
+        _write_once_json(
+            task_subdir / "fluency_survival_v113.json",
+            {
+                "schema_version": 113,
+                "task_id": str(task_id),
+                "chosen_attempt_index": int(chosen_attempt),
+                "attempts": list(attempts),
+                "external_world_used": bool(ext_used),
+                "external_world_reason_code": str(ext_used_reason),
+            },
+        )
+
+        final_attempt_dir = task_subdir / (f"attempt_{chosen_attempt:03d}" if chosen_attempt >= 0 else "attempt_000")
+        ext_info = _write_external_world_ledger(task_dir=final_attempt_dir, events=ext_events_final if ext_used else [])
+
+        freeze_path = final_attempt_dir / "freeze_manifest_v113.json"
+        sha256_paths = {
+            "v110_summary_json": str(final_attempt_dir / "summary.json"),
+            "v110_freeze_manifest_v110_json": str(final_attempt_dir / "freeze_manifest_v110.json"),
+            "task_eval_json": str(final_attempt_dir / "eval.json"),
+            "fluency_survival_v113_json": str(task_subdir / "fluency_survival_v113.json"),
+            "external_world_events_jsonl": str(ext_info["external_world_events_jsonl"]),
+            "external_world_registry_snapshot_v111_json": str(ext_info["external_world_registry_snapshot_v111_json"]),
+        }
+        freeze = _compute_freeze_manifest_v113(task_dir=final_attempt_dir, sha256_paths=sha256_paths)
+        _write_once_json(freeze_path, freeze)
+        ledger_hash = _sha256_file(freeze_path)
+
+        ok_task = bool(chosen_attempt >= 0)
+        eval_obj = {
+            "schema_version": 113,
+            "task_id": str(task_id),
+            "ok": bool(ok_task),
+            "chosen_attempt_index": int(chosen_attempt),
+            "external_world_events_total": int(ext_info["events_total"]),
+            "external_world_chain_hash_v111": str(ext_info["external_world_chain_hash_v111"]),
+            "external_world_used_reason_code": str(ext_used_reason),
+            "ledger_hash": str(ledger_hash),
+        }
+        eval_path = final_attempt_dir / "eval_v113.json"
+        _write_once_json(eval_path, eval_obj)
+
+        results.append(
+            {
+                "task_index": int(i),
+                "task_id": str(task_id),
+                "run_dir": str(f"task_{i:03d}/" + final_attempt_dir.name),
+                "ok": bool(ok_task),
+                "chosen_attempt_index": int(chosen_attempt),
+                "external_world_events_total": int(ext_info["events_total"]),
+                "ledger_hash": str(ledger_hash),
+            }
+        )
+
+        if not ok_task:
+            fail_reason = summarize_fluency_fail_code_v112(str(attempts[-1].get("reason_fluency") or "")) if attempts else "unknown"
+            failures.append(
+                {
+                    "task_index": int(i),
+                    "task_id": str(task_id),
+                    "fail_code": str(fail_reason),
+                    "attempts_total": int(len(attempts)),
+                    "final_attempt_seed": int(attempts[-1].get("seed_used") or seed) if attempts else int(seed),
+                    "final_attempt_dir": str(final_attempt_dir),
+                }
+            )
+
+    agg = {
+        "schema_version": 113,
+        "seed": int(seed),
+        "tasks_total": int(len(results)),
+        "tasks_ok": int(sum(1 for r in results if bool(r.get("ok")))),
+        "results": list(results),
+        "aggregate_sig": sha256_hex(canonical_json_dumps({"seed": int(seed), "results": results}).encode("utf-8")),
+    }
+    _write_once_json(out_dir / "eval.json", agg)
+
+    summary = {
+        "schema_version": 113,
+        "seed": int(seed),
+        "tasks_total": int(len(results)),
+        "tasks_ok": int(sum(1 for r in results if bool(r.get("ok")))),
+        "eval_sha256": _sha256_file(out_dir / "eval.json"),
+    }
+    _write_once_json(out_dir / "summary.json", summary)
+
+    fc = {
+        "schema_version": 113,
+        "seed": int(seed),
+        "tasks_total": int(len(results)),
+        "tasks_ok": int(sum(1 for r in results if bool(r.get("ok")))),
+        "failures_total": int(len(failures)),
+        "failures": list(failures),
+        "top_failures": {
+            k: int(sum(1 for f in failures if str(f.get("fail_code") or "") == k))
+            for k in sorted(set([str(f.get("fail_code") or "") for f in failures]), key=str)
+        },
+        "suggested_patch_scope": "scripts/run_family7_dla_v113.py",
+    }
+    _write_once_json(out_dir / "fail_catalog_v113.json", fc)
+
+    print(json.dumps({"ok": True, "out_dir": str(out_dir), "summary": summary}, ensure_ascii=False, indent=2, sort_keys=True))
+
+
+if __name__ == "__main__":
+    main()
+
--- /dev/null	2026-01-15 03:48:47
+++ scripts/smoke_v113_family7_real_history_stress.py	2026-01-15 03:37:58
@@ -0,0 +1,188 @@
+#!/usr/bin/env python3
+from __future__ import annotations
+
+import argparse
+import hashlib
+import json
+import os
+import subprocess
+import sys
+from pathlib import Path
+from typing import Any, Dict, List
+
+sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))
+
+from atos_core.act import canonical_json_dumps, sha256_hex
+from atos_core.external_world_gating_v113 import external_world_access_v113
+from atos_core.external_world_ledger_v111 import EXTERNAL_WORLD_ACTION_SEARCH_V111
+
+
+def _sha256_file(path: Path) -> str:
+    h = hashlib.sha256()
+    with open(path, "rb") as f:
+        while True:
+            b = f.read(1024 * 1024)
+            if not b:
+                break
+            h.update(b)
+    return h.hexdigest()
+
+
+def _ensure_absent(path: Path) -> None:
+    if path.exists():
+        raise SystemExit(f"worm_exists:{path}")
+
+
+def _load_json(path: Path) -> Any:
+    return json.loads(path.read_text(encoding="utf-8"))
+
+
+def _run_runner(*, tasks: str, out_dir: Path, seed: int) -> None:
+    _ensure_absent(out_dir)
+    out_dir.parent.mkdir(parents=True, exist_ok=True)
+    env = dict(os.environ)
+    cmd = [
+        sys.executable,
+        "scripts/run_family7_dla_v113.py",
+        "--tasks",
+        str(tasks),
+        "--out",
+        str(out_dir),
+        "--seed",
+        str(seed),
+        "--max_tasks",
+        "9999",
+        "--max_rewrites",
+        "4",
+    ]
+    p = subprocess.run(cmd, env=env, cwd=str(Path(__file__).resolve().parent.parent), capture_output=True, text=True)
+    if p.returncode != 0:
+        raise SystemExit("runner_failed:\nSTDOUT:\n{out}\nSTDERR:\n{err}".format(out=p.stdout, err=p.stderr))
+
+
+def _negative_tests(*, world_manifest: str) -> Dict[str, Any]:
+    ok1 = False
+    reason1 = ""
+    try:
+        external_world_access_v113(
+            allowed=False,
+            world_manifest=str(world_manifest),
+            action=EXTERNAL_WORLD_ACTION_SEARCH_V111,
+            reason_code="validator_failed_fluency_contract",
+            args={"query": "x", "limit": 1, "roles": ["user"]},
+            seed=0,
+            turn_index=0,
+            prev_event_sig="",
+        )
+        ok1 = True
+    except ValueError as e:
+        reason1 = str(e)
+
+    ok2 = False
+    reason2 = ""
+    try:
+        external_world_access_v113(
+            allowed=True,
+            world_manifest=str(world_manifest),
+            action=EXTERNAL_WORLD_ACTION_SEARCH_V111,
+            reason_code="invalid_reason_code_x",
+            args={"query": "x", "limit": 1, "roles": ["user"]},
+            seed=0,
+            turn_index=0,
+            prev_event_sig="",
+        )
+        ok2 = True
+    except ValueError as e:
+        reason2 = str(e)
+
+    return {
+        "access_not_allowed": {"ok": bool(ok1), "reason": str(reason1)},
+        "invalid_reason_code": {"ok": bool(ok2), "reason": str(reason2)},
+    }
+
+
+def main() -> None:
+    ap = argparse.ArgumentParser()
+    ap.add_argument("--tasks", required=True)
+    ap.add_argument("--out_base", required=True)
+    ap.add_argument("--seed", required=True, type=int)
+    args = ap.parse_args()
+
+    seed = int(args.seed)
+    tasks_path = str(args.tasks)
+    out_base = Path(str(args.out_base))
+
+    tasks: List[Dict[str, Any]] = []
+    with open(tasks_path, "r", encoding="utf-8") as f:
+        for line in f:
+            line = line.strip()
+            if not line:
+                continue
+            tasks.append(json.loads(line))
+    if not tasks:
+        raise SystemExit("empty_tasks")
+    world_manifest = str(tasks[0].get("world_manifest") or "")
+
+    neg = _negative_tests(world_manifest=world_manifest)
+    if neg["access_not_allowed"]["reason"] != "external_world_access_not_allowed":
+        raise SystemExit("negative_failed:access_not_allowed")
+    if neg["invalid_reason_code"]["reason"] != "invalid_reason_code":
+        raise SystemExit("negative_failed:invalid_reason_code")
+
+    out1 = Path(str(out_base) + "_try1")
+    out2 = Path(str(out_base) + "_try2")
+    _run_runner(tasks=tasks_path, out_dir=out1, seed=seed)
+    _run_runner(tasks=tasks_path, out_dir=out2, seed=seed)
+
+    s1 = _load_json(out1 / "summary.json")
+    s2 = _load_json(out2 / "summary.json")
+    eval_sha1 = str(s1.get("eval_sha256") or "")
+    eval_sha2 = str(s2.get("eval_sha256") or "")
+    if eval_sha1 != eval_sha2:
+        raise SystemExit("determinism_failed:eval_sha")
+
+    ev1 = _load_json(out1 / "eval.json")
+    ev2 = _load_json(out2 / "eval.json")
+    if canonical_json_dumps(ev1) != canonical_json_dumps(ev2):
+        raise SystemExit("determinism_failed:eval_json")
+
+    if int(ev1.get("tasks_ok") or 0) != int(ev1.get("tasks_total") or 0):
+        raise SystemExit("tasks_not_all_ok")
+
+    stress_200_idxs = [i for i, t in enumerate(tasks) if str(t.get("stress_kind") or "") == "STRESS_200"]
+    stress_500_idxs = [i for i, t in enumerate(tasks) if str(t.get("stress_kind") or "") == "STRESS_500"]
+    stress_300_idxs = [i for i, t in enumerate(tasks) if str(t.get("stress_kind") or "") == "STRESS_300"]
+    if len(tasks) < 20:
+        raise SystemExit("tasks_total_lt_20")
+    if len(stress_200_idxs) < 2:
+        raise SystemExit("stress_200_lt_2")
+    if len(stress_500_idxs) < 1 and len(stress_300_idxs) < 1:
+        raise SystemExit("stress_long_lt_1")
+
+    res1 = ev1.get("results") if isinstance(ev1.get("results"), list) else []
+    ext_counts = [int(r.get("external_world_events_total") or 0) for r in res1 if isinstance(r, dict)]
+    if sum(1 for c in ext_counts if c == 1) != 1:
+        raise SystemExit("external_world_in_cycle_expected_one_call")
+
+    core = {
+        "schema_version": 113,
+        "seed": int(seed),
+        "try1": {"eval_sha256": eval_sha1, "tasks_ok": int(s1.get("tasks_ok") or 0)},
+        "try2": {"eval_sha256": eval_sha2, "tasks_ok": int(s2.get("tasks_ok") or 0)},
+        "negative_tests": dict(neg),
+    }
+    summary_sha256 = sha256_hex(canonical_json_dumps(core).encode("utf-8"))
+    out = {
+        "ok": True,
+        "determinism_ok": True,
+        "summary_sha256": str(summary_sha256),
+        "core": core,
+        "try1_dir": str(out1),
+        "try2_dir": str(out2),
+    }
+    print(json.dumps(out, ensure_ascii=False, indent=2, sort_keys=True))
+
+
+if __name__ == "__main__":
+    main()
+
