--- patches/v60_base/engine.py	2026-01-11 15:53:15
+++ atos_core/engine.py	2026-01-11 15:53:46
@@ -2217,9 +2217,36 @@
 
             env: Dict[str, Any] = {}
             out_val: Any = None
-            for ins in act.program:
+            for ins_idx, ins in enumerate(act.program):
                 op = str(ins.op)
                 args = dict(ins.args or {})
+                if len(events) < int(max_events):
+                    ev: Dict[str, Any] = {
+                        "t": "INS",
+                        "step": int(step),
+                        "depth": int(depth),
+                        "concept_id": str(concept_id),
+                        "ins_idx": int(ins_idx),
+                        "op": str(op),
+                    }
+                    if op == "CSV_GET_INPUT":
+                        ev["name"] = str(args.get("name") or "")
+                        ev["out"] = str(args.get("out") or ev["name"])
+                    elif op == "CSV_CONST":
+                        ev["out"] = str(args.get("out") or "")
+                    elif op == "CSV_PRIMITIVE":
+                        ev["fn"] = str(args.get("fn") or "")
+                        ins_in = args.get("in", [])
+                        ev["in"] = list(ins_in) if isinstance(ins_in, list) else []
+                        ev["out"] = str(args.get("out") or "")
+                    elif op == "CSV_CALL":
+                        ev["callee"] = str(args.get("concept_id") or "")
+                        ev["out"] = str(args.get("out") or "")
+                        bind = args.get("bind", {})
+                        ev["bind"] = dict(bind) if isinstance(bind, dict) else {}
+                    elif op == "CSV_RETURN":
+                        ev["var"] = str(args.get("var") or "")
+                    events.append(ev)
 
                 if op == "CSV_GET_INPUT":
                     name = str(args.get("name") or "")
--- patches/v60_base/ethics.py	2026-01-11 15:53:15
+++ atos_core/ethics.py	2026-01-11 15:54:13
@@ -71,6 +71,9 @@
     [
         r"\b(como\s+matar|como\s+me\s+matar|suic[ií]dio)\b",
         r"\b(instru[cç][oõ]es\s+para\s+fazer\s+uma\s+bomba)\b",
+        r"\b(como\s+fazer\s+uma\s+bomba|como\s+fabricar\s+uma\s+bomba)\b",
+        r"\b(how\s+to\s+make\s+a\s+bomb|how\s+to\s+build\s+a\s+bomb)\b",
+        r"\b(how\s+to\s+kill\s+someone)\b",
     ]
 )
 
@@ -78,10 +81,26 @@
     [
         r"\b(invoca[cç][aã]o|evoca[cç][aã]o)\b.*\b(esp[ií]rito|dem[oô]nio)\b",
         r"\b(fa[cç]a\s+um\s+ritual|ritual\s+oculto)\b",
+        r"\b(summon\s+a\s+demon|occult\s+ritual)\b",
     ]
 )
 
+# LO-12: protect knowledge / prevent leaking obvious credentials or private keys (narrow patterns).
+_LO12_SECRET_PATTERNS = _regex_any(
+    [
+        r"-----BEGIN (?:RSA )?PRIVATE KEY-----",
+        r"\bAWS_SECRET_ACCESS_KEY\b",
+        r"\bAKIA[0-9A-Z]{16}\b",
+        r"\bASIA[0-9A-Z]{16}\b",
+        r"\bapi[_-]?key\s*=\s*[A-Za-z0-9_-]{16,}\b",
+        r"\bsecret[_-]?key\s*=\s*[A-Za-z0-9_-]{16,}\b",
+        r"\bsk-[A-Za-z0-9]{20,}\b",
+        r"\bghp_[A-Za-z0-9]{30,}\b",
+        r"\bxoxb-[0-9A-Za-z-]{10,}\b",
+    ]
+)
 
+
 def fail_closed_text(verdict: EthicsVerdict) -> str:
     laws = ",".join(verdict.violated_laws) if verdict.violated_laws else "LO"
     return f"[BLOQUEADO_POR_ÉTICA:{laws}]"
@@ -98,6 +117,8 @@
         violated.append("LO-06")
     if any(p.search(t) for p in _LO09_OCCULT_PATTERNS):
         violated.append("LO-09")
+    if any(p.search(t) for p in _LO12_SECRET_PATTERNS):
+        violated.append("LO-12")
 
     if violated:
         return EthicsVerdict(
@@ -172,4 +193,3 @@
         if not t.ok:
             return EthicsVerdict(False, "lo_violation_in_emission", t.violated_laws, t.required_evidence)
     return EthicsVerdict(True, "ok", [], [])
-
--- patches/v60_base/suite.py	2026-01-11 15:53:15
+++ atos_core/suite.py	2026-01-11 15:54:44
@@ -1,6 +1,7 @@
 from __future__ import annotations
 
 import json
+import os
 import re
 from collections import Counter
 from typing import Any, Callable, Dict, Iterable, List, Optional, Sequence, Tuple
@@ -702,6 +703,7 @@
     template_ngram_n: int = 6,
     template_prefix_window: int = 32,
     csv: Any = None,
+    goal_shadow_log_path: Optional[str] = None,
 ) -> Tuple[List[Dict[str, Any]], Dict[str, float]]:
     transcripts: List[Dict[str, Any]] = []
     all_gen_tokens: List[str] = []
@@ -764,6 +766,46 @@
                 except Exception:
                     pass
                 csv_step += 1
+
+            # Shadow-only: evaluate persistent goals after the normal chat turn and log,
+            # without affecting token generation. Opt-in by providing a log path.
+            if goal_shadow_log_path:
+                try:
+                    os.makedirs(os.path.dirname(goal_shadow_log_path) or ".", exist_ok=True)
+                    goals = []
+                    try:
+                        goals_fn = getattr(engine.store, "goal_acts", None)
+                        if callable(goals_fn):
+                            goals = list(goals_fn())
+                    except Exception:
+                        goals = []
+                    goals = [g for g in goals if getattr(g, "active", True)]
+                    goals.sort(key=lambda a: str(getattr(a, "id", "")))
+
+                    ctx_sig = f"chat␟d={i}␟t={j}"
+                    rows: List[Dict[str, Any]] = []
+                    for g in goals:
+                        gr = engine.execute_goal(goal_act_id=str(g.id), step=int(csv_step), max_depth=8)
+                        tr = gr.get("trace") if isinstance(gr, dict) else {}
+                        tr = tr if isinstance(tr, dict) else {}
+                        rows.append(
+                            {
+                                "ctx_sig": str(ctx_sig),
+                                "goal_id": str(g.id),
+                                "goal_active": True,
+                                "goal_satisfied": bool(tr.get("goal_satisfied", False)),
+                                "goal_progress": float(tr.get("goal_progress", 0.0) or 0.0),
+                                "selected_concept_id": str(tr.get("selected_concept_id") or ""),
+                                "reason": str(gr.get("reason") or ""),
+                            }
+                        )
+                    if rows:
+                        with open(goal_shadow_log_path, "a", encoding="utf-8") as f:
+                            for row in rows:
+                                f.write(json.dumps(row, ensure_ascii=False, sort_keys=True, separators=(",", ":")))
+                                f.write("\n")
+                except Exception:
+                    pass
 
             sig = reply_signature(resp)
             reply_sigs.append(sig)
--- /dev/null	2026-01-11 15:59:22
+++ atos_core/csv_miner.py	2026-01-11 15:56:20
@@ -0,0 +1,380 @@
+from __future__ import annotations
+
+import json
+import os
+from dataclasses import dataclass
+from typing import Any, Dict, Iterator, List, Optional, Sequence, Tuple
+
+from .act import Act, Instruction, canonical_json_dumps, deterministic_iso, sha256_hex
+from .concepts import PRIMITIVE_OPS
+
+
+def _read_jsonl(path: str) -> Iterator[Dict[str, Any]]:
+    with open(path, "r", encoding="utf-8") as f:
+        for line in f:
+            line = line.strip()
+            if not line:
+                continue
+            yield json.loads(line)
+
+
+def _value_to_text(v: Any) -> str:
+    if isinstance(v, (dict, list, tuple)):
+        return canonical_json_dumps(v)
+    if v is None:
+        return ""
+    return str(v)
+
+
+@dataclass(frozen=True)
+class CsvCandidate:
+    candidate_sig: str
+    ops: List[Dict[str, Any]]  # canonicalized: [{"fn","in":[...],"out":...}, ...]
+    input_schema: Dict[str, str]
+    output_type: str
+    validator_id: str
+    count: int
+    contexts_distinct: int
+    gain_bits_est: int
+    # Examples used for PCC test vectors (already canonical and deterministic).
+    examples: List[Dict[str, Any]]
+
+    def to_dict(self) -> Dict[str, Any]:
+        return {
+            "candidate_sig": str(self.candidate_sig),
+            "ops": list(self.ops),
+            "input_schema": dict(self.input_schema),
+            "output_type": str(self.output_type),
+            "validator_id": str(self.validator_id),
+            "count": int(self.count),
+            "contexts_distinct": int(self.contexts_distinct),
+            "gain_bits_est": int(self.gain_bits_est),
+            "examples": list(self.examples),
+        }
+
+
+def _infer_validator_id(*, ops: List[Dict[str, Any]], output_type: str) -> str:
+    if str(output_type) == "int":
+        return "int_value_exact"
+    if ops:
+        fns = [str(o.get("fn") or "") for o in ops]
+        if fns and fns[-1] == "json_canonical" and "make_dict_ab" in fns:
+            return "json_ab_int_exact"
+    return ""
+
+
+def _segment_signature(ops: List[Dict[str, Any]], input_schema: Dict[str, str], output_type: str) -> str:
+    body = {"ops": ops, "input_schema": input_schema, "output_type": str(output_type)}
+    return sha256_hex(canonical_json_dumps(body).encode("utf-8"))
+
+
+def _alpha_rename_segment(
+    segment: List[Tuple[str, List[str], str]],
+    required_vars: List[str],
+) -> Tuple[List[Dict[str, Any]], Dict[str, str]]:
+    """
+    Canonicalize variable names (alpha-renaming) for stable candidate signatures.
+    Returns (ops, varmap_original_to_canon).
+    """
+    varmap: Dict[str, str] = {}
+    for idx, v in enumerate(required_vars):
+        varmap[str(v)] = f"in{idx}"
+    for idx, (_, _, out_v) in enumerate(segment):
+        varmap[str(out_v)] = f"v{idx}"
+    ops: List[Dict[str, Any]] = []
+    for fn, ins, out_v in segment:
+        ops.append(
+            {
+                "fn": str(fn),
+                "in": [str(varmap.get(str(x), str(x))) for x in ins],
+                "out": str(varmap.get(str(out_v), str(out_v))),
+            }
+        )
+    return ops, varmap
+
+
+def _execute_ops_for_expected(
+    *, ops: List[Dict[str, Any]], inputs_by_canon_var: Dict[str, Any]
+) -> Tuple[Any, str, str]:
+    env: Dict[str, Any] = dict(inputs_by_canon_var)
+    out_var = ""
+    for op in ops:
+        fn_id = str(op.get("fn") or "")
+        spec_fn = PRIMITIVE_OPS.get(fn_id)
+        if spec_fn is None:
+            raise KeyError(f"unknown_primitive:{fn_id}")
+        spec, fn = spec_fn
+        in_vars = op.get("in", [])
+        if not isinstance(in_vars, list):
+            in_vars = []
+        vals = [env.get(str(v)) for v in in_vars]
+        if int(spec.arity) != int(len(vals)):
+            raise ValueError(f"arity_mismatch:{fn_id}:{spec.arity}:{len(vals)}")
+        out = fn(*vals) if int(spec.arity) > 1 else fn(vals[0])
+        out_var = str(op.get("out") or "")
+        env[out_var] = out
+    out_val = env.get(out_var)
+    out_text = _value_to_text(out_val)
+    return out_val, out_text, sha256_hex(out_text.encode("utf-8"))
+
+
+def mine_csv_candidates(
+    exec_jsonl_path: str,
+    *,
+    min_ops: int = 2,
+    max_ops: int = 6,
+    bits_per_op: int = 128,
+    overhead_bits: int = 1024,
+    max_examples_per_candidate: int = 10,
+) -> List[CsvCandidate]:
+    """
+    Deterministic miner for primitive-op subsegments.
+
+    Input: csv_exec.jsonl (records with events: GET_INPUT + PRIMITIVE + RETURN).
+    Output: ranked candidates with inferred interface + examples for PCC.
+    """
+    min_ops = max(1, int(min_ops))
+    max_ops = max(min_ops, int(max_ops))
+    bits_per_op = max(1, int(bits_per_op))
+    overhead_bits = max(0, int(overhead_bits))
+
+    agg: Dict[str, Dict[str, Any]] = {}
+
+    for rec in _read_jsonl(exec_jsonl_path):
+        ctx_sig = str(rec.get("ctx_sig") or "")
+        inputs = rec.get("inputs", {})
+        if not isinstance(inputs, dict):
+            inputs = {}
+        events = rec.get("events", [])
+        if not isinstance(events, list):
+            continue
+
+        var_origin: Dict[str, str] = {}
+        prims: List[Tuple[str, List[str], str]] = []
+        for ev in events:
+            if not isinstance(ev, dict):
+                continue
+            t = str(ev.get("t") or "")
+            if t in {"GET_INPUT", "I"}:
+                name = str(ev.get("name") or "")
+                out = str(ev.get("out") or "")
+                if out and name:
+                    var_origin[out] = name
+            elif t in {"PRIMITIVE", "P"}:
+                fn_id = str(ev.get("fn") or "")
+                ins = ev.get("in", [])
+                if not isinstance(ins, list):
+                    ins = []
+                out = str(ev.get("out") or "")
+                if fn_id and out:
+                    prims.append((fn_id, [str(x) for x in ins], out))
+
+        if not prims:
+            continue
+
+        for s in range(len(prims)):
+            for L in range(min_ops, max_ops + 1):
+                if s + L > len(prims):
+                    break
+                segment = prims[s : s + L]
+
+                required_vars: List[str] = []
+                defined: set = set()
+                var_types: Dict[str, str] = {}
+                ok = True
+                for fn_id, ins, out in segment:
+                    spec_fn = PRIMITIVE_OPS.get(fn_id)
+                    if spec_fn is None:
+                        ok = False
+                        break
+                    spec = spec_fn[0]
+                    if int(spec.arity) != int(len(ins)):
+                        ok = False
+                        break
+                    for idx, v in enumerate(ins):
+                        want_t = str(spec.input_types[idx])
+                        if v not in defined:
+                            if v not in required_vars:
+                                required_vars.append(v)
+                        prev_t = var_types.get(v)
+                        if prev_t and prev_t != want_t:
+                            ok = False
+                            break
+                        var_types[v] = want_t
+                    if not ok:
+                        break
+                    var_types[str(out)] = str(spec.output_type)
+                    defined.add(str(out))
+                if not ok:
+                    continue
+
+                out_var = str(segment[-1][2])
+                output_type = str(var_types.get(out_var) or "")
+                if not output_type:
+                    continue
+
+                # Build input schema via origin names if present; reject if missing values.
+                input_schema: Dict[str, str] = {}
+                input_bindings: List[Tuple[str, str]] = []  # (origin_name, canon_var)
+                missing = False
+                for idx, v in enumerate(required_vars):
+                    origin = str(var_origin.get(str(v)) or str(v))
+                    if origin not in inputs:
+                        missing = True
+                        break
+                    input_schema[origin] = str(var_types.get(str(v)) or "str")
+                    input_bindings.append((origin, f"in{idx}"))
+                if missing:
+                    continue
+
+                ops, varmap = _alpha_rename_segment(segment, required_vars)
+                validator_id = _infer_validator_id(ops=ops, output_type=output_type)
+                if not validator_id:
+                    continue
+
+                sig = _segment_signature(ops, input_schema, output_type)
+                st = agg.get(sig)
+                if st is None:
+                    st = {
+                        "ops": ops,
+                        "input_schema": dict(input_schema),
+                        "output_type": str(output_type),
+                        "validator_id": str(validator_id),
+                        "count": 0,
+                        "ctx": set(),
+                        "examples": [],
+                    }
+                    agg[sig] = st
+                st["count"] = int(st["count"]) + 1
+                st["ctx"].add(ctx_sig)
+
+                # Example for PCC test vectors.
+                if len(st["examples"]) < int(max_examples_per_candidate):
+                    inps_by_canon: Dict[str, Any] = {}
+                    for origin, canon_var in input_bindings:
+                        inps_by_canon[str(canon_var)] = inputs.get(origin)
+                    out_val, out_text, out_sig = _execute_ops_for_expected(
+                        ops=ops, inputs_by_canon_var=inps_by_canon
+                    )
+                    # Deterministic expected: prefer structured expected when possible.
+                    expected: Any = out_val
+                    if str(output_type) == "str" and validator_id == "json_ab_int_exact":
+                        # If the output is JSON text, expected should be the dict form when possible.
+                        # (We can reconstruct via executing primitives before json_canonical.)
+                        try:
+                            expected = json.loads(out_text)
+                        except Exception:
+                            expected = out_text
+                    st["examples"].append(
+                        {
+                            "ctx_sig": str(ctx_sig),
+                            "inputs": {origin: inputs.get(origin) for origin, _ in input_bindings},
+                            "expected": expected,
+                            "expected_output_text": str(out_text),
+                            "expected_sig": str(out_sig),
+                        }
+                    )
+
+    out: List[CsvCandidate] = []
+    for sig, st in agg.items():
+        count = int(st["count"])
+        contexts_distinct = int(len(st["ctx"]))
+        ops = list(st["ops"])
+        gain_bits_est = int(count) * int(len(ops)) * int(bits_per_op) - int(overhead_bits)
+        out.append(
+            CsvCandidate(
+                candidate_sig=str(sig),
+                ops=ops,
+                input_schema=dict(st["input_schema"]),
+                output_type=str(st["output_type"]),
+                validator_id=str(st["validator_id"]),
+                count=count,
+                contexts_distinct=contexts_distinct,
+                gain_bits_est=gain_bits_est,
+                examples=list(st["examples"]),
+            )
+        )
+
+    out.sort(
+        key=lambda c: (
+            -int(c.gain_bits_est),
+            -int(c.contexts_distinct),
+            str(c.candidate_sig),
+        )
+    )
+    return out
+
+
+def materialize_concept_act_from_candidate(
+    cand: CsvCandidate,
+    *,
+    step: int,
+    store_content_hash_excluding_semantic: str,
+    title: str,
+    overhead_bits: int = 1024,
+    meta: Optional[Dict[str, Any]] = None,
+) -> Act:
+    """
+    Turn a mined CsvCandidate into a concept_csv Act (without PCC certificate).
+    """
+    # Input schema order is stable for determinism.
+    in_keys = sorted(list(cand.input_schema.keys()))
+    program: List[Instruction] = []
+    for idx, name in enumerate(in_keys):
+        program.append(Instruction("CSV_GET_INPUT", {"name": str(name), "out": f"in{idx}"}))
+    for op in cand.ops:
+        program.append(
+            Instruction(
+                "CSV_PRIMITIVE",
+                {
+                    "fn": str(op.get("fn") or ""),
+                    "in": list(op.get("in") or []),
+                    "out": str(op.get("out") or ""),
+                },
+            )
+        )
+    if cand.ops:
+        program.append(Instruction("CSV_RETURN", {"var": str(cand.ops[-1].get("out") or "")}))
+
+    interface = {
+        "input_schema": dict(cand.input_schema),
+        "output_schema": {"value": str(cand.output_type)},
+        "validator_id": str(cand.validator_id),
+    }
+    ev = {
+        "name": "concept_csv_mined_v60",
+        "interface": interface,
+        "meta": {
+            "title": str(title),
+            "builder": "csv_miner_v60",
+            "trained_on_store_content_hash": str(store_content_hash_excluding_semantic),
+            "candidate_sig": str(cand.candidate_sig),
+            "gain_bits_est": int(cand.gain_bits_est),
+            "contexts_distinct": int(cand.contexts_distinct),
+            "count": int(cand.count),
+            **(dict(meta or {})),
+        },
+    }
+    act_body_for_id = {
+        "kind": "concept_csv",
+        "version": 1,
+        "match": {},
+        "program": [ins.to_dict() for ins in program],
+        "evidence": ev,
+        "deps": [],
+        "active": True,
+    }
+    act_id = f"act_concept_csv_{sha256_hex(canonical_json_dumps(act_body_for_id).encode('utf-8'))[:12]}"
+    return Act(
+        id=act_id,
+        version=1,
+        created_at=deterministic_iso(step=int(step)),
+        kind="concept_csv",
+        match={},
+        program=program,
+        evidence=ev,
+        cost={"overhead_bits": int(overhead_bits)},
+        deps=[],
+        active=True,
+    )
+
--- /dev/null	2026-01-11 15:59:22
+++ atos_core/proof.py	2026-01-11 16:00:33
@@ -0,0 +1,218 @@
+from __future__ import annotations
+
+import copy
+from dataclasses import dataclass
+from typing import Any, Dict, List, Optional, Tuple
+
+from .act import Act, canonical_json_dumps, sha256_hex
+from .engine import Engine, EngineConfig
+from .ethics import validate_act_for_promotion
+from .store import ActStore
+from .validators import run_validator
+
+
+@dataclass(frozen=True)
+class ProofVerdict:
+    ok: bool
+    reason: str
+    details: Dict[str, Any]
+
+    def to_dict(self) -> Dict[str, Any]:
+        return {"ok": bool(self.ok), "reason": str(self.reason), "details": dict(self.details)}
+
+
+def _sha256_text(s: str) -> str:
+    return sha256_hex(str(s).encode("utf-8"))
+
+
+def program_sha256(act: Act) -> str:
+    prog = [ins.to_dict() for ins in (act.program or [])]
+    return sha256_hex(canonical_json_dumps(prog).encode("utf-8"))
+
+
+def certificate_body_sha256(cert: Dict[str, Any]) -> str:
+    body = dict(cert)
+    body.pop("hashes", None)
+    return sha256_hex(canonical_json_dumps(body).encode("utf-8"))
+
+
+def certificate_sha256(cert: Dict[str, Any]) -> str:
+    c = copy.deepcopy(cert)
+    hashes = c.get("hashes")
+    if not isinstance(hashes, dict):
+        hashes = {}
+        c["hashes"] = hashes
+    hashes["certificate_sha256"] = ""
+    # act_body_sha256 is computed with placeholder semantics; exclude it from the cert hash to
+    # avoid circularity (it is verified independently).
+    hashes["act_body_sha256"] = ""
+    return sha256_hex(canonical_json_dumps(c).encode("utf-8"))
+
+
+def act_body_sha256_placeholder(act: Act) -> str:
+    d = act.to_dict()
+    ev = d.get("evidence")
+    if isinstance(ev, dict):
+        cert = ev.get("certificate_v1")
+        if isinstance(cert, dict):
+            hashes = cert.get("hashes")
+            if isinstance(hashes, dict):
+                hashes = dict(hashes)
+                hashes["act_body_sha256"] = ""
+                cert = dict(cert)
+                cert["hashes"] = hashes
+                ev = dict(ev)
+                ev["certificate_v1"] = cert
+                d["evidence"] = ev
+    return sha256_hex(canonical_json_dumps(d).encode("utf-8"))
+
+
+def build_concept_pcc_certificate_v1(
+    act: Act,
+    *,
+    mined_from: Dict[str, Any],
+    test_vectors: List[Dict[str, Any]],
+    ethics_verdict: Dict[str, Any],
+    uncertainty_policy: str = "no_ic",
+) -> Dict[str, Any]:
+    """
+    Build a deterministic Proof-Carrying Concept certificate (PCC) with stable hashes.
+    """
+    iface = {}
+    ev = act.evidence if isinstance(act.evidence, dict) else {}
+    iface = ev.get("interface") if isinstance(ev, dict) else {}
+    iface = iface if isinstance(iface, dict) else {}
+    interface = {
+        "input_schema": dict(iface.get("input_schema", {})),
+        "output_schema": dict(iface.get("output_schema", {})),
+        "validator_id": str(iface.get("validator_id") or ""),
+        "iface_sig": _sha256_text(
+            canonical_json_dumps(
+                {
+                    "in": iface.get("input_schema", {}),
+                    "out": iface.get("output_schema", {}),
+                    "validator_id": iface.get("validator_id", ""),
+                }
+            )
+        ),
+    }
+
+    cert: Dict[str, Any] = {
+        "schema_version": 1,
+        "mined_from": dict(mined_from),
+        "interface": interface,
+        "test_vectors": list(test_vectors),
+        "validator_results": [],
+        "ethics_verdict": dict(ethics_verdict),
+        "uncertainty_policy": str(uncertainty_policy),
+        "hashes": {},
+    }
+
+    # Pre-fill deterministic hashes (placeholders for act_body_sha256).
+    hashes: Dict[str, Any] = {
+        "program_sha256": str(program_sha256(act)),
+        "certificate_body_sha256": str(certificate_body_sha256(cert)),
+        "certificate_sha256": "",
+        "act_body_sha256": "",
+    }
+    cert["hashes"] = hashes
+    hashes["certificate_sha256"] = str(certificate_sha256(cert))
+    return cert
+
+
+def verify_concept_pcc(act: Act, store: ActStore) -> ProofVerdict:
+    if str(getattr(act, "kind", "")) != "concept_csv":
+        return ProofVerdict(False, "wrong_kind", {"kind": str(getattr(act, "kind", ""))})
+
+    ev = act.evidence if isinstance(act.evidence, dict) else {}
+    cert = ev.get("certificate_v1") if isinstance(ev, dict) else None
+    if not isinstance(cert, dict):
+        return ProofVerdict(False, "missing_certificate", {})
+    if int(cert.get("schema_version", 0) or 0) != 1:
+        return ProofVerdict(False, "bad_schema_version", {"schema_version": cert.get("schema_version")})
+
+    hashes = cert.get("hashes")
+    if not isinstance(hashes, dict):
+        return ProofVerdict(False, "missing_hashes", {})
+
+    want_prog = str(hashes.get("program_sha256") or "")
+    got_prog = program_sha256(act)
+    if want_prog != got_prog:
+        return ProofVerdict(False, "program_sha256_mismatch", {"want": want_prog, "got": got_prog})
+
+    want_body = str(hashes.get("certificate_body_sha256") or "")
+    got_body = certificate_body_sha256(cert)
+    if want_body != got_body:
+        return ProofVerdict(False, "certificate_body_sha256_mismatch", {"want": want_body, "got": got_body})
+
+    want_cert = str(hashes.get("certificate_sha256") or "")
+    got_cert = certificate_sha256(cert)
+    if want_cert != got_cert:
+        return ProofVerdict(False, "certificate_sha256_mismatch", {"want": want_cert, "got": got_cert})
+
+    want_act = str(hashes.get("act_body_sha256") or "")
+    got_act = act_body_sha256_placeholder(act)
+    if want_act and want_act != got_act:
+        return ProofVerdict(False, "act_body_sha256_mismatch", {"want": want_act, "got": got_act})
+
+    # Ethics must pass for promotion.
+    ethics = validate_act_for_promotion(act)
+    if not bool(ethics.ok):
+        return ProofVerdict(False, "ethics_fail_closed", ethics.to_dict())
+
+    iface = cert.get("interface") if isinstance(cert.get("interface"), dict) else {}
+    validator_id = str(iface.get("validator_id") or "")
+    if not validator_id:
+        return ProofVerdict(False, "missing_validator_id", {})
+
+    test_vectors = cert.get("test_vectors")
+    if not isinstance(test_vectors, list) or len(test_vectors) < 1:
+        return ProofVerdict(False, "missing_test_vectors", {})
+
+    # Verify vectors by executing the concept with the exact store + act present.
+    store2 = ActStore(acts=dict(store.acts), next_id_int=int(store.next_id_int))
+    if store2.get(act.id) is None:
+        store2.add(act)
+    engine = Engine(store2, seed=0, config=EngineConfig())
+
+    results: List[Dict[str, Any]] = []
+    for vec in test_vectors:
+        if not isinstance(vec, dict):
+            return ProofVerdict(False, "bad_vector", {"vector": vec})
+        inputs = vec.get("inputs")
+        if not isinstance(inputs, dict):
+            return ProofVerdict(False, "bad_vector_inputs", {"vector": vec})
+        expected = vec.get("expected")
+        expected_text = str(vec.get("expected_output_text") or "")
+
+        out = engine.execute_concept_csv(concept_act_id=act.id, inputs=dict(inputs), expected=expected, step=0)
+        meta = out.get("meta") if isinstance(out, dict) else {}
+        meta = meta if isinstance(meta, dict) else {}
+        ok = bool(meta.get("ok", False))
+        out_text = str(meta.get("output_text") or "")
+        eth = meta.get("ethics") if isinstance(meta.get("ethics"), dict) else {}
+        unc = meta.get("uncertainty") if isinstance(meta.get("uncertainty"), dict) else {}
+        unc_mode = str(unc.get("mode_out") or "")
+
+        vres = run_validator(validator_id, out_text, expected)
+        results.append(
+            {
+                "ok": ok,
+                "out_text": out_text,
+                "expected_text": expected_text,
+                "validator_passed": bool(vres.passed),
+                "validator_reason": str(vres.reason),
+                "ethics_ok": bool(eth.get("ok", True)),
+                "uncertainty_mode_out": unc_mode,
+            }
+        )
+        if out_text != expected_text:
+            return ProofVerdict(False, "vector_output_text_mismatch", {"result": results[-1]})
+        if not bool(vres.passed):
+            return ProofVerdict(False, "vector_validator_failed", {"result": results[-1]})
+        if not bool(eth.get("ok", True)):
+            return ProofVerdict(False, "vector_ethics_failed", {"result": results[-1]})
+        if unc_mode == "IC":
+            return ProofVerdict(False, "vector_uncertainty_ic", {"result": results[-1]})
+
+    return ProofVerdict(True, "ok", {"vectors_verified": len(results)})
--- /dev/null	2026-01-11 15:59:22
+++ scripts/csv_mine_end2end_v60.py	2026-01-11 15:59:21
@@ -0,0 +1,511 @@
+#!/usr/bin/env python3
+from __future__ import annotations
+
+import argparse
+import hashlib
+import json
+import os
+import sys
+from typing import Any, Dict, List, Optional, Tuple
+
+sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))
+
+from atos_core.act import Act, Patch, canonical_json_dumps, deterministic_iso, sha256_hex
+from atos_core.concepts import PRIMITIVE_OPS
+from atos_core.csv_miner import CsvCandidate, materialize_concept_act_from_candidate, mine_csv_candidates
+from atos_core.engine import Engine, EngineConfig
+from atos_core.ethics import validate_act_for_promotion
+from atos_core.ledger import Ledger
+from atos_core.proof import act_body_sha256_placeholder, build_concept_pcc_certificate_v1, verify_concept_pcc
+from atos_core.store import ActStore
+from atos_core.suite import CHAT_DIALOGUES_20X3, run_chat_suite
+
+
+def sha256_file(path: str) -> str:
+    h = hashlib.sha256()
+    with open(path, "rb") as f:
+        while True:
+            b = f.read(1024 * 1024)
+            if not b:
+                break
+            h.update(b)
+    return h.hexdigest()
+
+
+def _fail(msg: str, *, code: int = 2) -> None:
+    print(msg, file=sys.stderr)
+    raise SystemExit(code)
+
+
+def ensure_absent(path: str) -> None:
+    if os.path.exists(path):
+        _fail(f"ERROR: path already exists: {path}")
+
+
+def stable_act_id(prefix: str, body: Dict[str, Any]) -> str:
+    return f"{prefix}{sha256_hex(canonical_json_dumps(body).encode('utf-8'))[:12]}"
+
+
+def make_goal_act(
+    *,
+    step: int,
+    store_hash_excl_semantic: str,
+    title: str,
+    iface_sig: str,
+    inputs: Dict[str, Any],
+    expected: Any,
+    priority: int,
+) -> Act:
+    ev = {
+        "name": "goal_v0",
+        "meta": {
+            "title": str(title),
+            "trained_on_store_content_hash": str(store_hash_excl_semantic),
+        },
+        "goal": {
+            "priority": int(priority),
+            "selector": {"kind": "interface_sig", "iface_sig": str(iface_sig)},
+            "inputs": dict(inputs),
+            "expected": expected,
+        },
+    }
+    body = {
+        "kind": "goal",
+        "version": 1,
+        "match": {},
+        "program": [],
+        "evidence": ev,
+        "deps": [],
+        "active": True,
+    }
+    act_id = stable_act_id("act_goal_", body)
+    return Act(
+        id=act_id,
+        version=1,
+        created_at=deterministic_iso(step=int(step)),
+        kind="goal",
+        match={},
+        program=[],
+        evidence=ev,
+        cost={"overhead_bits": 1024},
+        deps=[],
+        active=True,
+    )
+
+
+def _iface_sig_from_act(act: Act) -> str:
+    ev = act.evidence if isinstance(act.evidence, dict) else {}
+    iface = ev.get("interface") if isinstance(ev, dict) else {}
+    iface = iface if isinstance(iface, dict) else {}
+    body = {
+        "in": iface.get("input_schema", {}),
+        "out": iface.get("output_schema", {}),
+        "validator_id": iface.get("validator_id", ""),
+    }
+    return sha256_hex(canonical_json_dumps(body).encode("utf-8"))
+
+
+def _trace_program_sig(events: List[Dict[str, Any]]) -> str:
+    return sha256_hex(canonical_json_dumps(events).encode("utf-8"))
+
+
+def _run_inline_extract_int_with_trace(*, text: str) -> Tuple[int, List[Dict[str, Any]]]:
+    _, fn_scan = PRIMITIVE_OPS["scan_digits"]
+    _, fn_d2i = PRIMITIVE_OPS["digits_to_int"]
+    events: List[Dict[str, Any]] = []
+    events.append({"t": "GET_INPUT", "name": "text", "out": "t"})
+    events.append({"t": "PRIMITIVE", "fn": "scan_digits", "in": ["t"], "out": "d"})
+    events.append({"t": "PRIMITIVE", "fn": "digits_to_int", "in": ["d"], "out": "n"})
+    events.append({"t": "RETURN", "var": "n"})
+    digits = fn_scan(text)
+    n = fn_d2i(digits)
+    return int(n), events
+
+
+def _transcript_hash(transcripts: List[Dict[str, Any]]) -> str:
+    full = "\n".join(str(t.get("full_text") or "") for t in transcripts)
+    return sha256_hex(full.encode("utf-8"))
+
+
+def write_jsonl(path: str, rows: List[Dict[str, Any]]) -> None:
+    os.makedirs(os.path.dirname(path) or ".", exist_ok=True)
+    ensure_absent(path)
+    tmp = path + ".tmp"
+    with open(tmp, "w", encoding="utf-8") as f:
+        for r in rows:
+            f.write(canonical_json_dumps(r))
+            f.write("\n")
+    os.replace(tmp, path)
+
+
+def write_promoted_acts_preserve_order(
+    *, base_acts_path: str, out_acts_path: str, appended_acts: List[Act]
+) -> str:
+    with open(base_acts_path, "rb") as f:
+        base_bytes = f.read()
+    if base_bytes and not base_bytes.endswith(b"\n"):
+        base_bytes += b"\n"
+    tail = b"".join(canonical_json_dumps(a.to_dict()).encode("utf-8") + b"\n" for a in appended_acts)
+    tmp = out_acts_path + ".tmp"
+    with open(tmp, "wb") as f:
+        f.write(base_bytes)
+        f.write(tail)
+    os.replace(tmp, out_acts_path)
+    return sha256_file(out_acts_path)
+
+
+def main() -> None:
+    ap = argparse.ArgumentParser()
+    ap.add_argument("--acts_run", required=True)
+    ap.add_argument("--out", required=True)
+    ap.add_argument("--seed", type=int, default=0)
+    ap.add_argument("--patch_diff", default="")
+    ap.add_argument("--freeze_path", default="")
+    ap.add_argument("--episodes", type=int, default=5)
+    ap.add_argument("--max_new_tokens", type=int, default=80)
+    ap.add_argument("--chat_dialogues", type=int, default=2, help="How many CHAT_DIALOGUES_20X3 to use for goal-shadow demo.")
+    args = ap.parse_args()
+
+    ensure_absent(args.out)
+    if args.freeze_path:
+        ensure_absent(args.freeze_path)
+    os.makedirs(args.out, exist_ok=False)
+
+    base_acts = os.path.join(args.acts_run, "acts.jsonl")
+    if not os.path.exists(base_acts):
+        _fail(f"ERROR: missing base acts.jsonl: {base_acts}")
+    base_acts_sha256 = sha256_file(base_acts)
+
+    store = ActStore.load_jsonl(base_acts)
+    store_hash_excl = store.content_hash(exclude_kinds=["gate_table_ctxsig", "concept_csv", "goal"])
+
+    traces_dir = os.path.join(args.out, "traces")
+    os.makedirs(traces_dir, exist_ok=False)
+    csv_exec_path = os.path.join(traces_dir, "csv_exec.jsonl")
+
+    # (a) Baseline inline mini-suite (deterministic) + trace for mining.
+    baseline_rows: List[Dict[str, Any]] = []
+    trace_rows: List[Dict[str, Any]] = []
+    episodes = max(3, int(args.episodes))
+    for i in range(episodes):
+        text = f"abc{i}123"
+        out_int, events = _run_inline_extract_int_with_trace(text=text)
+        out_text = str(int(out_int))
+        inputs = {"text": str(text)}
+        rec = {
+            "run_id": str(args.out),
+            "ctx_sig": f"inline␟extract_int␟i={i}",
+            "goal_id": f"inline_goal_extract_int_{i}",
+            "program_sig": _trace_program_sig(events),
+            "events": list(events),
+            "inputs": dict(inputs),
+            "inputs_sig": sha256_hex(canonical_json_dumps(inputs).encode("utf-8")),
+            "output_text": out_text,
+            "output_sig": sha256_hex(out_text.encode("utf-8")),
+        }
+        trace_rows.append(rec)
+        baseline_rows.append(
+            {
+                "goal_id": str(rec["goal_id"]),
+                "inputs": dict(inputs),
+                "expected": int(out_int),
+                "expected_output_text": str(out_text),
+            }
+        )
+
+    write_jsonl(csv_exec_path, trace_rows)
+    csv_exec_sha256 = sha256_file(csv_exec_path)
+
+    baseline_text = canonical_json_dumps({"baseline": baseline_rows})
+    baseline_hash = sha256_hex(baseline_text.encode("utf-8"))
+
+    # (b) Mine deterministic candidates and pick top-1.
+    candidates = mine_csv_candidates(csv_exec_path, min_ops=2, max_ops=6, bits_per_op=128, overhead_bits=1024)
+    mined_candidates_path = os.path.join(args.out, "mined_candidates.json")
+    with open(mined_candidates_path, "w", encoding="utf-8") as f:
+        f.write(json.dumps({"candidates": [c.to_dict() for c in candidates]}, ensure_ascii=False, indent=2))
+    if not candidates:
+        _fail("ERROR: miner produced 0 candidates")
+    top1 = candidates[0]
+
+    # (c) Materialize concept + build PCC certificate + verify.
+    concept = materialize_concept_act_from_candidate(
+        top1,
+        step=100,
+        store_content_hash_excluding_semantic=store_hash_excl,
+        title="mined_extract_int_v60",
+        overhead_bits=1024,
+        meta={
+            "trace_file_sha256": str(csv_exec_sha256),
+        },
+    )
+
+    # Build test vectors from examples (>=3, deterministic, unique by expected_sig).
+    uniq: set = set()
+    test_vectors: List[Dict[str, Any]] = []
+    for ex in top1.examples:
+        if not isinstance(ex, dict):
+            continue
+        sig = str(ex.get("expected_sig") or "")
+        if sig in uniq:
+            continue
+        uniq.add(sig)
+        tv = {
+            "inputs": dict(ex.get("inputs") or {}),
+            "expected": ex.get("expected"),
+            "expected_output_text": str(ex.get("expected_output_text") or ""),
+        }
+        test_vectors.append(tv)
+        if len(test_vectors) >= 3:
+            break
+    if len(test_vectors) < 3:
+        _fail("ERROR: not enough test vectors for PCC (need >=3)")
+
+    ethics = validate_act_for_promotion(concept)
+    if not bool(ethics.ok):
+        _fail(f"ERROR: mined concept fails ethics promotion: {ethics.reason}:{ethics.violated_laws}")
+
+    cert = build_concept_pcc_certificate_v1(
+        concept,
+        mined_from={
+            "trace_file": str(csv_exec_path),
+            "trace_file_sha256": str(csv_exec_sha256),
+            "store_hash_excluding_semantic": str(store_hash_excl),
+            "seed": int(args.seed),
+            "candidate_sig": str(top1.candidate_sig),
+            "ctx_sigs": [str(x.get("ctx_sig") or "") for x in top1.examples[:5] if isinstance(x, dict)],
+        },
+        test_vectors=test_vectors,
+        ethics_verdict=ethics.to_dict(),
+        uncertainty_policy="no_ic",
+    )
+    concept.evidence.setdefault("certificate_v1", cert)
+    # Fill act_body_sha256 deterministically with placeholder semantics.
+    try:
+        concept.evidence["certificate_v1"]["hashes"]["act_body_sha256"] = act_body_sha256_placeholder(concept)
+    except Exception:
+        pass
+
+    proof_v = verify_concept_pcc(concept, store)
+    if not bool(proof_v.ok):
+        _fail(f"ERROR: PCC verify failed: {proof_v.reason}:{proof_v.details}")
+
+    # (d) Promote: base + [concept] + [goals], preserve base order, WORM.
+    promo_dir = os.path.join(args.out, "promotion")
+    os.makedirs(promo_dir, exist_ok=False)
+    acts_promoted = os.path.join(promo_dir, "acts_promoted.jsonl")
+
+    concept_iface_sig = _iface_sig_from_act(concept)
+    goals: List[Act] = []
+    for i, row in enumerate(baseline_rows):
+        title = f"goal_extract_int_{i}"
+        goals.append(
+            make_goal_act(
+                step=200 + i,
+                store_hash_excl_semantic=store_hash_excl,
+                title=title,
+                iface_sig=concept_iface_sig,
+                inputs=dict(row["inputs"]),
+                expected=row["expected"],
+                priority=10,
+            )
+        )
+    appended = [concept] + goals
+    promoted_sha256 = write_promoted_acts_preserve_order(
+        base_acts_path=base_acts, out_acts_path=acts_promoted, appended_acts=appended
+    )
+
+    promotion_ledger_path = os.path.join(promo_dir, "promotion_ledger.jsonl")
+    ledger = Ledger(path=promotion_ledger_path)
+    for idx, a in enumerate(appended):
+        patch = Patch(kind="ADD_ACT", payload={"act_id": str(a.id), "kind": str(a.kind)})
+        ledger.append(
+            step=int(idx),
+            patch=patch,
+            acts_hash=str(promoted_sha256),
+            metrics={"promotion": True, "act_id": str(a.id), "kind": str(a.kind)},
+            snapshot_path=None,
+        )
+    promotion_chain_ok = ledger.verify_chain()
+
+    promotion_manifest = {
+        "base_acts_path": str(base_acts),
+        "base_acts_sha256": str(base_acts_sha256),
+        "acts_promoted_path": str(acts_promoted),
+        "acts_promoted_sha256": str(promoted_sha256),
+        "store_hash_excluding_semantic": str(store_hash_excl),
+        "csv_exec_path": str(csv_exec_path),
+        "csv_exec_sha256": str(csv_exec_sha256),
+        "mined_top1": top1.to_dict(),
+        "promoted_concept_id": str(concept.id),
+        "promoted_goal_ids": [str(g.id) for g in goals],
+        "promotion_chain_ok": bool(promotion_chain_ok),
+        "ethics": ethics.to_dict(),
+        "pcc_verify": proof_v.to_dict(),
+    }
+    promotion_manifest_path = os.path.join(promo_dir, "promotion_manifest.json")
+    with open(promotion_manifest_path, "w", encoding="utf-8") as f:
+        f.write(json.dumps(promotion_manifest, ensure_ascii=False, indent=2))
+
+    # (e) From-store via goal->concept and invariance check against baseline.
+    store2 = ActStore.load_jsonl(acts_promoted)
+    engine2 = Engine(store2, seed=int(args.seed), config=EngineConfig())
+
+    from_store_rows: List[Dict[str, Any]] = []
+    mismatch_goals = 0
+    call_depth_max = 0
+    ethics_passed = 0
+    ic_count = 0
+    for i, g in enumerate(goals):
+        r = engine2.execute_goal(goal_act_id=g.id, step=i, max_depth=8)
+        tr = r.get("trace") if isinstance(r, dict) else {}
+        tr = tr if isinstance(tr, dict) else {}
+        meta = tr.get("concept_meta") if isinstance(tr.get("concept_meta"), dict) else {}
+        eth2 = meta.get("ethics") if isinstance(meta.get("ethics"), dict) else {}
+        unc2 = meta.get("uncertainty") if isinstance(meta.get("uncertainty"), dict) else {}
+        if bool(eth2.get("ok", True)):
+            ethics_passed += 1
+        if str(unc2.get("mode_out") or "") == "IC":
+            ic_count += 1
+        evs = r.get("events") if isinstance(r, dict) else []
+        if isinstance(evs, list):
+            for ev in evs:
+                if isinstance(ev, dict):
+                    call_depth_max = max(call_depth_max, int(ev.get("depth", 0) or 0))
+        out_text = str(meta.get("output_text") or "")
+        expected_text = str(baseline_rows[i]["expected_output_text"])
+        if out_text != expected_text:
+            mismatch_goals += 1
+        from_store_rows.append(
+            {
+                "goal_id": str(g.id),
+                "ok": bool(r.get("ok", False)),
+                "output_text": out_text,
+                "expected_output_text": expected_text,
+                "selected_concept_id": str(tr.get("selected_concept_id") or ""),
+                "ethics": eth2,
+                "uncertainty": unc2,
+            }
+        )
+
+    from_store_text = canonical_json_dumps({"from_store": from_store_rows})
+    from_store_hash = sha256_hex(from_store_text.encode("utf-8"))
+
+    reuse = sum(1 for r in from_store_rows if str(r.get("selected_concept_id") or ""))
+    reuse_rate = float(reuse / max(1, len(from_store_rows)))
+
+    # (f) Goals in chat loop shadow (telemetry only, must not change tokens).
+    goal_shadow_path = os.path.join(traces_dir, "goal_shadow.jsonl")
+    chat_dialogues = CHAT_DIALOGUES_20X3[: max(0, int(args.chat_dialogues))]
+    engine_chat_a = Engine(store2, seed=int(args.seed), config=EngineConfig())
+    base_transcripts, _ = run_chat_suite(
+        engine_chat_a,
+        dialogues=chat_dialogues,
+        max_new_tokens=int(args.max_new_tokens),
+        prefix_k=8,
+        template_ngram_n=6,
+        template_prefix_window=32,
+        csv=None,
+        goal_shadow_log_path=None,
+    )
+    chat_hash_base = _transcript_hash(base_transcripts)
+
+    engine_chat_b = Engine(store2, seed=int(args.seed), config=EngineConfig())
+    shadow_transcripts, _ = run_chat_suite(
+        engine_chat_b,
+        dialogues=chat_dialogues,
+        max_new_tokens=int(args.max_new_tokens),
+        prefix_k=8,
+        template_ngram_n=6,
+        template_prefix_window=32,
+        csv=None,
+        goal_shadow_log_path=goal_shadow_path,
+    )
+    chat_hash_shadow = _transcript_hash(shadow_transcripts)
+    goal_shadow_invariance_ok = chat_hash_shadow == chat_hash_base
+
+    goal_shadow_lines = 0
+    try:
+        with open(goal_shadow_path, "r", encoding="utf-8") as f:
+            for _ in f:
+                goal_shadow_lines += 1
+    except Exception:
+        goal_shadow_lines = 0
+
+    summary = {
+        "seed": int(args.seed),
+        "goals_total": int(len(goals)),
+        "mismatch_goals": int(mismatch_goals),
+        "csv_invariance_ok": bool(mismatch_goals == 0),
+        "mined_candidates_total": int(len(candidates)),
+        "promoted_concepts_total": 1,
+        "gain_bits_est_total": int(top1.gain_bits_est),
+        "reuse_rate": float(reuse_rate),
+        "call_depth_max": int(call_depth_max),
+        "ethics_checks_passed": int(ethics_passed),
+        "uncertainty_ic_count": int(ic_count),
+        "promotion_chain_ok": bool(promotion_chain_ok),
+        "baseline_hash": str(baseline_hash),
+        "from_store_hash": str(from_store_hash),
+        "goal_shadow_invariance_ok": bool(goal_shadow_invariance_ok),
+        "goal_shadow_lines": int(goal_shadow_lines),
+        "chat_hash_base": str(chat_hash_base),
+        "chat_hash_shadow": str(chat_hash_shadow),
+    }
+
+    summary_csv = os.path.join(args.out, "summary.csv")
+    with open(summary_csv, "w", encoding="utf-8") as f:
+        f.write(
+            "seed,goals_total,mismatch_goals,csv_invariance_ok,mined_candidates_total,promoted_concepts_total,gain_bits_est_total,reuse_rate,call_depth_max,ethics_checks_passed,uncertainty_ic_count,promotion_chain_ok,goal_shadow_invariance_ok,goal_shadow_lines,baseline_hash,from_store_hash\n"
+        )
+        f.write(
+            f"{summary['seed']},{summary['goals_total']},{summary['mismatch_goals']},{int(summary['csv_invariance_ok'])},{summary['mined_candidates_total']},{summary['promoted_concepts_total']},{summary['gain_bits_est_total']},{summary['reuse_rate']},{summary['call_depth_max']},{summary['ethics_checks_passed']},{summary['uncertainty_ic_count']},{int(summary['promotion_chain_ok'])},{int(summary['goal_shadow_invariance_ok'])},{summary['goal_shadow_lines']},{summary['baseline_hash']},{summary['from_store_hash']}\n"
+        )
+    summary_json = os.path.join(args.out, "summary.json")
+    with open(summary_json, "w", encoding="utf-8") as f:
+        f.write(
+            json.dumps(
+                {
+                    "summary": summary,
+                    "baseline": baseline_rows,
+                    "from_store": from_store_rows,
+                    "promotion_manifest": promotion_manifest,
+                },
+                ensure_ascii=False,
+                indent=2,
+            )
+        )
+
+    if args.freeze_path:
+        sha: Dict[str, str] = {
+            "base_acts_jsonl": str(base_acts_sha256),
+            "csv_exec_jsonl": str(csv_exec_sha256),
+            "mined_candidates_json": str(sha256_file(mined_candidates_path)),
+            "acts_promoted_jsonl": str(promoted_sha256),
+            "promotion_ledger": str(sha256_file(promotion_ledger_path)),
+            "promotion_manifest": str(sha256_file(promotion_manifest_path)),
+            "summary_csv": str(sha256_file(summary_csv)),
+            "summary_json": str(sha256_file(summary_json)),
+            "goal_shadow_jsonl": str(sha256_file(goal_shadow_path)) if os.path.exists(goal_shadow_path) else "",
+        }
+        if args.patch_diff and os.path.exists(args.patch_diff):
+            sha["patch_diff"] = str(sha256_file(args.patch_diff))
+
+        freeze = {
+            "name": "V60_CSV_MINER_PCC_GOAL_SHADOW",
+            "acts_source_run": str(args.acts_run),
+            "out_dir": str(args.out),
+            "commands": [" ".join(sys.argv)],
+            "verify_chain": bool(promotion_chain_ok),
+            "sha256": sha,
+            "summary": summary,
+        }
+        with open(args.freeze_path, "w", encoding="utf-8") as f:
+            f.write(json.dumps(freeze, ensure_ascii=False, indent=2))
+
+    print(json.dumps({"summary": summary, "out_dir": str(args.out)}, ensure_ascii=False, indent=2))
+
+
+if __name__ == "__main__":
+    main()
+
--- /dev/null	2026-01-11 15:59:22
+++ scripts/smoke_csv_miner_v60.py	2026-01-11 16:00:00
@@ -0,0 +1,189 @@
+#!/usr/bin/env python3
+from __future__ import annotations
+
+import argparse
+import json
+import os
+import sys
+from typing import Any, Dict, List
+
+sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))
+
+from atos_core.act import Act, Instruction, canonical_json_dumps, deterministic_iso, sha256_hex
+from atos_core.csv_miner import materialize_concept_act_from_candidate, mine_csv_candidates
+from atos_core.engine import Engine, EngineConfig
+from atos_core.ethics import validate_act_for_promotion
+from atos_core.proof import (
+    act_body_sha256_placeholder,
+    build_concept_pcc_certificate_v1,
+    verify_concept_pcc,
+)
+from atos_core.store import ActStore
+
+
+def _fail(msg: str) -> None:
+    print(msg, file=sys.stderr)
+    raise SystemExit(2)
+
+
+def ensure_absent(path: str) -> None:
+    if os.path.exists(path):
+        _fail(f"ERROR: path already exists: {path}")
+
+
+def write_jsonl(path: str, rows: List[Dict[str, Any]]) -> str:
+    os.makedirs(os.path.dirname(path) or ".", exist_ok=True)
+    ensure_absent(path)
+    tmp = path + ".tmp"
+    with open(tmp, "w", encoding="utf-8") as f:
+        for r in rows:
+            f.write(canonical_json_dumps(r))
+            f.write("\n")
+    os.replace(tmp, path)
+    return sha256_hex(open(path, "rb").read())
+
+
+def make_concept_const(*, title: str, text: str) -> Act:
+    iface = {"input_schema": {}, "output_schema": {"value": "str"}, "validator_id": ""}
+    ev = {"name": "concept_csv_v0", "interface": iface, "meta": {"title": str(title)}}
+    body = {
+        "kind": "concept_csv",
+        "version": 1,
+        "match": {},
+        "program": [
+            Instruction("CSV_CONST", {"out": "s", "value": str(text)}).to_dict(),
+            Instruction("CSV_RETURN", {"var": "s"}).to_dict(),
+        ],
+        "evidence": ev,
+        "deps": [],
+        "active": True,
+    }
+    act_id = f"act_concept_csv_{sha256_hex(canonical_json_dumps(body).encode('utf-8'))[:12]}"
+    return Act(
+        id=act_id,
+        version=1,
+        created_at=deterministic_iso(step=0),
+        kind="concept_csv",
+        match={},
+        program=[Instruction("CSV_CONST", {"out": "s", "value": str(text)}), Instruction("CSV_RETURN", {"var": "s"})],
+        evidence=ev,
+        cost={"overhead_bits": 1024},
+        deps=[],
+        active=True,
+    )
+
+
+def main() -> None:
+    ap = argparse.ArgumentParser()
+    ap.add_argument("--out", required=True, help="WORM out dir (must not exist)")
+    args = ap.parse_args()
+
+    ensure_absent(args.out)
+    os.makedirs(args.out, exist_ok=False)
+
+    # (1) Create a tiny trace file and ensure miner finds scan_digits->digits_to_int.
+    traces_dir = os.path.join(args.out, "traces")
+    os.makedirs(traces_dir, exist_ok=False)
+    csv_exec = os.path.join(traces_dir, "csv_exec.jsonl")
+
+    rows: List[Dict[str, Any]] = []
+    for i, text in enumerate(["abc0123", "x9y7", "id=42"]):
+        events = [
+            {"t": "GET_INPUT", "name": "text", "out": "t"},
+            {"t": "PRIMITIVE", "fn": "scan_digits", "in": ["t"], "out": "d"},
+            {"t": "PRIMITIVE", "fn": "digits_to_int", "in": ["d"], "out": "n"},
+            {"t": "RETURN", "var": "n"},
+        ]
+        rows.append(
+            {
+                "run_id": str(args.out),
+                "ctx_sig": f"inline␟i={i}",
+                "goal_id": f"g{i}",
+                "program_sig": sha256_hex(canonical_json_dumps(events).encode("utf-8")),
+                "events": events,
+                "inputs": {"text": str(text)},
+                "inputs_sig": sha256_hex(str(text).encode("utf-8")),
+                "output_text": "",
+                "output_sig": "",
+            }
+        )
+    write_jsonl(csv_exec, rows)
+    cands = mine_csv_candidates(csv_exec, min_ops=2, max_ops=6)
+    if not cands:
+        _fail("FAIL: miner produced 0 candidates")
+    top = cands[0]
+    fns = [op.get("fn") for op in top.ops]
+    if fns != ["scan_digits", "digits_to_int"]:
+        _fail(f"FAIL: expected ops scan_digits->digits_to_int, got {fns}")
+
+    # (2) PCC verify ok for mined candidate.
+    store = ActStore()
+    concept = materialize_concept_act_from_candidate(
+        top,
+        step=1,
+        store_content_hash_excluding_semantic="dummy",
+        title="mined_extract_int_v60_smoke",
+    )
+    ethics = validate_act_for_promotion(concept)
+    if not bool(ethics.ok):
+        _fail(f"FAIL: ethics rejected mined concept: {ethics.reason}")
+    test_vectors = []
+    for ex in top.examples[:3]:
+        test_vectors.append(
+            {
+                "inputs": dict(ex.get("inputs") or {}),
+                "expected": ex.get("expected"),
+                "expected_output_text": str(ex.get("expected_output_text") or ""),
+            }
+        )
+    cert = build_concept_pcc_certificate_v1(
+        concept,
+        mined_from={"trace_file_sha256": sha256_hex(open(csv_exec, "rb").read())},
+        test_vectors=test_vectors,
+        ethics_verdict=ethics.to_dict(),
+    )
+    concept.evidence.setdefault("certificate_v1", cert)
+    concept.evidence["certificate_v1"]["hashes"]["act_body_sha256"] = act_body_sha256_placeholder(concept)
+    v = verify_concept_pcc(concept, store)
+    if not bool(v.ok):
+        _fail(f"FAIL: PCC verify should pass, got {v.reason}:{v.details}")
+
+    # (3) Tamper test: change expected_output_text -> verify must fail.
+    tampered = Act.from_dict(concept.to_dict())
+    cert2 = tampered.evidence.get("certificate_v1", {})
+    if isinstance(cert2, dict) and isinstance(cert2.get("test_vectors"), list) and cert2["test_vectors"]:
+        cert2["test_vectors"][0]["expected_output_text"] = "999999"
+    v2 = verify_concept_pcc(tampered, store)
+    if bool(v2.ok):
+        _fail("FAIL: tampered PCC should fail")
+
+    # (4) Promotion rejects concept that downgrades to IC (strong claim without evidence).
+    concept_ic = make_concept_const(title="strong_claim", text="COM CERTEZA PARIS")
+    store2 = ActStore()
+    store2.add(concept_ic)
+    eng = Engine(store2, seed=0, config=EngineConfig())
+    out = eng.execute_concept_csv(concept_act_id=concept_ic.id, inputs={}, expected=None, step=0)
+    meta = out.get("meta") if isinstance(out, dict) else {}
+    meta = meta if isinstance(meta, dict) else {}
+    unc = meta.get("uncertainty") if isinstance(meta.get("uncertainty"), dict) else {}
+    if str(unc.get("mode_out") or "") != "IC":
+        _fail(f"FAIL: expected IR->IC downgrade, got {unc}")
+
+    # (5) Ethics fail-closed blocks LO-02/LO-06 (synthetic).
+    concept_bad = make_concept_const(title="sentience", text="EU SOU CONSCIENTE")
+    store3 = ActStore()
+    store3.add(concept_bad)
+    eng2 = Engine(store3, seed=0, config=EngineConfig())
+    out_bad = eng2.execute_concept_csv(concept_act_id=concept_bad.id, inputs={}, expected=None, step=0)
+    meta_bad = out_bad.get("meta") if isinstance(out_bad, dict) else {}
+    meta_bad = meta_bad if isinstance(meta_bad, dict) else {}
+    eth_bad = meta_bad.get("ethics") if isinstance(meta_bad.get("ethics"), dict) else {}
+    if bool(eth_bad.get("ok", True)):
+        _fail(f"FAIL: expected ethics fail-closed, got {eth_bad}")
+
+    print(json.dumps({"ok": True, "top_candidate_sig": top.candidate_sig}, ensure_ascii=False))
+
+
+if __name__ == "__main__":
+    main()
+
