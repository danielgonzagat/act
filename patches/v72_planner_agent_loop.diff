--- /dev/null	2026-01-12 10:03:21
+++ atos_core/agent_loop_v72.py	2026-01-12 09:54:11
@@ -0,0 +1,424 @@
+from __future__ import annotations
+
+import copy
+import os
+from typing import Any, Dict, List, Sequence, Tuple
+
+from .act import Act, canonical_json_dumps, sha256_hex
+from .ato_v71 import ATOv71
+from .engine import Engine, EngineConfig
+from .mind_graph_v71 import MindGraphV71
+from .planner_v72 import PlanV72, PlannerV72
+from .render_v71 import render_projection
+from .store import ActStore
+from .validators import run_validator
+
+
+def _stable_hash_obj(obj: Any) -> str:
+    return sha256_hex(canonical_json_dumps(obj).encode("utf-8"))
+
+
+def _safe_deepcopy(obj: Any) -> Any:
+    try:
+        return copy.deepcopy(obj)
+    except Exception:
+        if isinstance(obj, dict):
+            return dict(obj)
+        if isinstance(obj, list):
+            return list(obj)
+        return obj
+
+
+def _concept_ato_from_act(*, act: Act, step: int, concept_state: str = "ACTIVE") -> ATOv71:
+    ev = act.evidence if isinstance(act.evidence, dict) else {}
+    iface = ev.get("interface") if isinstance(ev.get("interface"), dict) else {}
+    iface = iface if isinstance(iface, dict) else {}
+    in_schema = iface.get("input_schema") if isinstance(iface.get("input_schema"), dict) else {}
+    out_schema = iface.get("output_schema") if isinstance(iface.get("output_schema"), dict) else {}
+    validator_id = str(iface.get("validator_id") or "")
+    interface_sig = _stable_hash_obj({"in": in_schema, "out": out_schema, "validator_id": validator_id})
+    program_sha256 = _stable_hash_obj([ins.to_dict() for ins in (act.program or [])])
+    slots = {"inputs": sorted(str(k) for k in in_schema.keys()), "outputs": sorted(str(k) for k in out_schema.keys())}
+    invariants = {
+        "input_schema": {str(k): str(v) for k, v in sorted(in_schema.items(), key=lambda kv: str(kv[0]))},
+        "output_schema": {str(k): str(v) for k, v in sorted(out_schema.items(), key=lambda kv: str(kv[0]))},
+        "validator_id": str(validator_id),
+    }
+    subgraph = {
+        "interface_sig": str(interface_sig),
+        "program_sha256": str(program_sha256),
+        "program_len": int(len(act.program or [])),
+        "concept_state": str(concept_state),
+    }
+    return ATOv71(
+        ato_id=str(act.id),
+        ato_type="CONCEPT",
+        subgraph=subgraph,
+        slots=slots,
+        bindings={},
+        cost=float(len(act.program or [])),
+        evidence_refs=[{"kind": "concept_act", "act_id": str(act.id)}],
+        invariants=invariants,
+        created_step=int(step),
+        last_step=int(step),
+    )
+
+
+def _ingest_concept_calls_as_edges(
+    *,
+    mg: MindGraphV71,
+    store: ActStore,
+    step_base: int,
+    concept_calls: Sequence[Dict[str, Any]],
+) -> Dict[str, Any]:
+    """
+    Convert Engine.execute_concept_csv trace.concept_calls into MindGraph edges:
+      - ensure concept nodes exist
+      - add CALLS edges based on call_depth stack (caller -> callee)
+    """
+    calls = [c for c in concept_calls if isinstance(c, dict)]
+    created_nodes = 0
+    created_edges = 0
+    stack: Dict[int, str] = {}
+
+    for idx, ce in enumerate(calls):
+        cid = str(ce.get("concept_id") or "")
+        if not cid:
+            continue
+        if cid not in [n.get("ato_id") for n in (mg.snapshot_graph_state().get("nodes") or []) if isinstance(n, dict)]:
+            act = store.get_concept_act(cid)
+            if act is not None:
+                mg.add_node(step=int(step_base) + int(idx), ato=_concept_ato_from_act(act=act, step=int(step_base)), reason="from_concept_call")
+                created_nodes += 1
+
+        depth = int(ce.get("call_depth", 0) or 0)
+        if depth > 0 and (depth - 1) in stack:
+            caller = str(stack.get(depth - 1) or "")
+            callee = cid
+            if caller and callee and caller != callee:
+                ev_ref = {
+                    "kind": "concept_call",
+                    "idx": int(idx),
+                    "call_depth": int(depth),
+                    "bindings_sig": str(ce.get("bindings_sig") or ""),
+                    "return_sig": str(ce.get("return_sig") or ""),
+                    "concept_sig": str(ce.get("concept_sig") or ""),
+                    "interface_sig": str(ce.get("interface_sig") or ""),
+                    "program_sha256": str(ce.get("program_sha256") or ""),
+                    "bindings": ce.get("bindings") if isinstance(ce.get("bindings"), dict) else {},
+                    "return": ce.get("return") if isinstance(ce.get("return"), dict) else {},
+                }
+                mg.add_edge(
+                    step=int(step_base) + int(idx),
+                    src_ato_id=str(caller),
+                    dst_ato_id=str(callee),
+                    edge_type="CALLS",
+                    evidence_refs=[_safe_deepcopy(ev_ref)],
+                    reason="from_engine_concept_calls",
+                )
+                created_edges += 1
+
+        stack[depth] = cid
+        for k in list(stack.keys()):
+            if k > depth:
+                stack.pop(k, None)
+
+    return {"nodes_added": int(created_nodes), "edges_added": int(created_edges)}
+
+
+def run_goal_spec_v72(
+    *,
+    goal_spec,
+    store: ActStore,
+    seed: int,
+    out_dir: str,
+    max_depth: int = 6,
+    max_expansions: int = 256,
+    max_events: int = 512,
+) -> Dict[str, Any]:
+    """
+    Minimal agent loop (deterministic, auditably traced into MindGraphV71):
+      GOAL_SPEC -> PLANNER -> EXECUTOR -> MIND_GRAPH (WORM) -> RENDER/RESPONSE
+    """
+    # Prepare planner + plan.
+    planner = PlannerV72(max_depth=int(max_depth), max_expansions=int(max_expansions))
+    plan, planner_debug = planner.plan(goal_spec=goal_spec, store=store)
+    if plan is None:
+        return {"ok": False, "reason": "plan_not_found", "planner": dict(planner_debug)}
+
+    # MindGraph run dir must be WORM-absent.
+    mg_dir = f"{str(out_dir).rstrip(os.sep)}/mind_graph"
+    mg = MindGraphV71(mg_dir)
+
+    step_ctr = 0
+    goal_id = goal_spec.goal_id()
+    goal_sig = goal_spec.goal_sig()
+    plan_id = f"plan_v72_{str(plan.plan_sig)}"
+
+    # GOAL node.
+    goal_node = ATOv71(
+        ato_id=str(goal_id),
+        ato_type="GOAL",
+        subgraph={"goal_kind": str(goal_spec.goal_kind), "goal_sig": str(goal_sig), "validator_id": str(goal_spec.validator_id)},
+        slots={"output_key": str(goal_spec.output_key)},
+        bindings=_safe_deepcopy(goal_spec.bindings),
+        cost=0.0,
+        evidence_refs=[{"kind": "goal_spec_v72", "goal_sig": str(goal_sig)}],
+        invariants={"expected": goal_spec.expected, "validator_id": str(goal_spec.validator_id)},
+        created_step=int(step_ctr),
+        last_step=int(step_ctr),
+    )
+    mg.add_node(step=int(step_ctr), ato=goal_node, reason="goal_spec")
+    step_ctr += 1
+
+    # PLAN node.
+    plan_node = ATOv71(
+        ato_id=str(plan_id),
+        ato_type="PLAN",
+        subgraph={"plan_sig": str(plan.plan_sig), "planner": "planner_v72"},
+        slots={"steps_total": int(len(plan.steps))},
+        bindings={"output_key": str(goal_spec.output_key)},
+        cost=float(len(plan.steps)),
+        evidence_refs=[{"kind": "planner_debug", "state": _safe_deepcopy(planner_debug)}],
+        invariants={"plan": plan.to_dict()},
+        created_step=int(step_ctr),
+        last_step=int(step_ctr),
+    )
+    mg.add_node(step=int(step_ctr), ato=plan_node, reason="plan")
+    mg.add_edge(
+        step=int(step_ctr),
+        src_ato_id=str(goal_id),
+        dst_ato_id=str(plan_id),
+        edge_type="DEPENDS_ON",
+        evidence_refs=[{"kind": "goal_depends_on_plan", "goal_sig": str(goal_sig), "plan_sig": str(plan.plan_sig)}],
+        reason="goal_plan",
+    )
+    step_ctr += 1
+
+    # Ensure concept nodes exist for all concepts in store (stable).
+    concept_acts = store.concept_acts()
+    for act in sorted(concept_acts, key=lambda a: str(a.id)):
+        mg.add_node(step=int(step_ctr), ato=_concept_ato_from_act(act=act, step=int(step_ctr)), reason="concept_catalog")
+        step_ctr += 1
+
+    # Executor state.
+    vars_state: Dict[str, Any] = _safe_deepcopy(goal_spec.bindings)
+    engine = Engine(store, seed=int(seed), config=EngineConfig(enable_contracts=False))
+
+    all_concept_calls: List[Dict[str, Any]] = []
+    operator_ids: List[str] = []
+    state_ids: List[str] = []
+
+    for s in plan.steps:
+        # OPERATOR node.
+        op_id = f"op_v72_{str(s.step_id)}"
+        operator_ids.append(op_id)
+        op_node = ATOv71(
+            ato_id=str(op_id),
+            ato_type="OPERATOR",
+            subgraph={"kind": "CALL_CONCEPT", "concept_id": str(s.concept_id), "produces": str(s.produces), "idx": int(s.idx)},
+            slots={"bind_map": _safe_deepcopy(s.bind_map)},
+            bindings={},
+            cost=1.0,
+            evidence_refs=[{"kind": "plan_step", "step": s.to_dict()}],
+            invariants={"requires": list(sorted(s.bind_map.keys()))},
+            created_step=int(step_ctr),
+            last_step=int(step_ctr),
+        )
+        mg.add_node(step=int(step_ctr), ato=op_node, reason="operator_step")
+        mg.add_edge(
+            step=int(step_ctr),
+            src_ato_id=str(plan_id),
+            dst_ato_id=str(op_id),
+            edge_type="DEPENDS_ON",
+            evidence_refs=[{"kind": "plan_depends_on_operator", "idx": int(s.idx), "step_id": str(s.step_id)}],
+            reason="plan_ops",
+        )
+        mg.add_edge(
+            step=int(step_ctr),
+            src_ato_id=str(op_id),
+            dst_ato_id=str(s.concept_id),
+            edge_type="CALLS",
+            evidence_refs=[{"kind": "operator_calls_concept", "concept_id": str(s.concept_id)}],
+            reason="op_calls_concept",
+        )
+        step_ctr += 1
+
+        # Execute concept (no expected for intermediate steps).
+        concept_inputs: Dict[str, Any] = {}
+        for k, vname in sorted(s.bind_map.items(), key=lambda kv: str(kv[0])):
+            concept_inputs[str(k)] = vars_state.get(str(vname))
+
+        exec_res = engine.execute_concept_csv(
+            concept_act_id=str(s.concept_id),
+            inputs=dict(concept_inputs),
+            expected=None,
+            step=int(step_ctr),
+            max_depth=8,
+            max_events=int(max_events),
+            validate_output=False,
+        )
+        meta = exec_res.get("meta") if isinstance(exec_res, dict) else {}
+        meta = meta if isinstance(meta, dict) else {}
+        ok_exec = bool(meta.get("ok", False))
+        out_text = str(meta.get("output_text") or exec_res.get("output") or "")
+        out_sig = str(meta.get("output_sig") or "")
+
+        # Save variable for next steps.
+        vars_state[str(s.produces)] = out_text
+
+        # STATE node (snapshot).
+        state_snapshot = _safe_deepcopy(vars_state)
+        state_sig = _stable_hash_obj({"vars": state_snapshot, "idx": int(s.idx)})
+        state_id = f"state_v72_{state_sig}"
+        state_ids.append(state_id)
+        st_node = ATOv71(
+            ato_id=str(state_id),
+            ato_type="STATE",
+            subgraph={"idx": int(s.idx), "after_op": str(op_id)},
+            slots={"keys": sorted(str(k) for k in state_snapshot.keys())},
+            bindings=state_snapshot,
+            cost=0.0,
+            evidence_refs=[{"kind": "operator_result", "ok": bool(ok_exec), "output_sig": str(out_sig)}],
+            invariants={},
+            created_step=int(step_ctr),
+            last_step=int(step_ctr),
+        )
+        mg.add_node(step=int(step_ctr), ato=st_node, reason="state_after_step")
+        mg.add_edge(
+            step=int(step_ctr),
+            src_ato_id=str(op_id),
+            dst_ato_id=str(state_id),
+            edge_type="CAUSES",
+            evidence_refs=[
+                {
+                    "kind": "op_causes_state",
+                    "idx": int(s.idx),
+                    "produces": str(s.produces),
+                    "output_text": str(out_text),
+                    "output_sig": str(out_sig),
+                    "bind_map": _safe_deepcopy(s.bind_map),
+                }
+            ],
+            reason="op_state",
+        )
+
+        # Ingest concept_calls into CALLS edges between concept nodes.
+        tr = exec_res.get("trace") if isinstance(exec_res.get("trace"), dict) else {}
+        concept_calls = tr.get("concept_calls") if isinstance(tr.get("concept_calls"), list) else []
+        all_concept_calls.extend([_safe_deepcopy(c) for c in concept_calls if isinstance(c, dict)])
+        _ingest_concept_calls_as_edges(
+            mg=mg,
+            store=store,
+            step_base=int(step_ctr) + 1,
+            concept_calls=concept_calls,  # source may be mutated later; mg must snapshot
+        )
+        step_ctr += 2
+
+    # Final evaluation.
+    final_val = vars_state.get(str(goal_spec.output_key))
+    vres = run_validator(str(goal_spec.validator_id), str(final_val or ""), goal_spec.expected)
+    eval_ok = bool(vres.passed)
+    eval_id = f"eval_v72_{goal_sig}"
+    eval_node = ATOv71(
+        ato_id=str(eval_id),
+        ato_type="EVAL",
+        subgraph={"goal_sig": str(goal_sig), "plan_sig": str(plan.plan_sig)},
+        slots={"output_key": str(goal_spec.output_key)},
+        bindings={"ok": bool(eval_ok), "reason": str(vres.reason), "got": str(final_val or ""), "expected": goal_spec.expected},
+        cost=0.0,
+        evidence_refs=[{"kind": "validator", "validator_id": str(goal_spec.validator_id), "reason": str(vres.reason)}],
+        invariants={},
+        created_step=int(step_ctr),
+        last_step=int(step_ctr),
+    )
+    mg.add_node(step=int(step_ctr), ato=eval_node, reason="eval_final")
+    if state_ids:
+        mg.add_edge(
+            step=int(step_ctr),
+            src_ato_id=str(eval_id),
+            dst_ato_id=str(state_ids[-1]),
+            edge_type="DERIVED_FROM",
+            evidence_refs=[{"kind": "eval_from_state", "state_id": str(state_ids[-1])}],
+            reason="eval_state",
+        )
+    step_ctr += 1
+
+    # Render projection from active roots.
+    snap_before_irrelevant = mg.snapshot_graph_state()
+    graph_sig_before_irrelevant = mg.graph_sig()
+    roots = [str(goal_id), str(plan_id), str(eval_id)]
+    r_v1 = render_projection(
+        graph_snapshot=snap_before_irrelevant,
+        root_ids=roots,
+        max_depth=16,
+        bindings=_safe_deepcopy(goal_spec.bindings),
+        goals=[{"goal_id": str(goal_id), "goal_sig": str(goal_sig)}],
+        plan_state={"plan_id": str(plan_id), "plan_sig": str(plan.plan_sig)},
+        style="v1",
+    )
+    r_v2 = render_projection(
+        graph_snapshot=snap_before_irrelevant,
+        root_ids=roots,
+        max_depth=16,
+        bindings=_safe_deepcopy(goal_spec.bindings),
+        goals=[{"goal_id": str(goal_id), "goal_sig": str(goal_sig)}],
+        plan_state={"plan_id": str(plan_id), "plan_sig": str(plan.plan_sig)},
+        style="v2",
+    )
+
+    # Add an unreachable irrelevant node and re-render v1 to prove invariance (active subgraph unchanged).
+    irrelevant = ATOv71(
+        ato_id="obs_v72_irrelevant_00",
+        ato_type="OBS",
+        subgraph={"kind": "irrelevant"},
+        slots={},
+        bindings={"note": "ignored"},
+        cost=0.0,
+        evidence_refs=[{"kind": "irrelevant"}],
+        invariants={},
+        created_step=int(step_ctr),
+        last_step=int(step_ctr),
+    )
+    mg.add_node(step=int(step_ctr), ato=irrelevant, reason="irrelevant_unreachable")
+    step_ctr += 1
+
+    snap_after_irrelevant = mg.snapshot_graph_state()
+    graph_sig_after_irrelevant = mg.graph_sig()
+    r_v1_after = render_projection(
+        graph_snapshot=snap_after_irrelevant,
+        root_ids=roots,
+        max_depth=16,
+        bindings=_safe_deepcopy(goal_spec.bindings),
+        goals=[{"goal_id": str(goal_id), "goal_sig": str(goal_sig)}],
+        plan_state={"plan_id": str(plan_id), "plan_sig": str(plan.plan_sig)},
+        style="v1",
+    )
+
+    return {
+        "ok": bool(eval_ok),
+        "goal_id": str(goal_id),
+        "goal_sig": str(goal_sig),
+        "plan": plan.to_dict(),
+        "planner": dict(planner_debug),
+        "final": {
+            "output_key": str(goal_spec.output_key),
+            "got": str(final_val or ""),
+            "expected": goal_spec.expected,
+            "validator": {"passed": bool(vres.passed), "reason": str(vres.reason)},
+        },
+        "graph": {
+            "graph_sig_before_irrelevant": str(graph_sig_before_irrelevant),
+            "graph_sig": str(graph_sig_after_irrelevant),
+            "chains": mg.verify_chains(),
+        },
+        "render": {
+            "render_sig_v1": str(r_v1.get("render_sig") or ""),
+            "render_sig_v2": str(r_v2.get("render_sig") or ""),
+            "render_sig_v1_after_irrelevant": str(r_v1_after.get("render_sig") or ""),
+            "text_v1": str(r_v1.get("text") or ""),
+            "text_v2": str(r_v2.get("text") or ""),
+        },
+        "trace": {"concept_calls": list(all_concept_calls)},
+        "snapshot": snap_after_irrelevant,
+    }
--- /dev/null	2026-01-12 10:03:21
+++ atos_core/goal_spec_v72.py	2026-01-12 09:51:39
@@ -0,0 +1,92 @@
+from __future__ import annotations
+
+import copy
+import json
+import re
+from dataclasses import dataclass, field
+from typing import Any, Dict, Optional, Tuple
+
+from .act import canonical_json_dumps, sha256_hex
+
+
+def _stable_hash_obj(obj: Any) -> str:
+    return sha256_hex(canonical_json_dumps(obj).encode("utf-8"))
+
+
+_SPEC_RE = re.compile(r"^\s*V72_SPEC\s*=\s*(\{.*\})\s*$")
+
+
+@dataclass(frozen=True)
+class GoalSpecV72:
+    goal_kind: str
+    bindings: Dict[str, Any]
+    output_key: str
+    expected: Any
+    validator_id: str = "text_exact"
+    created_step: int = 0
+
+    def __post_init__(self) -> None:
+        object.__setattr__(self, "goal_kind", str(self.goal_kind or ""))
+        object.__setattr__(self, "output_key", str(self.output_key or ""))
+        object.__setattr__(self, "validator_id", str(self.validator_id or ""))
+        object.__setattr__(self, "created_step", int(self.created_step or 0))
+        b = self.bindings if isinstance(self.bindings, dict) else {}
+        try:
+            b2 = copy.deepcopy(b)
+        except Exception:
+            b2 = dict(b)
+        object.__setattr__(self, "bindings", b2 if isinstance(b2, dict) else {})
+
+    def to_canonical_dict(self) -> Dict[str, Any]:
+        return {
+            "schema_version": 1,
+            "goal_kind": str(self.goal_kind),
+            "bindings": {str(k): self.bindings.get(k) for k in sorted(self.bindings.keys(), key=str)},
+            "output_key": str(self.output_key),
+            "expected": self.expected,
+            "validator_id": str(self.validator_id),
+        }
+
+    def goal_sig(self) -> str:
+        return _stable_hash_obj(self.to_canonical_dict())
+
+    def goal_id(self) -> str:
+        return f"goal_v72_{self.goal_sig()}"
+
+
+def parse_goal_spec_from_prompt(prompt: str) -> Tuple[Optional[GoalSpecV72], str]:
+    """
+    Spec-first-line parser:
+      first line must be: V72_SPEC=<json>
+    Returns (GoalSpecV72|None, reason).
+    """
+    lines = str(prompt or "").splitlines()
+    if not lines:
+        return None, "empty_prompt"
+    m = _SPEC_RE.match(lines[0].strip())
+    if not m:
+        return None, "missing_v72_spec_first_line"
+    raw = m.group(1)
+    try:
+        obj = json.loads(raw)
+    except Exception:
+        return None, "bad_json"
+    if not isinstance(obj, dict):
+        return None, "spec_not_dict"
+
+    goal_kind = str(obj.get("goal_kind") or "goal_v72")
+    bindings = obj.get("bindings") if isinstance(obj.get("bindings"), dict) else {}
+    output_key = str(obj.get("output_key") or "")
+    expected = obj.get("expected")
+    validator_id = str(obj.get("validator_id") or "text_exact")
+    if not output_key:
+        return None, "missing_output_key"
+    return GoalSpecV72(
+        goal_kind=goal_kind,
+        bindings=dict(bindings),
+        output_key=output_key,
+        expected=expected,
+        validator_id=validator_id,
+        created_step=int(obj.get("created_step", 0) or 0),
+    ), "ok"
+
--- /dev/null	2026-01-12 10:03:21
+++ atos_core/planner_v72.py	2026-01-12 09:55:01
@@ -0,0 +1,262 @@
+from __future__ import annotations
+
+import copy
+from dataclasses import dataclass, field
+from typing import Any, Dict, Iterable, List, Optional, Sequence, Set, Tuple
+
+from .act import Act, canonical_json_dumps, sha256_hex
+from .goal_spec_v72 import GoalSpecV72
+
+
+def _stable_hash_obj(obj: Any) -> str:
+    return sha256_hex(canonical_json_dumps(obj).encode("utf-8"))
+
+
+def _sorted_keys(d: Dict[str, Any]) -> List[str]:
+    return [str(k) for k in sorted(d.keys(), key=str)]
+
+
+def _state_sig(vars_avail: Set[str]) -> str:
+    return _stable_hash_obj({"vars": sorted(set(str(v) for v in vars_avail if str(v)))})
+
+
+@dataclass(frozen=True)
+class PlanStepV72:
+    step_id: str
+    idx: int
+    concept_id: str
+    bind_map: Dict[str, str]
+    produces: str
+
+    def to_dict(self) -> Dict[str, Any]:
+        return {
+            "step_id": str(self.step_id),
+            "idx": int(self.idx),
+            "concept_id": str(self.concept_id),
+            "bind_map": {str(k): str(self.bind_map.get(k) or "") for k in sorted(self.bind_map.keys(), key=str)},
+            "produces": str(self.produces),
+        }
+
+
+@dataclass(frozen=True)
+class PlanV72:
+    steps: List[PlanStepV72]
+    plan_sig: str
+
+    def to_dict(self) -> Dict[str, Any]:
+        return {"schema_version": 1, "steps": [s.to_dict() for s in self.steps], "plan_sig": str(self.plan_sig)}
+
+
+@dataclass(frozen=True)
+class OperatorTemplateV72:
+    concept_id: str
+    input_keys: List[str]
+    output_key: str
+    validator_id: str
+
+    def to_dict(self) -> Dict[str, Any]:
+        return {
+            "concept_id": str(self.concept_id),
+            "input_keys": list(self.input_keys),
+            "output_key": str(self.output_key),
+            "validator_id": str(self.validator_id),
+        }
+
+
+def _operator_templates_from_store(store) -> List[OperatorTemplateV72]:
+    ops: List[OperatorTemplateV72] = []
+    concept_acts: Iterable[Act]
+    try:
+        concept_acts = store.concept_acts()
+    except Exception:
+        concept_acts = []
+    for act in concept_acts:
+        if act is None or str(getattr(act, "kind", "")) != "concept_csv" or (not bool(getattr(act, "active", True))):
+            continue
+        ev = act.evidence if isinstance(act.evidence, dict) else {}
+        iface = ev.get("interface") if isinstance(ev.get("interface"), dict) else {}
+        iface = iface if isinstance(iface, dict) else {}
+        in_schema = iface.get("input_schema") if isinstance(iface.get("input_schema"), dict) else {}
+        out_schema = iface.get("output_schema") if isinstance(iface.get("output_schema"), dict) else {}
+        validator_id = str(iface.get("validator_id") or "")
+        in_keys = _sorted_keys(in_schema)
+        out_keys = _sorted_keys(out_schema)
+        for out_k in out_keys:
+            ops.append(
+                OperatorTemplateV72(
+                    concept_id=str(act.id),
+                    input_keys=list(in_keys),
+                    output_key=str(out_k),
+                    validator_id=str(validator_id),
+                )
+            )
+    ops.sort(key=lambda o: (str(o.concept_id), str(o.output_key)))
+    return ops
+
+
+def _enumerate_bind_maps(
+    *,
+    input_keys: Sequence[str],
+    vars_avail: Sequence[str],
+    max_maps: int = 64,
+) -> List[Dict[str, str]]:
+    """
+    Deterministic bind_map enumeration:
+      - Prefer identity bindings when possible.
+      - Otherwise allow mapping each input slot to an available var.
+      - Enforce distinct var bindings when input_keys are distinct.
+    """
+    avail = [str(v) for v in vars_avail if str(v)]
+    avail = sorted(set(avail))
+    if not input_keys:
+        return [{}]
+    keys = [str(k) for k in input_keys]
+
+    # Candidate vars per slot: identity-first then others.
+    cand_per: List[List[str]] = []
+    for k in keys:
+        cands = []
+        if k in avail:
+            cands.append(k)
+        cands.extend([v for v in avail if v != k])
+        # Stable unique.
+        seen: Set[str] = set()
+        out: List[str] = []
+        for v in cands:
+            if v not in seen:
+                seen.add(v)
+                out.append(v)
+        cand_per.append(out)
+
+    out_maps: List[Dict[str, str]] = []
+
+    def _rec(i: int, cur: Dict[str, str], used: Set[str]) -> None:
+        if len(out_maps) >= int(max_maps):
+            return
+        if i >= len(keys):
+            out_maps.append(dict(cur))
+            return
+        slot = keys[i]
+        for v in cand_per[i]:
+            # Enforce distinctness across different input slots.
+            if slot not in cur and v in used:
+                continue
+            cur[slot] = v
+            used2 = set(used)
+            used2.add(v)
+            _rec(i + 1, cur, used2)
+            cur.pop(slot, None)
+            if len(out_maps) >= int(max_maps):
+                return
+
+    _rec(0, {}, set())
+
+    # Deterministic order by canonical json.
+    out_maps.sort(key=lambda m: canonical_json_dumps({str(k): str(m.get(k) or "") for k in sorted(m.keys(), key=str)}))
+    return out_maps
+
+
+def _apply_operator(
+    *,
+    op: OperatorTemplateV72,
+    vars_avail: Set[str],
+) -> Optional[Tuple[Set[str], Dict[str, str]]]:
+    if str(op.output_key) in vars_avail:
+        return None
+    # Slot-causal binding (minimal, deterministic):
+    # each required input slot key must already exist in vars_avail, and bind_map is identity.
+    if not vars_avail:
+        return None
+    req = [str(k) for k in (op.input_keys or []) if str(k)]
+    if any(k not in vars_avail for k in req):
+        return None
+    bm0 = {str(k): str(k) for k in req}
+    nxt = set(vars_avail)
+    nxt.add(str(op.output_key))
+    return nxt, bm0
+
+
+@dataclass
+class PlannerV72:
+    max_depth: int = 6
+    max_expansions: int = 256
+
+    def plan(self, *, goal_spec: GoalSpecV72, store) -> Tuple[Optional[PlanV72], Dict[str, Any]]:
+        """
+        Deterministic small search over interface-defined operators.
+        Goal: produce goal_spec.output_key from initial bindings.
+        """
+        ops = _operator_templates_from_store(store)
+        init_vars = set(str(k) for k in (goal_spec.bindings or {}).keys() if str(k))
+        target = str(goal_spec.output_key or "")
+        debug: Dict[str, Any] = {
+            "init_vars": sorted(init_vars),
+            "target": str(target),
+            "operators_total": int(len(ops)),
+            "expanded": 0,
+            "found": False,
+        }
+        if not target:
+            return None, {**debug, "reason": "missing_target"}
+        if target in init_vars:
+            # Already satisfied: empty plan.
+            plan_body = {"schema_version": 1, "steps": []}
+            psig = _stable_hash_obj(plan_body)
+            return PlanV72(steps=[], plan_sig=psig), {**debug, "found": True, "reason": "already_satisfied"}
+
+        # Frontier entries: (cost, depth, state_sig, vars, steps_so_far_as_templates+bindmaps)
+        frontier: List[Tuple[int, int, str, Set[str], List[Tuple[OperatorTemplateV72, Dict[str, str]]]]] = []
+        frontier.append((0, 0, _state_sig(init_vars), set(init_vars), []))
+        seen: Set[str] = set()
+
+        def _push(ent: Tuple[int, int, str, Set[str], List[Tuple[OperatorTemplateV72, Dict[str, str]]]]) -> None:
+            frontier.append(ent)
+            frontier.sort(key=lambda x: (int(x[0]), int(x[1]), str(x[2])))
+
+        while frontier and debug["expanded"] < int(self.max_expansions):
+            cost, depth, sig, vars_avail, path = frontier.pop(0)
+            if sig in seen:
+                continue
+            seen.add(sig)
+            debug["expanded"] = int(debug["expanded"]) + 1
+
+            if target in vars_avail:
+                # Build deterministic PlanV72.
+                steps: List[PlanStepV72] = []
+                for idx, (op, bm) in enumerate(path):
+                    step_body = {
+                        "idx": int(idx),
+                        "concept_id": str(op.concept_id),
+                        "bind_map": {str(k): str(bm.get(k) or "") for k in sorted(bm.keys(), key=str)},
+                        "produces": str(op.output_key),
+                    }
+                    step_id = _stable_hash_obj(step_body)
+                    steps.append(
+                        PlanStepV72(
+                            step_id=str(step_id),
+                            idx=int(idx),
+                            concept_id=str(op.concept_id),
+                            bind_map=dict(step_body["bind_map"]),
+                            produces=str(op.output_key),
+                        )
+                    )
+                plan_body = {"schema_version": 1, "steps": [s.to_dict() for s in steps]}
+                psig = _stable_hash_obj(plan_body)
+                debug["found"] = True
+                debug["reason"] = "ok"
+                debug["plan_sig"] = str(psig)
+                return PlanV72(steps=steps, plan_sig=str(psig)), debug
+
+            if int(depth) >= int(self.max_depth):
+                continue
+
+            for op in ops:
+                applied = _apply_operator(op=op, vars_avail=set(vars_avail))
+                if applied is None:
+                    continue
+                nxt_vars, bm = applied
+                nxt_sig = _state_sig(nxt_vars)
+                nxt_path = list(path) + [(op, dict(bm))]
+                _push((int(cost) + 1, int(depth) + 1, nxt_sig, nxt_vars, nxt_path))
+
+        return None, {**debug, "reason": "not_found_or_budget_exhausted"}
--- /dev/null	2026-01-12 10:03:21
+++ scripts/smoke_planner_agent_loop_v72.py	2026-01-12 10:01:42
@@ -0,0 +1,307 @@
+#!/usr/bin/env python3
+from __future__ import annotations
+
+import argparse
+import hashlib
+import json
+import os
+import sys
+from typing import Any, Dict, List, Tuple
+
+sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))
+
+from atos_core.act import Act, Instruction, canonical_json_dumps, deterministic_iso, sha256_hex
+from atos_core.agent_loop_v72 import run_goal_spec_v72
+from atos_core.goal_spec_v72 import GoalSpecV72
+from atos_core.store import ActStore
+
+
+def sha256_file(path: str) -> str:
+    h = hashlib.sha256()
+    with open(path, "rb") as f:
+        while True:
+            b = f.read(1024 * 1024)
+            if not b:
+                break
+            h.update(b)
+    return h.hexdigest()
+
+
+def sha256_text(s: str) -> str:
+    return hashlib.sha256(str(s).encode("utf-8")).hexdigest()
+
+
+def sha256_canon(obj: Any) -> str:
+    return sha256_hex(canonical_json_dumps(obj).encode("utf-8"))
+
+
+def _fail(msg: str, *, code: int = 2) -> None:
+    print(msg, file=sys.stderr)
+    raise SystemExit(code)
+
+
+def ensure_absent(path: str) -> None:
+    if os.path.exists(path):
+        _fail(f"ERROR: path already exists: {path}")
+
+
+def write_json(path: str, obj: Any) -> str:
+    ensure_absent(path)
+    tmp = path + ".tmp"
+    with open(tmp, "w", encoding="utf-8") as f:
+        f.write(json.dumps(obj, ensure_ascii=False, indent=2, sort_keys=True))
+        f.write("\n")
+    os.replace(tmp, path)
+    return sha256_file(path)
+
+
+def make_concept_act(
+    *,
+    act_id: str,
+    input_schema: Dict[str, str],
+    output_schema: Dict[str, str],
+    validator_id: str,
+    program: List[Instruction],
+) -> Act:
+    return Act(
+        id=str(act_id),
+        version=1,
+        created_at=deterministic_iso(step=0),
+        kind="concept_csv",
+        match={},
+        program=list(program),
+        evidence={
+            "interface": {
+                "input_schema": dict(input_schema),
+                "output_schema": dict(output_schema),
+                "validator_id": str(validator_id),
+            }
+        },
+        cost={},
+        deps=[],
+        active=True,
+    )
+
+
+def smoke_try(*, out_dir: str, seed: int) -> Dict[str, Any]:
+    store = ActStore()
+
+    # Micro-world concepts forcing multi-step planning.
+    normalize_x_id = "concept_v72_normalize_x_v0"
+    normalize_y_id = "concept_v72_normalize_y_v0"
+    add_nx_ny_id = "concept_v72_add_nx_ny_v0"
+
+    normalize_x = make_concept_act(
+        act_id=normalize_x_id,
+        input_schema={"x": "str"},
+        output_schema={"nx": "str"},
+        validator_id="text_exact",
+        program=[
+            Instruction("CSV_GET_INPUT", {"name": "x", "out": "x"}),
+            Instruction("CSV_PRIMITIVE", {"fn": "scan_digits", "in": ["x"], "out": "d"}),
+            Instruction("CSV_PRIMITIVE", {"fn": "digits_to_int", "in": ["d"], "out": "i"}),
+            Instruction("CSV_PRIMITIVE", {"fn": "int_to_digits", "in": ["i"], "out": "nx"}),
+            Instruction("CSV_RETURN", {"var": "nx"}),
+        ],
+    )
+    store.add(normalize_x)
+
+    normalize_y = make_concept_act(
+        act_id=normalize_y_id,
+        input_schema={"y": "str"},
+        output_schema={"ny": "str"},
+        validator_id="text_exact",
+        program=[
+            Instruction("CSV_GET_INPUT", {"name": "y", "out": "y"}),
+            Instruction("CSV_PRIMITIVE", {"fn": "scan_digits", "in": ["y"], "out": "d"}),
+            Instruction("CSV_PRIMITIVE", {"fn": "digits_to_int", "in": ["d"], "out": "i"}),
+            Instruction("CSV_PRIMITIVE", {"fn": "int_to_digits", "in": ["i"], "out": "ny"}),
+            Instruction("CSV_RETURN", {"var": "ny"}),
+        ],
+    )
+    store.add(normalize_y)
+
+    add_nx_ny = make_concept_act(
+        act_id=add_nx_ny_id,
+        input_schema={"nx": "str", "ny": "str"},
+        output_schema={"sum": "str"},
+        validator_id="text_exact",
+        program=[
+            Instruction("CSV_GET_INPUT", {"name": "nx", "out": "nx"}),
+            Instruction("CSV_GET_INPUT", {"name": "ny", "out": "ny"}),
+            Instruction("CSV_PRIMITIVE", {"fn": "scan_digits", "in": ["nx"], "out": "dx"}),
+            Instruction("CSV_PRIMITIVE", {"fn": "scan_digits", "in": ["ny"], "out": "dy"}),
+            Instruction("CSV_PRIMITIVE", {"fn": "digits_to_int", "in": ["dx"], "out": "ix"}),
+            Instruction("CSV_PRIMITIVE", {"fn": "digits_to_int", "in": ["dy"], "out": "iy"}),
+            Instruction("CSV_PRIMITIVE", {"fn": "add_int", "in": ["ix", "iy"], "out": "sum_i"}),
+            Instruction("CSV_PRIMITIVE", {"fn": "int_to_digits", "in": ["sum_i"], "out": "sum"}),
+            Instruction("CSV_RETURN", {"var": "sum"}),
+        ],
+    )
+    store.add(add_nx_ny)
+
+    goal_spec = GoalSpecV72(
+        goal_kind="v72_sum_norm",
+        bindings={"x": "0004", "y": "0008"},
+        output_key="sum",
+        expected="12",
+        validator_id="text_exact",
+        created_step=0,
+    )
+
+    res = run_goal_spec_v72(
+        goal_spec=goal_spec,
+        store=store,
+        seed=int(seed),
+        out_dir=str(out_dir),
+        max_depth=6,
+        max_expansions=256,
+        max_events=256,
+    )
+
+    if not bool(res.get("ok", False)):
+        _fail(f"ERROR: agent_loop_v72 not ok: {res.get('final')}")
+
+    plan = res.get("plan") if isinstance(res.get("plan"), dict) else {}
+    steps = plan.get("steps") if isinstance(plan.get("steps"), list) else []
+    plan_sig = str(plan.get("plan_sig") or "")
+    if not plan_sig:
+        _fail("ERROR: missing plan_sig")
+    if int(len(steps)) < 2:
+        _fail(f"ERROR: expected multi-step plan >=2, got={len(steps)}")
+
+    final = res.get("final") if isinstance(res.get("final"), dict) else {}
+    got = str(final.get("got") or "")
+    expected = str(final.get("expected") or "")
+    v = final.get("validator") if isinstance(final.get("validator"), dict) else {}
+    if got != expected or not bool(v.get("passed", False)):
+        _fail(f"ERROR: final_mismatch: got={got} expected={expected} validator={v}")
+
+    chains = (res.get("graph") or {}).get("chains") if isinstance(res.get("graph"), dict) else {}
+    if not (bool(chains.get("mind_nodes_chain_ok")) and bool(chains.get("mind_edges_chain_ok"))):
+        _fail(f"ERROR: mind_graph_chain_failed: {chains}")
+
+    render = res.get("render") if isinstance(res.get("render"), dict) else {}
+    render_sig_v1 = str(render.get("render_sig_v1") or "")
+    render_sig_v2 = str(render.get("render_sig_v2") or "")
+    render_sig_v1_after = str(render.get("render_sig_v1_after_irrelevant") or "")
+    if not render_sig_v1 or not render_sig_v2:
+        _fail("ERROR: missing render_sig")
+    if render_sig_v1 == render_sig_v2:
+        _fail("ERROR: expected render_sig_v1 != render_sig_v2")
+    if render_sig_v1_after != render_sig_v1:
+        _fail("ERROR: render_sig_v1 changed after adding unreachable node")
+
+    # Anti-aliasing: mutating goal_spec.bindings and returned trace must not change the stored snapshot.
+    snap = res.get("snapshot") if isinstance(res.get("snapshot"), dict) else {}
+    snap_sig_before = sha256_canon(snap)
+    try:
+        goal_spec.bindings["x"] = "MUTATED"
+    except Exception:
+        pass
+    tr = res.get("trace") if isinstance(res.get("trace"), dict) else {}
+    calls = tr.get("concept_calls") if isinstance(tr.get("concept_calls"), list) else []
+    if calls and isinstance(calls[0], dict) and isinstance(calls[0].get("bindings"), dict):
+        calls[0]["bindings"]["x"] = "MUTATED"
+    snap_sig_after = sha256_canon(snap)
+    if snap_sig_after != snap_sig_before:
+        _fail("ERROR: anti_aliasing_failed: snapshot changed after external mutations")
+
+    nodes_total = int(len(snap.get("nodes", [])) if isinstance(snap.get("nodes"), list) else 0)
+    edges_total = int(len(snap.get("edges", [])) if isinstance(snap.get("edges"), list) else 0)
+
+    return {
+        "seed": int(seed),
+        "plan": {"plan_sig": str(plan_sig), "steps_total": int(len(steps))},
+        "final": {"got": str(got), "expected": str(expected), "validator": dict(v)},
+        "graph": {
+            "graph_sig": str((res.get("graph") or {}).get("graph_sig") if isinstance(res.get("graph"), dict) else ""),
+            "chains": dict(chains) if isinstance(chains, dict) else {},
+            "nodes_total": int(nodes_total),
+            "edges_total": int(edges_total),
+        },
+        "render": {
+            "render_sig_v1": str(render_sig_v1),
+            "render_sig_v2": str(render_sig_v2),
+            "render_sig_v1_after_irrelevant": str(render_sig_v1_after),
+        },
+        "checks": {
+            "plan_multi_step": True,
+            "output_ok": True,
+            "chains_ok": True,
+            "anti_aliasing_ok": True,
+            "render_substitutable_styles": bool(render_sig_v1 != render_sig_v2),
+            "render_invariance_unreachable": bool(render_sig_v1_after == render_sig_v1),
+        },
+        "snapshot_sig": {"before": str(snap_sig_before), "after": str(snap_sig_after)},
+    }
+
+
+def main() -> None:
+    ap = argparse.ArgumentParser()
+    ap.add_argument("--out_base", default="results/run_smoke_planner_agent_loop_v72")
+    ap.add_argument("--seed", type=int, default=0)
+    args = ap.parse_args()
+
+    out_base = str(args.out_base)
+    seed = int(args.seed)
+
+    results: Dict[str, Any] = {"seed": seed, "tries": {}}
+    sigs: List[Tuple[str, str, str]] = []
+    summary_shas: List[str] = []
+
+    for t in (1, 2):
+        out_dir = f"{out_base}_try{t}"
+        ensure_absent(out_dir)
+        os.makedirs(out_dir, exist_ok=False)
+
+        ev = smoke_try(out_dir=out_dir, seed=seed)
+        eval_path = os.path.join(out_dir, "eval.json")
+        eval_sha = write_json(eval_path, ev)
+
+        core = {
+            "seed": int(seed),
+            "plan_sig": str(ev.get("plan", {}).get("plan_sig") if isinstance(ev.get("plan"), dict) else ""),
+            "steps_total": int(ev.get("plan", {}).get("steps_total") if isinstance(ev.get("plan"), dict) else 0),
+            "graph_sig": str(ev.get("graph", {}).get("graph_sig") if isinstance(ev.get("graph"), dict) else ""),
+            "render_sig_v1": str(ev.get("render", {}).get("render_sig_v1") if isinstance(ev.get("render"), dict) else ""),
+            "render_sig_v2": str(ev.get("render", {}).get("render_sig_v2") if isinstance(ev.get("render"), dict) else ""),
+            "final_got": str(ev.get("final", {}).get("got") if isinstance(ev.get("final"), dict) else ""),
+            "sha256_eval_json": str(eval_sha),
+        }
+        summary_sha = sha256_text(canonical_json_dumps(core))
+        smoke = {"summary": core, "determinism": {"summary_sha256": str(summary_sha)}}
+        smoke_path = os.path.join(out_dir, "smoke_summary.json")
+        smoke_sha = write_json(smoke_path, smoke)
+
+        sigs.append((str(core["plan_sig"]), str(core["graph_sig"]), str(core["render_sig_v1"])))
+        summary_shas.append(str(summary_sha))
+
+        results["tries"][f"try{t}"] = {
+            "out_dir": out_dir,
+            "eval_json": {"path": eval_path, "sha256": eval_sha},
+            "smoke_summary_json": {"path": smoke_path, "sha256": smoke_sha},
+            "summary_sha256": summary_sha,
+        }
+
+    determinism_ok = bool(
+        len(sigs) == 2
+        and sigs[0] == sigs[1]
+        and len(summary_shas) == 2
+        and summary_shas[0] == summary_shas[1]
+    )
+    if not determinism_ok:
+        _fail(f"ERROR: determinism mismatch: sigs={sigs} summary_shas={summary_shas}")
+    results["determinism"] = {
+        "ok": True,
+        "summary_sha256": summary_shas[0],
+        "plan_sig": sigs[0][0],
+        "graph_sig": sigs[0][1],
+        "render_sig_v1": sigs[0][2],
+    }
+    print(json.dumps(results, ensure_ascii=False, indent=2, sort_keys=True))
+
+
+if __name__ == "__main__":
+    main()
+
