--- /dev/null	2026-01-17 18:14:46
+++ atos_core/arc_loader_v131.py	2026-01-17 18:06:25
@@ -0,0 +1,197 @@
+from __future__ import annotations
+
+import hashlib
+import json
+from dataclasses import dataclass
+from pathlib import Path
+from typing import Any, Dict, Iterator, List, Optional, Sequence, Tuple
+
+from .act import canonical_json_dumps, sha256_hex
+from .grid_v124 import GridV124, grid_from_list_v124, grid_shape_v124, unique_colors_v124
+
+ARC_LOADER_SCHEMA_VERSION_V131 = 131
+
+
+def _sha256_file(path: Path) -> str:
+    h = hashlib.sha256()
+    with open(path, "rb") as f:
+        while True:
+            b = f.read(1024 * 1024)
+            if not b:
+                break
+            h.update(b)
+    return h.hexdigest()
+
+
+def _resolve_arc_tasks_root_v131(*, arc_root: str, split: Optional[str]) -> Path:
+    root = Path(str(arc_root)).resolve()
+    if not root.exists():
+        raise FileNotFoundError(f"arc_root_missing:{root}")
+
+    requested = str(split or "").strip()
+    candidates: List[Tuple[Path, List[str]]] = []
+    for base in (root, root / "data"):
+        if not base.exists():
+            continue
+        found: List[str] = []
+        for name in ("training", "evaluation"):
+            if (base / name).is_dir():
+                found.append(name)
+        if found:
+            candidates.append((base, found))
+
+    if candidates:
+        # Prefer direct <arc_root>/<split> over <arc_root>/data/<split>.
+        base, found = candidates[0]
+        if requested and requested in found:
+            return base / requested
+        raise ValueError(
+            f"arc_split_required:requested={requested or '<missing>'} available={','.join(found)} root={root}"
+        )
+
+    # Flat layout: arc_root/*.json
+    return root
+
+
+def iter_arc_task_paths_v131(*, arc_root: str, split: Optional[str] = None) -> List[Path]:
+    tasks_root = _resolve_arc_tasks_root_v131(arc_root=str(arc_root), split=split)
+    paths = [p for p in tasks_root.rglob("*.json") if p.is_file()]
+    paths.sort(key=lambda p: p.relative_to(tasks_root).as_posix())
+    return paths
+
+
+def _load_json(path: Path) -> Any:
+    with open(path, "r", encoding="utf-8") as f:
+        return json.load(f)
+
+
+def _canonical_task_id(root: Path, path: Path) -> str:
+    return path.relative_to(root).as_posix()
+
+
+def _derive_meta_v131(*, train_pairs: Sequence[Tuple[GridV124, GridV124]], test_in: GridV124) -> Dict[str, Any]:
+    shapes_in = sorted({grid_shape_v124(inp) for inp, _ in train_pairs} | {grid_shape_v124(test_in)})
+    shapes_out = sorted({grid_shape_v124(out) for _, out in train_pairs})
+    colors = set()
+    for inp, out in train_pairs:
+        colors.update(unique_colors_v124(inp))
+        colors.update(unique_colors_v124(out))
+    colors.update(unique_colors_v124(test_in))
+    return {
+        "shapes_in": [{"h": int(h), "w": int(w)} for h, w in shapes_in],
+        "shapes_out": [{"h": int(h), "w": int(w)} for h, w in shapes_out],
+        "colors": [int(c) for c in sorted(colors)],
+    }
+
+
+@dataclass(frozen=True)
+class CanonicalArcTaskV131:
+    task_id: str
+    train_pairs: List[Tuple[GridV124, GridV124]]
+    test_in: GridV124
+    meta: Dict[str, Any]
+
+    def to_dict(self) -> Dict[str, Any]:
+        return {
+            "schema_version": int(ARC_LOADER_SCHEMA_VERSION_V131),
+            "task_id": str(self.task_id),
+            "train_pairs": [
+                {"in_grid": [list(r) for r in inp], "out_grid": [list(r) for r in out]}
+                for inp, out in self.train_pairs
+            ],
+            "test_in_grid": [list(r) for r in self.test_in],
+            "meta": dict(self.meta),
+        }
+
+
+def load_arc_task_v131(*, root: Path, path: Path) -> CanonicalArcTaskV131:
+    raw = _load_json(path)
+    if not isinstance(raw, dict):
+        raise ValueError("arc_task_not_object")
+    train_raw = raw.get("train")
+    test_raw = raw.get("test")
+    if not isinstance(train_raw, list) or not isinstance(test_raw, list) or not test_raw:
+        raise ValueError("arc_task_missing_train_or_test")
+
+    train_pairs: List[Tuple[GridV124, GridV124]] = []
+    for pair in train_raw:
+        if not isinstance(pair, dict):
+            raise ValueError("arc_train_pair_not_object")
+        inp = grid_from_list_v124(pair.get("input"))
+        out = grid_from_list_v124(pair.get("output"))
+        train_pairs.append((inp, out))
+
+    test0 = test_raw[0]
+    if not isinstance(test0, dict):
+        raise ValueError("arc_test_pair_not_object")
+    test_in = grid_from_list_v124(test0.get("input"))
+
+    task_id = _canonical_task_id(root, path)
+    meta = _derive_meta_v131(train_pairs=train_pairs, test_in=test_in)
+    return CanonicalArcTaskV131(task_id=str(task_id), train_pairs=train_pairs, test_in=test_in, meta=meta)
+
+
+def write_arc_canonical_jsonl_v131(
+    *, arc_root: str, out_jsonl_path: str, limit: Optional[int] = None, split: Optional[str] = None
+) -> Dict[str, Any]:
+    root = Path(str(arc_root)).resolve()
+    tasks_root = _resolve_arc_tasks_root_v131(arc_root=str(root), split=split)
+    out_path = Path(str(out_jsonl_path))
+    if out_path.exists():
+        raise ValueError(f"worm_exists:{out_path}")
+    out_path.parent.mkdir(parents=True, exist_ok=True)
+
+    task_paths = [p for p in tasks_root.rglob("*.json") if p.is_file()]
+    task_paths.sort(key=lambda p: p.relative_to(tasks_root).as_posix())
+    if limit is not None:
+        task_paths = task_paths[: int(limit)]
+
+    inputs: List[Dict[str, Any]] = []
+    count_written = 0
+    with open(out_path, "x", encoding="utf-8") as f:
+        for p in task_paths:
+            task = load_arc_task_v131(root=tasks_root, path=p)
+            f.write(canonical_json_dumps(task.to_dict()))
+            f.write("\n")
+            inputs.append(
+                {"task_id": str(task.task_id), "relpath": p.relative_to(tasks_root).as_posix(), "sha256": _sha256_file(p)}
+            )
+            count_written += 1
+
+    manifest = {
+        "schema_version": int(ARC_LOADER_SCHEMA_VERSION_V131),
+        "kind": "arc_canonical_manifest_v131",
+        "arc_root_input": str(root),
+        "tasks_root": str(tasks_root),
+        "split": str(split or ""),
+        "tasks_total": int(count_written),
+        "inputs": list(inputs),
+        "sha256": {"canonical_jsonl": _sha256_file(out_path)},
+    }
+    manifest["manifest_sig"] = sha256_hex(canonical_json_dumps(manifest).encode("utf-8"))
+    return manifest
+
+
+def iter_canonical_tasks_v131(jsonl_path: str) -> Iterator[CanonicalArcTaskV131]:
+    p = Path(str(jsonl_path))
+    with open(p, "r", encoding="utf-8") as f:
+        for line in f:
+            line = line.strip()
+            if not line:
+                continue
+            d = json.loads(line)
+            if not isinstance(d, dict):
+                continue
+            task_id = str(d.get("task_id") or "")
+            tps = d.get("train_pairs") if isinstance(d.get("train_pairs"), list) else []
+            train_pairs: List[Tuple[GridV124, GridV124]] = []
+            for tp in tps:
+                if not isinstance(tp, dict):
+                    continue
+                inp = grid_from_list_v124(tp.get("in_grid"))
+                out = grid_from_list_v124(tp.get("out_grid"))
+                train_pairs.append((inp, out))
+            test_in = grid_from_list_v124(d.get("test_in_grid"))
+            meta = dict(d.get("meta", {})) if isinstance(d.get("meta"), dict) else {}
+            yield CanonicalArcTaskV131(task_id=task_id, train_pairs=train_pairs, test_in=test_in, meta=meta)
+
--- /dev/null	2026-01-17 18:14:46
+++ atos_core/arc_solver_v131.py	2026-01-17 18:07:37
@@ -0,0 +1,414 @@
+from __future__ import annotations
+
+import heapq
+from dataclasses import dataclass
+from typing import Any, Dict, List, Optional, Sequence, Tuple
+
+from .act import canonical_json_dumps, sha256_hex
+from .grid_v124 import (
+    GridV124,
+    crop_to_bbox_nonzero_v124,
+    grid_equal_v124,
+    grid_hash_v124,
+    grid_shape_v124,
+    pad_to_v124,
+    reflect_h_v124,
+    reflect_v_v124,
+    rotate180_v124,
+    rotate270_v124,
+    rotate90_v124,
+    translate_v124,
+    unique_colors_v124,
+)
+
+ARC_SOLVER_SCHEMA_VERSION_V131 = 131
+
+
+def _validate_grid_values_v131(g: GridV124) -> None:
+    for row in g:
+        for x in row:
+            xx = int(x)
+            if xx < 0 or xx > 9:
+                raise ValueError("grid_cell_out_of_range")
+
+
+@dataclass(frozen=True)
+class ProgramStepV131:
+    op: str
+    args: Dict[str, Any]
+
+    def to_dict(self) -> Dict[str, Any]:
+        d: Dict[str, Any] = {"op": str(self.op)}
+        for k in sorted(self.args.keys()):
+            d[str(k)] = self.args[k]
+        return d
+
+
+@dataclass(frozen=True)
+class ProgramV131:
+    steps: Tuple[ProgramStepV131, ...]
+
+    def to_dict(self) -> Dict[str, Any]:
+        return {
+            "schema_version": int(ARC_SOLVER_SCHEMA_VERSION_V131),
+            "kind": "arc_program_v131",
+            "steps": [s.to_dict() for s in self.steps],
+        }
+
+    def program_sig(self) -> str:
+        return sha256_hex(canonical_json_dumps(self.to_dict()).encode("utf-8"))
+
+
+def _replace_color_v131(g: GridV124, *, from_color: int, to_color: int) -> GridV124:
+    fc = int(from_color)
+    tc = int(to_color)
+    if fc < 0 or fc > 9 or tc < 0 or tc > 9:
+        raise ValueError("color_out_of_range")
+    return tuple(tuple(tc if int(x) == fc else int(x) for x in row) for row in g)
+
+
+def _map_colors_v131(g: GridV124, *, mapping: Dict[str, int]) -> GridV124:
+    m: Dict[int, int] = {}
+    for k, v in mapping.items():
+        kk = int(k)
+        vv = int(v)
+        if kk < 0 or kk > 9 or vv < 0 or vv > 9:
+            raise ValueError("color_out_of_range")
+        m[kk] = vv
+    return tuple(tuple(int(m.get(int(x), int(x))) for x in row) for row in g)
+
+
+def apply_program_v131(program: ProgramV131, g: GridV124) -> GridV124:
+    _validate_grid_values_v131(g)
+    cur = g
+    for step in program.steps:
+        op = str(step.op)
+        a = dict(step.args)
+        if op == "rotate90":
+            cur = rotate90_v124(cur)
+        elif op == "rotate180":
+            cur = rotate180_v124(cur)
+        elif op == "rotate270":
+            cur = rotate270_v124(cur)
+        elif op == "reflect_h":
+            cur = reflect_h_v124(cur)
+        elif op == "reflect_v":
+            cur = reflect_v_v124(cur)
+        elif op == "translate":
+            cur = translate_v124(cur, dx=int(a["dx"]), dy=int(a["dy"]), pad=int(a.get("pad", 0)))
+        elif op == "crop_bbox_nonzero":
+            cur = crop_to_bbox_nonzero_v124(cur, bg=int(a.get("bg", 0)))
+        elif op == "pad_to":
+            cur = pad_to_v124(cur, height=int(a["height"]), width=int(a["width"]), pad=int(a.get("pad", 0)))
+        elif op == "replace_color":
+            cur = _replace_color_v131(cur, from_color=int(a["from_color"]), to_color=int(a["to_color"]))
+        elif op == "map_colors":
+            cur = _map_colors_v131(cur, mapping=dict(a.get("mapping", {})))
+        else:
+            raise ValueError(f"unknown_op:{op}")
+        _validate_grid_values_v131(cur)
+    return cur
+
+
+def _program_cost_bits_v131(program: ProgramV131) -> int:
+    # Deterministic MDL proxy:
+    # - each step has a base cost
+    # - each scalar param costs +4 bits
+    # - mapping costs +8 bits per entry
+    bits = 0
+    for s in program.steps:
+        bits += 10
+        for k, v in s.args.items():
+            if k == "mapping" and isinstance(v, dict):
+                bits += 8 * int(len(v))
+            else:
+                bits += 4
+    return int(bits)
+
+
+def _infer_color_mapping_v131(inp: GridV124, out: GridV124) -> Optional[Dict[str, int]]:
+    hi, wi = grid_shape_v124(inp)
+    ho, wo = grid_shape_v124(out)
+    if hi != ho or wi != wo or hi == 0 or wi == 0:
+        return None
+    mapping: Dict[int, int] = {}
+    for r in range(hi):
+        for c in range(wi):
+            a = int(inp[r][c])
+            b = int(out[r][c])
+            if a in mapping and mapping[a] != b:
+                return None
+            mapping[a] = b
+    return {str(k): int(mapping[k]) for k in sorted(mapping.keys())}
+
+
+def _bg_candidates_v131(grids: Sequence[GridV124]) -> List[int]:
+    out: List[int] = [0]
+    for g in grids:
+        h, w = grid_shape_v124(g)
+        if h > 0 and w > 0:
+            out.extend([int(g[0][0]), int(g[0][w - 1]), int(g[h - 1][0]), int(g[h - 1][w - 1])])
+    return sorted(set(int(x) for x in out))
+
+
+def _op_variants_v131(*, train_pairs: Sequence[Tuple[GridV124, GridV124]], max_translate_shift: int = 2) -> List[ProgramStepV131]:
+    colors: List[int] = []
+    shapes_out: List[Tuple[int, int]] = []
+    inferred_maps: List[Dict[str, int]] = []
+    train_inputs: List[GridV124] = []
+    for inp, out in train_pairs:
+        train_inputs.append(inp)
+        colors.extend(unique_colors_v124(inp))
+        colors.extend(unique_colors_v124(out))
+        shapes_out.append(grid_shape_v124(out))
+        m = _infer_color_mapping_v131(inp, out)
+        if m is not None:
+            inferred_maps.append(m)
+
+    colors = sorted(set(int(c) for c in colors))
+    shapes_out = sorted(set((int(h), int(w)) for h, w in shapes_out))
+    inferred_maps = sorted(inferred_maps, key=lambda m: canonical_json_dumps(m))
+
+    steps: List[ProgramStepV131] = []
+    # Basic transforms
+    for op in ["rotate90", "rotate180", "rotate270", "reflect_h", "reflect_v"]:
+        steps.append(ProgramStepV131(op=op, args={}))
+
+    # crop_bbox_nonzero with plausible backgrounds
+    for bg in _bg_candidates_v131(train_inputs):
+        steps.append(ProgramStepV131(op="crop_bbox_nonzero", args={"bg": int(bg)}))
+
+    # small translations (bounded deterministically)
+    for dy in range(-int(max_translate_shift), int(max_translate_shift) + 1):
+        for dx in range(-int(max_translate_shift), int(max_translate_shift) + 1):
+            if dx == 0 and dy == 0:
+                continue
+            steps.append(ProgramStepV131(op="translate", args={"dx": int(dx), "dy": int(dy), "pad": 0}))
+
+    # pad_to shapes seen in outputs
+    for h, w in shapes_out:
+        steps.append(ProgramStepV131(op="pad_to", args={"height": int(h), "width": int(w), "pad": 0}))
+
+    # replace_color from observed colors
+    for fc in colors:
+        for tc in colors:
+            if fc == tc:
+                continue
+            steps.append(ProgramStepV131(op="replace_color", args={"from_color": int(fc), "to_color": int(tc)}))
+
+    # map_colors inferred from at least one training pair
+    for m in inferred_maps:
+        steps.append(ProgramStepV131(op="map_colors", args={"mapping": dict(m)}))
+
+    # Deterministic ordering
+    steps.sort(key=lambda s: (str(s.op), canonical_json_dumps(s.to_dict())))
+    return steps
+
+
+def _summarize_mismatch_v131(*, got: GridV124, want: GridV124) -> Dict[str, Any]:
+    hg, wg = grid_shape_v124(got)
+    hw, ww = grid_shape_v124(want)
+    if (hg, wg) != (hw, ww):
+        return {"kind": "shape_mismatch", "got": {"h": hg, "w": wg}, "want": {"h": hw, "w": ww}}
+    diff = 0
+    for r in range(hg):
+        for c in range(wg):
+            if int(got[r][c]) != int(want[r][c]):
+                diff += 1
+    return {"kind": "cell_mismatch", "diff_cells": int(diff), "total_cells": int(hg * wg)}
+
+
+def solve_arc_task_v131(
+    *,
+    train_pairs: Sequence[Tuple[GridV124, GridV124]],
+    test_in: GridV124,
+    max_depth: int = 3,
+    max_programs: int = 2000,
+    trace_program_limit: int = 50,
+    max_translate_shift: int = 2,
+) -> Dict[str, Any]:
+    """
+    ARC solver base (C1): deterministic compositional search over typed grid ops.
+    FAIL-CLOSED: if multiple minimal programs disagree on test_in output -> UNKNOWN.
+    """
+    try:
+        _validate_grid_values_v131(test_in)
+        for inp, out in train_pairs:
+            _validate_grid_values_v131(inp)
+            _validate_grid_values_v131(out)
+    except Exception as e:
+        return {
+            "schema_version": 131,
+            "kind": "arc_solve_result_v131",
+            "status": "FAIL",
+            "failure_reason": {"kind": "INVARIANT_VIOLATION", "details": {"error": str(e)}},
+        }
+
+    if not train_pairs:
+        return {
+            "schema_version": 131,
+            "kind": "arc_solve_result_v131",
+            "status": "FAIL",
+            "failure_reason": {"kind": "TYPE_MISMATCH", "details": {"error": "no_train_pairs"}},
+        }
+
+    ops = _op_variants_v131(train_pairs=list(train_pairs), max_translate_shift=int(max_translate_shift))
+    step_fps: List[str] = [canonical_json_dumps(s.to_dict()) for s in ops]
+    step_costs: List[int] = []
+    for s in ops:
+        step_costs.append(10 + sum(8 * int(len(v)) if k == "mapping" and isinstance(v, dict) else 4 for k, v in s.args.items()))
+
+    def _child_sig(parent_sig: str, step_fp: str) -> str:
+        return sha256_hex((str(parent_sig) + "|" + str(step_fp)).encode("utf-8"))
+
+    root_sig = sha256_hex(b"arc_program_v131_root")
+    frontier: List[Tuple[int, int, str, int, Tuple[ProgramStepV131, ...]]] = []
+    push_idx = 0
+    heapq.heappush(frontier, (0, 0, str(root_sig), push_idx, tuple()))
+
+    seen: set[str] = set()
+    best_cost: Optional[int] = None
+    solutions: List[ProgramV131] = []
+
+    tried = 0
+    trace_programs: List[Dict[str, Any]] = []
+    budget_exhausted = False
+
+    while frontier:
+        cost_bits, plen, sig, _, steps = heapq.heappop(frontier)
+        if sig in seen:
+            continue
+        seen.add(sig)
+
+        if best_cost is not None and int(cost_bits) > int(best_cost):
+            break
+
+        program = ProgramV131(steps=tuple(steps))
+
+        ok_train = True
+        mismatch: Optional[Dict[str, Any]] = None
+        for inp, out in train_pairs:
+            try:
+                got = apply_program_v131(program, inp)
+            except Exception as e:
+                ok_train = False
+                mismatch = {"kind": "apply_error", "error": str(e)}
+                break
+            if not grid_equal_v124(got, out):
+                ok_train = False
+                mismatch = _summarize_mismatch_v131(got=got, want=out)
+                break
+
+        if len(trace_programs) < int(trace_program_limit):
+            trace_programs.append(
+                {
+                    "program_sig": program.program_sig(),
+                    "cost_bits": int(cost_bits),
+                    "depth": int(plen),
+                    "steps": [s.to_dict() for s in program.steps],
+                    "ok_train": bool(ok_train),
+                    "mismatch": mismatch,
+                }
+            )
+
+        tried += 1
+        if tried >= int(max_programs):
+            budget_exhausted = True
+            break
+
+        if ok_train:
+            if best_cost is None:
+                best_cost = int(cost_bits)
+            if int(cost_bits) == int(best_cost):
+                solutions.append(program)
+            continue
+
+        if plen >= int(max_depth):
+            continue
+
+        for step_idx, step in enumerate(ops):
+            child_sig = _child_sig(str(sig), str(step_fps[step_idx]))
+            if child_sig in seen:
+                continue
+            child_steps = tuple(list(steps) + [step])
+            child_cost = int(cost_bits) + int(step_costs[step_idx])
+            push_idx += 1
+            heapq.heappush(frontier, (int(child_cost), int(plen + 1), str(child_sig), int(push_idx), child_steps))
+
+    trace = {
+        "schema_version": 131,
+        "kind": "arc_trace_v131",
+        "candidates_tested": int(tried),
+        "candidates_kept_in_trace": int(len(trace_programs)),
+        "trace_programs": list(trace_programs),
+        "budget_exhausted": bool(budget_exhausted),
+        "max_programs": int(max_programs),
+        "max_depth": int(max_depth),
+    }
+
+    if solutions:
+        sols_sorted = sorted(solutions, key=lambda p: p.program_sig())
+        best_program = sols_sorted[0]
+        outputs: List[Tuple[str, GridV124]] = []
+        for p in sols_sorted:
+            try:
+                pred = apply_program_v131(p, test_in)
+            except Exception as e:
+                return {
+                    "schema_version": 131,
+                    "kind": "arc_solve_result_v131",
+                    "status": "FAIL",
+                    "failure_reason": {"kind": "INVARIANT_VIOLATION", "details": {"error": str(e)}},
+                    "trace": dict(trace),
+                }
+            outputs.append((p.program_sig(), pred))
+
+        pred_hashes = {sig: grid_hash_v124(g) for sig, g in outputs}
+        out_hashes = sorted(set(pred_hashes.values()))
+        if len(out_hashes) == 1:
+            pred_grid = outputs[0][1]
+            return {
+                "schema_version": 131,
+                "kind": "arc_solve_result_v131",
+                "status": "SOLVED",
+                "program": [s.to_dict() for s in best_program.steps],
+                "program_sig": best_program.program_sig(),
+                "program_cost_bits": int(_program_cost_bits_v131(best_program)),
+                "predicted_grid_hash": grid_hash_v124(pred_grid),
+                "predicted_output": [list(r) for r in pred_grid],
+                "trace": dict(trace),
+            }
+
+        return {
+            "schema_version": 131,
+            "kind": "arc_solve_result_v131",
+            "status": "UNKNOWN",
+            "failure_reason": {
+                "kind": "AMBIGUOUS_RULE",
+                "details": {"solutions": int(len(solutions)), "distinct_predicted_outputs": int(len(out_hashes))},
+            },
+            "candidate_program_sigs": [p.program_sig() for p in sols_sorted],
+            "predicted_grid_hash_by_solution": {str(k): str(pred_hashes[k]) for k in sorted(pred_hashes.keys())},
+            "trace": dict(trace),
+        }
+
+    if budget_exhausted:
+        return {
+            "schema_version": 131,
+            "kind": "arc_solve_result_v131",
+            "status": "FAIL",
+            "failure_reason": {
+                "kind": "SEARCH_BUDGET_EXCEEDED",
+                "details": {"candidates_tested": int(tried), "max_programs": int(max_programs), "max_depth": int(max_depth)},
+            },
+            "trace": dict(trace),
+        }
+
+    return {
+        "schema_version": 131,
+        "kind": "arc_solve_result_v131",
+        "status": "FAIL",
+        "failure_reason": {"kind": "MISSING_OPERATOR", "details": {"max_depth": int(max_depth)}},
+        "trace": dict(trace),
+    }
+
--- /dev/null	2026-01-17 18:14:46
+++ scripts/run_arc_scalpel_v131.py	2026-01-17 18:09:02
@@ -0,0 +1,446 @@
+#!/usr/bin/env python3
+from __future__ import annotations
+
+import argparse
+import hashlib
+import json
+import os
+import sys
+from pathlib import Path
+from typing import Any, Dict, List, Optional, Sequence, Tuple
+
+# Prevent any bytecode writes outside run dirs.
+os.environ.setdefault("PYTHONDONTWRITEBYTECODE", "1")
+
+sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))
+
+
+def _ensure_absent(path: Path) -> None:
+    if path.exists():
+        raise SystemExit(f"worm_exists:{path}")
+
+
+def _sha256_file(path: Path) -> str:
+    h = hashlib.sha256()
+    with open(path, "rb") as f:
+        while True:
+            b = f.read(1024 * 1024)
+            if not b:
+                break
+            h.update(b)
+    return h.hexdigest()
+
+
+def _write_once_json(path: Path, obj: Any) -> None:
+    _ensure_absent(path)
+    path.parent.mkdir(parents=True, exist_ok=True)
+    tmp = path.with_suffix(path.suffix + ".tmp")
+    if tmp.exists():
+        raise SystemExit(f"tmp_exists:{tmp}")
+    tmp.write_text(json.dumps(obj, ensure_ascii=False, indent=2, sort_keys=True) + "\n", encoding="utf-8")
+    os.replace(str(tmp), str(path))
+
+
+def _write_text_x(path: Path, text: str) -> None:
+    _ensure_absent(path)
+    path.parent.mkdir(parents=True, exist_ok=True)
+    with open(path, "x", encoding="utf-8") as f:
+        f.write(text)
+
+
+def _excluded_dir_parts_v131() -> set:
+    return {
+        ".git",
+        "__pycache__",
+        ".pycache",
+        "results",
+        "external_world",
+        "external_world_v122",
+        "external_world_v122_try2",
+        "external_world_v122_try3",
+        "external_world_v122_try4",
+        "external_world_v122_try5",
+        "external_world_v122_try6",
+    }
+
+
+def _repo_snapshot_sha256_v131(*, root: Path, exclude_paths: Sequence[Path]) -> str:
+    excluded = _excluded_dir_parts_v131()
+    excludes = [p.resolve() for p in exclude_paths]
+    rows: List[Dict[str, Any]] = []
+    for p in root.rglob("*"):
+        if not p.is_file():
+            continue
+        if any(part in excluded for part in p.parts):
+            continue
+        rp = p.resolve()
+        if any(str(rp).startswith(str(ex)) for ex in excludes):
+            continue
+        rel = p.relative_to(root).as_posix()
+        rows.append({"path": str(rel), "sha256": _sha256_file(p)})
+    rows.sort(key=lambda r: str(r["path"]))
+    body = {"schema_version": 131, "kind": "repo_snapshot_v131", "files": rows}
+    from atos_core.act import canonical_json_dumps, sha256_hex
+
+    return sha256_hex(canonical_json_dumps(body).encode("utf-8"))
+
+
+def _sanitize_task_id(task_id: str) -> str:
+    s = "".join([c if c.isalnum() or c in ("-", "_", ".") else "_" for c in str(task_id)])
+    return s or "task"
+
+
+def _build_report_markdown_v131(*, eval_obj: Dict[str, Any], backlog: Sequence[Dict[str, Any]]) -> str:
+    total = int(eval_obj.get("tasks_total") or 0)
+    solved = int(eval_obj.get("tasks_solved") or 0)
+    unknown = int(eval_obj.get("tasks_unknown") or 0)
+    failed = int(eval_obj.get("tasks_failed") or 0)
+    failures = eval_obj.get("failure_counts")
+    failures = failures if isinstance(failures, dict) else {}
+    top = sorted(((str(k), int(failures[k])) for k in failures.keys()), key=lambda kv: (-int(kv[1]), str(kv[0])))[:15]
+
+    lines: List[str] = []
+    lines.append("# ARC_DIAG_REPORT_v131")
+    lines.append("")
+    lines.append("## Solve rate")
+    lines.append(f"- tasks_total={total} solved={solved} unknown={unknown} failed={failed}")
+    if total:
+        lines.append(f"- solve_rate={solved/total:.3f}")
+    lines.append("")
+    lines.append("## Top failures (failure_reason.kind)")
+    if not top:
+        lines.append("- (none)")
+    else:
+        for k, n in top:
+            lines.append(f"- {k}: {n}")
+    lines.append("")
+    lines.append("## Backlog (operator gaps) — propostas gerais")
+    if not backlog:
+        lines.append("- (none)")
+    else:
+        for item in backlog:
+            lines.append(f"### {item['name']}")
+            lines.append(f"- signature: `{item['signature']}`")
+            lines.append(f"- invariants: {item['invariants']}")
+            lines.append(f"- examples: {item['examples']}")
+            lines.append(f"- covers: {item['covers']}")
+            lines.append("")
+    return "\n".join(lines)
+
+
+def _derive_backlog_v131(*, failure_counts: Dict[str, int]) -> List[Dict[str, Any]]:
+    # Deterministic, general operator proposals based on common failure kinds.
+    # This is a diagnostic only; solver must not branch on these.
+    ordered = sorted(((str(k), int(failure_counts[k])) for k in failure_counts.keys()), key=lambda kv: (-kv[1], kv[0]))
+    dominant = ordered[0][0] if ordered else ""
+    out: List[Dict[str, Any]] = []
+
+    # Always include a small, general backlog (no task-specific patterns).
+    out.append(
+        {
+            "name": "connected_components4(grid[, color]) -> object_set",
+            "signature": "(GRID[, COLOR]) -> OBJECT_SET",
+            "invariants": "Determinístico; 4-neigh; objetos ordenados por bbox/cor; cores 0..9.",
+            "examples": "Ex: separar 2 blobs por cor e selecionar maior/menor por área.",
+            "covers": "MISSING_OPERATOR / AMBIGUOUS_RULE em tarefas de seleção de objeto e edição localizada.",
+        }
+    )
+    out.append(
+        {
+            "name": "paint_rect(grid, bbox, color[, mode]) -> grid",
+            "signature": "(GRID, BBOX, COLOR[, MODE]) -> GRID",
+            "invariants": "Não altera shape; pinta apenas dentro do bbox; cores 0..9.",
+            "examples": "Ex: desenhar retângulo preenchido onde delta mudou.",
+            "covers": "MISSING_OPERATOR quando o delta é retangular e envolve preenchimento/borda.",
+        }
+    )
+    out.append(
+        {
+            "name": "draw_rect_border(grid, bbox, color, thickness=1) -> grid",
+            "signature": "(GRID, BBOX, COLOR[, INT]) -> GRID",
+            "invariants": "Não altera shape; desenha contorno; thickness pequeno e determinístico.",
+            "examples": "Ex: borda do bbox do objeto não-zero.",
+            "covers": "MISSING_OPERATOR em tarefas de borda/contorno.",
+        }
+    )
+    out.append(
+        {
+            "name": "paste(base, patch, at=(r,c)[, transparent]) -> grid",
+            "signature": "(GRID, GRID, (INT,INT)[, COLOR]) -> GRID",
+            "invariants": "Determinístico; não muda shape; recorta patch se necessário; transparent opcional.",
+            "examples": "Ex: copiar um subgrid e colar em offset.",
+            "covers": "MISSING_OPERATOR em tarefas de composição/duplicação local.",
+        }
+    )
+    out.append(
+        {
+            "name": "symmetry_detect(grid) -> {axis, kind}",
+            "signature": "(GRID) -> SYMMETRY_HYP",
+            "invariants": "Somente diagnóstico/hyp; retorna evidência (mismatch count) e eixos candidatos.",
+            "examples": "Ex: detectar se o output é reflexo do input.",
+            "covers": "MISSING_OPERATOR em tarefas de flip/rotate/reflect parametrizado.",
+        }
+    )
+
+    if dominant == "SEARCH_BUDGET_EXCEEDED":
+        out.append(
+            {
+                "name": "inverse_propose(op, inp, out) -> small_candidates",
+                "signature": "(OP, GRID, GRID) -> [PARAMS]",
+                "invariants": "Só usa train pairs; candidatos ordenados; sem task_id; reduz branching.",
+                "examples": "Ex: inferir mapping de cores funcional; inferir bbox da máscara de delta.",
+                "covers": "SEARCH_BUDGET_EXCEEDED por explosão combinatória.",
+            }
+        )
+    return out[:10]
+
+
+def _build_outputs_manifest_v131(*, out_dir: Path) -> Dict[str, Any]:
+    per_task_dir = out_dir / "per_task"
+    per_task_files = [p for p in per_task_dir.glob("*.json") if p.is_file()]
+    per_task_files.sort(key=lambda p: p.name)
+
+    def rel(p: Path) -> str:
+        return p.relative_to(out_dir).as_posix()
+
+    files: List[Dict[str, Any]] = []
+    fixed = [
+        out_dir / "summary.json",
+        out_dir / "smoke_summary.json",
+        out_dir / "eval.json",
+        out_dir / "per_task_manifest.jsonl",
+        out_dir / "trace_candidates.jsonl",
+        out_dir / "ARC_DIAG_REPORT_v131.md",
+        out_dir / "isolation_check_v131.json",
+        out_dir / "input" / "arc_manifest_v131.json",
+        out_dir / "input" / "arc_tasks_canonical_v131.jsonl",
+    ]
+    for p in fixed:
+        files.append({"path": rel(p), "sha256": _sha256_file(p)})
+    for p in per_task_files:
+        files.append({"path": rel(p), "sha256": _sha256_file(p)})
+
+    body = {"schema_version": 131, "kind": "arc_outputs_manifest_v131", "files": files}
+    from atos_core.act import canonical_json_dumps, sha256_hex
+
+    body["manifest_sig"] = sha256_hex(canonical_json_dumps(body).encode("utf-8"))
+    return body
+
+
+def _run_one(*, arc_root: str, split: str, limit: int, seed: int, out_dir: Path) -> Dict[str, Any]:
+    _ensure_absent(out_dir)
+    out_dir.mkdir(parents=True, exist_ok=False)
+    os.environ["PYTHONPYCACHEPREFIX"] = str(out_dir / ".pycache")
+
+    from atos_core.act import canonical_json_dumps, sha256_hex
+    from atos_core.arc_loader_v131 import iter_canonical_tasks_v131, write_arc_canonical_jsonl_v131
+    from atos_core.arc_solver_v131 import solve_arc_task_v131
+
+    repo_root = Path(__file__).resolve().parent.parent
+    snap_before = _repo_snapshot_sha256_v131(root=repo_root, exclude_paths=[out_dir])
+
+    input_dir = out_dir / "input"
+    input_dir.mkdir(parents=True, exist_ok=False)
+    canon_jsonl = input_dir / "arc_tasks_canonical_v131.jsonl"
+    manifest = write_arc_canonical_jsonl_v131(
+        arc_root=str(arc_root), out_jsonl_path=str(canon_jsonl), limit=int(limit), split=str(split)
+    )
+    manifest_path = input_dir / "arc_manifest_v131.json"
+    _write_once_json(manifest_path, manifest)
+
+    per_task_dir = out_dir / "per_task"
+    per_task_dir.mkdir(parents=True, exist_ok=False)
+
+    per_task_manifest_path = out_dir / "per_task_manifest.jsonl"
+    trace_candidates_path = out_dir / "trace_candidates.jsonl"
+    _ensure_absent(per_task_manifest_path)
+    _ensure_absent(trace_candidates_path)
+
+    tasks_total = 0
+    tasks_solved = 0
+    tasks_unknown = 0
+    tasks_failed = 0
+    failure_counts: Dict[str, int] = {}
+
+    with open(per_task_manifest_path, "x", encoding="utf-8") as per_f, open(
+        trace_candidates_path, "x", encoding="utf-8"
+    ) as trace_f:
+        for task in iter_canonical_tasks_v131(str(canon_jsonl)):
+            tasks_total += 1
+            solve = solve_arc_task_v131(train_pairs=list(task.train_pairs), test_in=task.test_in)
+            status = str(solve.get("status") or "FAIL")
+
+            if status == "SOLVED":
+                tasks_solved += 1
+            elif status == "UNKNOWN":
+                tasks_unknown += 1
+            else:
+                tasks_failed += 1
+
+            failure_kind = ""
+            fr = solve.get("failure_reason")
+            if isinstance(fr, dict):
+                failure_kind = str(fr.get("kind") or "")
+            if status != "SOLVED":
+                k = failure_kind or "unknown_failure_kind"
+                failure_counts[k] = int(failure_counts.get(k, 0)) + 1
+
+            per = {
+                "schema_version": 131,
+                "kind": "arc_task_result_v131",
+                "task_id": str(task.task_id),
+                "input": task.to_dict(),
+                "result": dict(solve),
+            }
+            per["per_task_sig"] = sha256_hex(canonical_json_dumps(per).encode("utf-8"))
+            task_path = per_task_dir / (_sanitize_task_id(task.task_id) + ".json")
+            _write_once_json(task_path, per)
+
+            pm_row = {
+                "schema_version": 131,
+                "kind": "arc_per_task_manifest_row_v131",
+                "task_id": str(task.task_id),
+                "status": str(status),
+                "failure_kind": str(failure_kind),
+                "program_sig": str(solve.get("program_sig") or ""),
+                "program_cost_bits": int(solve.get("program_cost_bits") or 0),
+                "predicted_grid_hash": str(solve.get("predicted_grid_hash") or ""),
+                "per_task_sha256": _sha256_file(task_path),
+            }
+            pm_row["row_sig"] = sha256_hex(canonical_json_dumps(pm_row).encode("utf-8"))
+            per_f.write(canonical_json_dumps(pm_row))
+            per_f.write("\n")
+
+            tr = solve.get("trace") if isinstance(solve.get("trace"), dict) else {}
+            trace_programs = tr.get("trace_programs") if isinstance(tr.get("trace_programs"), list) else []
+            for i, cand in enumerate(trace_programs):
+                if not isinstance(cand, dict):
+                    continue
+                row = {
+                    "schema_version": 131,
+                    "kind": "arc_candidate_trace_row_v131",
+                    "task_id": str(task.task_id),
+                    "candidate_index": int(i),
+                    "program_sig": str(cand.get("program_sig") or ""),
+                    "cost_bits": int(cand.get("cost_bits") or 0),
+                    "depth": int(cand.get("depth") or 0),
+                    "ok_train": bool(cand.get("ok_train")),
+                    "mismatch": cand.get("mismatch"),
+                }
+                row["row_sig"] = sha256_hex(canonical_json_dumps(row).encode("utf-8"))
+                trace_f.write(canonical_json_dumps(row))
+                trace_f.write("\n")
+
+    eval_obj = {
+        "schema_version": 131,
+        "kind": "arc_eval_v131",
+        "seed": int(seed),
+        "arc_root": str(arc_root),
+        "split": str(split),
+        "limit": int(limit),
+        "tasks_total": int(tasks_total),
+        "tasks_solved": int(tasks_solved),
+        "tasks_unknown": int(tasks_unknown),
+        "tasks_failed": int(tasks_failed),
+        "failure_counts": {str(k): int(failure_counts[k]) for k in sorted(failure_counts.keys())},
+        "sha256": {
+            "arc_canonical_jsonl": _sha256_file(canon_jsonl),
+            "arc_manifest_json": _sha256_file(manifest_path),
+            "per_task_manifest_jsonl": _sha256_file(per_task_manifest_path),
+            "trace_candidates_jsonl": _sha256_file(trace_candidates_path),
+        },
+    }
+    eval_obj["eval_sig"] = sha256_hex(canonical_json_dumps(eval_obj).encode("utf-8"))
+    eval_path = out_dir / "eval.json"
+    _write_once_json(eval_path, eval_obj)
+
+    summary = {
+        "schema_version": 131,
+        "kind": "arc_summary_v131",
+        "seed": int(seed),
+        "tasks_total": int(tasks_total),
+        "tasks_solved": int(tasks_solved),
+        "tasks_unknown": int(tasks_unknown),
+        "tasks_failed": int(tasks_failed),
+        "solve_rate": float(tasks_solved / tasks_total) if tasks_total else 0.0,
+        "eval_sha256": _sha256_file(eval_path),
+    }
+    from atos_core.act import sha256_hex as _sha
+
+    summary_sha256 = _sha(canonical_json_dumps(summary).encode("utf-8"))
+    summary["summary_sha256"] = str(summary_sha256)
+    summary["summary_sig"] = sha256_hex(canonical_json_dumps(summary).encode("utf-8"))
+    _write_once_json(out_dir / "summary.json", summary)
+    _write_once_json(out_dir / "smoke_summary.json", {"summary_sha256": str(summary_sha256), "eval_sha256": _sha256_file(eval_path)})
+
+    backlog = _derive_backlog_v131(failure_counts=failure_counts)
+    report_text = _build_report_markdown_v131(eval_obj=eval_obj, backlog=backlog)
+    _write_text_x(out_dir / "ARC_DIAG_REPORT_v131.md", report_text)
+
+    snap_after = _repo_snapshot_sha256_v131(root=repo_root, exclude_paths=[out_dir])
+    isolation = {
+        "schema_version": 131,
+        "kind": "arc_isolation_check_v131",
+        "excluded_dir_parts": sorted(list(_excluded_dir_parts_v131())),
+        "repo_snapshot_before": str(snap_before),
+        "repo_snapshot_after": str(snap_after),
+        "ok": bool(str(snap_before) == str(snap_after)),
+    }
+    _write_once_json(out_dir / "isolation_check_v131.json", isolation)
+
+    outputs_manifest = _build_outputs_manifest_v131(out_dir=out_dir)
+    _write_once_json(out_dir / "outputs_manifest.json", outputs_manifest)
+
+    return {
+        "out_dir": str(out_dir),
+        "summary_sha256": str(summary_sha256),
+        "eval_sha256": _sha256_file(eval_path),
+        "isolation_ok": bool(isolation["ok"]),
+        "outputs_manifest_sig": str(outputs_manifest.get("manifest_sig") or ""),
+    }
+
+
+def main(argv: Optional[Sequence[str]] = None) -> None:
+    ap = argparse.ArgumentParser()
+    ap.add_argument("--arc_root", default="data/arc_v124_sample")
+    ap.add_argument("--split", default="sample")
+    ap.add_argument("--limit", type=int, default=999999)
+    ap.add_argument("--seed", type=int, default=0)
+    ap.add_argument("--out_base", required=True)
+    args = ap.parse_args(list(argv) if argv is not None else None)
+
+    out_base = Path(str(args.out_base))
+    out_try1 = Path(str(out_base) + "_try1")
+    out_try2 = Path(str(out_base) + "_try2")
+    _ensure_absent(out_try1)
+    _ensure_absent(out_try2)
+
+    r1 = _run_one(arc_root=str(args.arc_root), split=str(args.split), limit=int(args.limit), seed=int(args.seed), out_dir=out_try1)
+    r2 = _run_one(arc_root=str(args.arc_root), split=str(args.split), limit=int(args.limit), seed=int(args.seed), out_dir=out_try2)
+
+    ok = bool(r1["summary_sha256"] == r2["summary_sha256"]) and bool(
+        r1["outputs_manifest_sig"] == r2["outputs_manifest_sig"]
+    )
+    if not ok:
+        raise SystemExit("determinism_mismatch_v131")
+
+    if not bool(r1["isolation_ok"]) or not bool(r2["isolation_ok"]):
+        raise SystemExit("isolation_failed_v131")
+
+    out = {
+        "ok": True,
+        "determinism_ok": True,
+        "schema_version": 131,
+        "out_try1": str(out_try1),
+        "out_try2": str(out_try2),
+        "summary_sha256": str(r1["summary_sha256"]),
+        "eval_sha256_try1": str(r1["eval_sha256"]),
+        "eval_sha256_try2": str(r2["eval_sha256"]),
+        "outputs_manifest_sig": str(r1["outputs_manifest_sig"]),
+    }
+    print(json.dumps(out, sort_keys=True))
+
+
+if __name__ == "__main__":
+    main()
+
--- /dev/null	2026-01-17 18:14:46
+++ tests/test_arc_solver_v131.py	2026-01-17 18:09:18
@@ -0,0 +1,50 @@
+from __future__ import annotations
+
+import unittest
+
+from atos_core.arc_solver_v131 import solve_arc_task_v131
+from atos_core.grid_v124 import grid_from_list_v124
+
+
+class TestArcSolverV131(unittest.TestCase):
+    def test_determinism_same_seed(self) -> None:
+        inp = grid_from_list_v124([[1, 2], [3, 4]])
+        out = grid_from_list_v124([[3, 1], [4, 2]])  # rotate90
+        test_in = grid_from_list_v124([[9, 8], [7, 6]])
+
+        r1 = solve_arc_task_v131(train_pairs=[(inp, out)], test_in=test_in, max_depth=1, max_programs=200)
+        r2 = solve_arc_task_v131(train_pairs=[(inp, out)], test_in=test_in, max_depth=1, max_programs=200)
+        self.assertEqual(r1.get("status"), "SOLVED")
+        self.assertEqual(r2.get("status"), "SOLVED")
+        self.assertEqual(r1.get("program_sig"), r2.get("program_sig"))
+        self.assertEqual(r1.get("predicted_grid_hash"), r2.get("predicted_grid_hash"))
+
+    def test_ambiguous_rule_fail_closed(self) -> None:
+        # Train pair where both rotate90 and reflect_v map inp -> out (same MDL), but they diverge on test_in.
+        inp = grid_from_list_v124([[1, 2], [3, 1]])
+        out = grid_from_list_v124([[3, 1], [1, 2]])
+        test_in = grid_from_list_v124([[1, 0], [2, 3]])
+
+        r = solve_arc_task_v131(train_pairs=[(inp, out)], test_in=test_in, max_depth=1, max_programs=500)
+        self.assertEqual(r.get("status"), "UNKNOWN")
+        fr = r.get("failure_reason")
+        self.assertIsInstance(fr, dict)
+        self.assertEqual(fr.get("kind"), "AMBIGUOUS_RULE")
+        hashes = r.get("predicted_grid_hash_by_solution")
+        self.assertIsInstance(hashes, dict)
+        self.assertGreaterEqual(len(hashes.keys()), 2)
+
+    def test_invariant_violation_classification(self) -> None:
+        # Invalid grid values must fail deterministically with an invariant failure, not a crash.
+        bad_inp = ((10,),)  # out of allowed 0..9
+        out = grid_from_list_v124([[0]])
+        r = solve_arc_task_v131(train_pairs=[(bad_inp, out)], test_in=grid_from_list_v124([[0]]))
+        self.assertEqual(r.get("status"), "FAIL")
+        fr = r.get("failure_reason")
+        self.assertIsInstance(fr, dict)
+        self.assertEqual(fr.get("kind"), "INVARIANT_VIOLATION")
+
+
+if __name__ == "__main__":
+    unittest.main()
+
