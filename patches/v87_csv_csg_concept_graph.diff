--- /dev/null
+++ atos_core/csg_v87.py
@@ -0,0 +1,473 @@
+from __future__ import annotations
+
+import copy
+import json
+import os
+from dataclasses import dataclass
+from typing import Any, Dict, Iterable, Iterator, List, Optional, Sequence, Tuple
+
+from .act import Instruction, canonical_json_dumps, deterministic_iso, sha256_hex
+from .store import ActStore
+
+
+def _stable_hash_obj(obj: Any) -> str:
+    return sha256_hex(canonical_json_dumps(obj).encode("utf-8"))
+
+
+def _ensure_str_list(items: Any) -> List[str]:
+    if not isinstance(items, list):
+        return []
+    out: List[str] = []
+    for x in items:
+        if isinstance(x, str) and x:
+            out.append(x)
+    # stable unique
+    return sorted(set(out))
+
+
+def _canon_goal_kinds(match: Dict[str, Any]) -> Dict[str, Any]:
+    m = dict(match) if isinstance(match, dict) else {}
+    gk = m.get("goal_kinds")
+    if isinstance(gk, list):
+        m["goal_kinds"] = _ensure_str_list(gk)
+    return m
+
+
+def _read_jsonl(path: str) -> Iterator[Dict[str, Any]]:
+    if not os.path.exists(path):
+        return iter(())
+    with open(path, "r", encoding="utf-8") as f:
+        for line in f:
+            line = line.strip()
+            if not line:
+                continue
+            yield json.loads(line)
+
+
+def append_chained_jsonl_v87(path: str, entry: Dict[str, Any], *, prev_hash: Optional[str]) -> str:
+    os.makedirs(os.path.dirname(path) or ".", exist_ok=True)
+    body = dict(entry)
+    body["prev_hash"] = prev_hash
+    entry_hash = sha256_hex(canonical_json_dumps(body).encode("utf-8"))
+    body["entry_hash"] = entry_hash
+    with open(path, "a", encoding="utf-8") as f:
+        f.write(canonical_json_dumps(body))
+        f.write("\n")
+    return entry_hash
+
+
+def verify_chained_jsonl_v87(path: str) -> bool:
+    prev: Optional[str] = None
+    for row in _read_jsonl(path):
+        row = dict(row)
+        entry_hash = row.pop("entry_hash", None)
+        if row.get("prev_hash") != prev:
+            return False
+        expected = sha256_hex(canonical_json_dumps(row).encode("utf-8"))
+        if expected != entry_hash:
+            return False
+        prev = str(entry_hash)
+    return True
+
+
+def _canon_bind_map(bind_map: Any) -> Dict[str, str]:
+    if not isinstance(bind_map, dict):
+        return {}
+    out: Dict[str, str] = {}
+    for k, v in bind_map.items():
+        ks = str(k)
+        vs = str(v)
+        if not ks or not vs:
+            continue
+        out[ks] = vs
+    return {k: out[k] for k in sorted(out.keys())}
+
+
+def _canon_nodes(nodes: Any) -> List[Dict[str, Any]]:
+    if not isinstance(nodes, list):
+        return []
+    out: List[Dict[str, Any]] = []
+    for n in nodes:
+        if not isinstance(n, dict):
+            continue
+        act_id = str(n.get("act_id") or "")
+        if not act_id:
+            continue
+        produces = str(n.get("produces") or n.get("out") or "")
+        role = str(n.get("role") or "")
+        node = {
+            "act_id": act_id,
+            "bind": _canon_bind_map(n.get("bind") if "bind" in n else n.get("bind_map")),
+            "produces": produces,
+        }
+        if role:
+            node["role"] = role
+        out.append(node)
+    return out
+
+
+def _toposort_nodes(nodes: Sequence[Dict[str, Any]]) -> List[int]:
+    """
+    Deterministic topological sort using node_sig tie-breaks.
+    Cycles fail-closed.
+    """
+    node_sigs: List[str] = [_stable_hash_obj(n) for n in nodes]
+    produces_map: Dict[str, int] = {}
+    for idx, n in enumerate(nodes):
+        pv = str(n.get("produces") or "")
+        if not pv:
+            continue
+        if pv in produces_map:
+            raise ValueError(f"duplicate_produces_var:{pv}")
+        produces_map[pv] = idx
+
+    deps: List[set] = [set() for _ in nodes]
+    outs: List[set] = [set() for _ in nodes]
+    for j, n in enumerate(nodes):
+        uses = n.get("bind")
+        if not isinstance(uses, dict):
+            continue
+        for v in uses.values():
+            vs = str(v)
+            if not vs:
+                continue
+            i = produces_map.get(vs)
+            if i is None:
+                continue
+            deps[j].add(i)
+            outs[i].add(j)
+
+    incoming = [len(d) for d in deps]
+    ready: List[int] = [i for i, c in enumerate(incoming) if c == 0]
+    ready.sort(key=lambda i: node_sigs[i])
+
+    order: List[int] = []
+    while ready:
+        i = ready.pop(0)
+        order.append(i)
+        for j in sorted(outs[i], key=lambda x: node_sigs[x]):
+            incoming[j] -= 1
+            if incoming[j] == 0:
+                ready.append(j)
+                ready.sort(key=lambda x: node_sigs[x])
+
+    if len(order) != len(nodes):
+        raise ValueError("cycle_detected")
+    return order
+
+
+def _derive_edges(nodes: Sequence[Dict[str, Any]]) -> List[Dict[str, Any]]:
+    # Build mapping var->producer index.
+    produces_map: Dict[str, int] = {}
+    for idx, n in enumerate(nodes):
+        pv = str(n.get("produces") or "")
+        if pv:
+            produces_map[pv] = idx
+
+    edges: List[Dict[str, Any]] = []
+    for dst_idx, n in enumerate(nodes):
+        bind = n.get("bind")
+        if not isinstance(bind, dict):
+            continue
+        for var in bind.values():
+            v = str(var)
+            if not v:
+                continue
+            src_idx = produces_map.get(v)
+            if src_idx is None:
+                continue
+            if int(src_idx) == int(dst_idx):
+                continue
+            edges.append({"src": int(src_idx), "dst": int(dst_idx), "var": str(v)})
+    edges.sort(key=lambda e: (int(e["src"]), int(e["dst"]), str(e["var"])))
+    return edges
+
+
+def _infer_interface_from_nodes(nodes: Sequence[Dict[str, Any]]) -> Tuple[List[str], List[str]]:
+    produced: set = set()
+    used: set = set()
+    for n in nodes:
+        pv = str(n.get("produces") or "")
+        if pv:
+            produced.add(pv)
+        bind = n.get("bind")
+        if isinstance(bind, dict):
+            for v in bind.values():
+                vs = str(v)
+                if vs:
+                    used.add(vs)
+    inputs = sorted(used - produced)
+    outputs = sorted(produced - used) if produced else []
+    return inputs, outputs
+
+
+def canonicalize_csg_v87(csg: Dict[str, Any]) -> Dict[str, Any]:
+    """
+    Canonical CSG schema (V87):
+      {
+        "schema_version": 1,
+        "nodes": [{"act_id","bind","produces","role"?}, ...],   # topologically sorted
+        "edges": [{"src","dst","var"}, ...],                   # derived from bind/produces
+        "interface": {
+           "inputs": [...],
+           "outputs": [...],
+           "validator_ids": [...],
+           "match": {...}
+        }
+      }
+    """
+    if not isinstance(csg, dict):
+        raise ValueError("csg_not_dict")
+    schema_version = int(csg.get("schema_version", 1) or 1)
+    if schema_version != 1:
+        raise ValueError(f"unsupported_csg_schema_version:{schema_version}")
+
+    nodes_raw = _canon_nodes(csg.get("nodes"))
+    if not nodes_raw:
+        raise ValueError("empty_nodes")
+
+    # Canonical order is deterministic topological sort.
+    order = _toposort_nodes(nodes_raw)
+    nodes: List[Dict[str, Any]] = [nodes_raw[i] for i in order]
+    edges = _derive_edges(nodes)
+
+    iface = csg.get("interface")
+    iface = dict(iface) if isinstance(iface, dict) else {}
+    inf_inputs, inf_outputs = _infer_interface_from_nodes(nodes)
+    iface_inputs = _ensure_str_list(iface.get("inputs")) if "inputs" in iface else inf_inputs
+    iface_outputs = _ensure_str_list(iface.get("outputs")) if "outputs" in iface else inf_outputs
+    if "inputs" in iface and iface_inputs != inf_inputs:
+        raise ValueError(f"interface_inputs_mismatch:want={iface_inputs}:got={inf_inputs}")
+    if "outputs" in iface and iface_outputs != inf_outputs:
+        raise ValueError(f"interface_outputs_mismatch:want={iface_outputs}:got={inf_outputs}")
+
+    validator_ids = _ensure_str_list(iface.get("validator_ids"))
+    match = _canon_goal_kinds(iface.get("match") if isinstance(iface.get("match"), dict) else {})
+
+    iface_canon: Dict[str, Any] = {
+        "inputs": list(iface_inputs),
+        "outputs": list(iface_outputs),
+        "validator_ids": list(validator_ids),
+        "match": dict(match),
+    }
+    return {
+        "schema_version": 1,
+        "nodes": list(nodes),
+        "edges": list(edges),
+        "interface": iface_canon,
+    }
+
+
+def csg_hash_v87(csg: Dict[str, Any]) -> str:
+    canon = canonicalize_csg_v87(csg)
+    return sha256_hex(canonical_json_dumps(canon).encode("utf-8"))
+
+
+def estimate_cost_v87(csg: Dict[str, Any], store: ActStore) -> Dict[str, Any]:
+    canon = canonicalize_csg_v87(csg)
+    nodes = canon.get("nodes") if isinstance(canon.get("nodes"), list) else []
+    edges = canon.get("edges") if isinstance(canon.get("edges"), list) else []
+
+    uniq_act_ids: List[str] = []
+    seen: set = set()
+    for n in nodes:
+        if not isinstance(n, dict):
+            continue
+        aid = str(n.get("act_id") or "")
+        if aid and aid not in seen:
+            seen.add(aid)
+            uniq_act_ids.append(aid)
+
+    prog_len_total = 0
+    for aid in sorted(uniq_act_ids):
+        act = store.get_concept_act(str(aid))
+        if act is None:
+            continue
+        prog_len_total += int(len(act.program or []))
+
+    cost_units = int(len(nodes)) * 1000 + int(len(edges)) * 100 + int(prog_len_total)
+    return {
+        "schema_version": 1,
+        "estimate_kind": "v87_simple",
+        "nodes": int(len(nodes)),
+        "edges": int(len(edges)),
+        "act_program_len_total": int(prog_len_total),
+        "cost_units": int(cost_units),
+    }
+
+
+def csg_expand_v87(csg: Dict[str, Any], store: ActStore) -> List[Dict[str, Any]]:
+    """
+    Deterministic linearization of a CSG for replay/audit.
+    Returns a list of CALL-like steps: {"act_id","bind","produces","idx"}.
+    """
+    canon = canonicalize_csg_v87(csg)
+    nodes = canon.get("nodes") if isinstance(canon.get("nodes"), list) else []
+    steps: List[Dict[str, Any]] = []
+    for idx, n in enumerate(nodes):
+        if not isinstance(n, dict):
+            continue
+        aid = str(n.get("act_id") or "")
+        if not aid:
+            continue
+        if store.get_concept_act(aid) is None:
+            raise ValueError(f"missing_node_act:{aid}")
+        steps.append(
+            {
+                "idx": int(idx),
+                "act_id": str(aid),
+                "bind": dict(n.get("bind") if isinstance(n.get("bind"), dict) else {}),
+                "produces": str(n.get("produces") or ""),
+                "role": str(n.get("role") or ""),
+            }
+        )
+    return steps
+
+
+def csg_to_concept_program_v87(csg: Dict[str, Any]) -> List[Instruction]:
+    """
+    Convert CSG nodes into a linear concept_csv program using CSV_CALL steps.
+    The program assumes interface.inputs are provided as direct inputs with the same names.
+    Returns the first output in interface.outputs via CSV_RETURN.
+    """
+    canon = canonicalize_csg_v87(csg)
+    iface = canon.get("interface") if isinstance(canon.get("interface"), dict) else {}
+    inputs = iface.get("inputs") if isinstance(iface.get("inputs"), list) else []
+    outputs = iface.get("outputs") if isinstance(iface.get("outputs"), list) else []
+    nodes = canon.get("nodes") if isinstance(canon.get("nodes"), list) else []
+
+    prog: List[Instruction] = []
+    for name in inputs:
+        n = str(name)
+        if not n:
+            continue
+        prog.append(Instruction("CSV_GET_INPUT", {"name": n, "out": n}))
+
+    for n in nodes:
+        if not isinstance(n, dict):
+            continue
+        callee = str(n.get("act_id") or "")
+        bind = n.get("bind") if isinstance(n.get("bind"), dict) else {}
+        out = str(n.get("produces") or "")
+        prog.append(Instruction("CSV_CALL", {"concept_id": callee, "bind": dict(bind), "out": out}))
+
+    ret_var = str(outputs[0]) if outputs else str(nodes[-1].get("produces") or "")
+    if not ret_var:
+        raise ValueError("missing_return_var")
+    prog.append(Instruction("CSV_RETURN", {"var": ret_var}))
+    return prog
+
+
+@dataclass
+class CsvCsgLogsV87:
+    run_dir: str
+    concepts_path: str
+    evidence_path: str
+    telemetry_path: str
+    _concepts_prev: Optional[str] = None
+    _evidence_prev: Optional[str] = None
+    _telemetry_prev: Optional[str] = None
+
+    @staticmethod
+    def init(run_dir: str) -> "CsvCsgLogsV87":
+        os.makedirs(run_dir, exist_ok=False)
+        return CsvCsgLogsV87(
+            run_dir=str(run_dir),
+            concepts_path=os.path.join(run_dir, "concepts.jsonl"),
+            evidence_path=os.path.join(run_dir, "concept_evidence.jsonl"),
+            telemetry_path=os.path.join(run_dir, "concept_telemetry.jsonl"),
+        )
+
+    def append_concept_def(self, *, step: int, concept: Dict[str, Any]) -> str:
+        self._concepts_prev = append_chained_jsonl_v87(
+            self.concepts_path,
+            {
+                "time": deterministic_iso(step=int(step)),
+                "step": int(step),
+                "event": "CONCEPT_DEF",
+                "payload": dict(concept),
+            },
+            prev_hash=self._concepts_prev,
+        )
+        return str(self._concepts_prev)
+
+    def append_evidence(self, *, step: int, evidence: Dict[str, Any]) -> str:
+        self._evidence_prev = append_chained_jsonl_v87(
+            self.evidence_path,
+            {
+                "time": deterministic_iso(step=int(step)),
+                "step": int(step),
+                "event": "EVIDENCE",
+                "payload": dict(evidence),
+            },
+            prev_hash=self._evidence_prev,
+        )
+        return str(self._evidence_prev)
+
+    def append_telemetry(self, *, step: int, telemetry: Dict[str, Any]) -> str:
+        self._telemetry_prev = append_chained_jsonl_v87(
+            self.telemetry_path,
+            {
+                "time": deterministic_iso(step=int(step)),
+                "step": int(step),
+                "event": "TELEMETRY",
+                "payload": dict(telemetry),
+            },
+            prev_hash=self._telemetry_prev,
+        )
+        return str(self._telemetry_prev)
+
+    def verify_chains(self) -> Dict[str, Any]:
+        return {
+            "concepts_chain_ok": bool(verify_chained_jsonl_v87(self.concepts_path)),
+            "evidence_chain_ok": bool(verify_chained_jsonl_v87(self.evidence_path)),
+            "telemetry_chain_ok": bool(verify_chained_jsonl_v87(self.telemetry_path)),
+        }
+
+
+def build_csg_concept_def_v87(
+    *,
+    csg: Dict[str, Any],
+    store: ActStore,
+    concept_id_prefix: str = "concept_csv_v87_",
+) -> Dict[str, Any]:
+    """
+    Build a deterministic concept definition (registry row) for a CSG-backed concept.
+    """
+    csg_canon = canonicalize_csg_v87(csg)
+    csg_hash = sha256_hex(canonical_json_dumps(csg_canon).encode("utf-8"))
+    iface = csg_canon.get("interface") if isinstance(csg_canon.get("interface"), dict) else {}
+    match = iface.get("match") if isinstance(iface.get("match"), dict) else {}
+    cost = estimate_cost_v87(csg_canon, store)
+    concept_id = f"{str(concept_id_prefix)}{csg_hash}"
+    concept = {
+        "schema_version": 1,
+        "concept_id": str(concept_id),
+        "csg_hash": str(csg_hash),
+        "csg": dict(csg_canon),
+        "interface": dict(iface),
+        "match": dict(match),
+        "cost": dict(cost),
+        "concept_sig": _stable_hash_obj(
+            {
+                "concept_id": str(concept_id),
+                "csg_hash": str(csg_hash),
+                "interface": dict(iface),
+                "match": dict(match),
+                "cost": dict(cost),
+            }
+        ),
+    }
+    return concept
+
+
+def maybe_deepcopy(obj: Any) -> Any:
+    try:
+        return copy.deepcopy(obj)
+    except Exception:
+        if isinstance(obj, dict):
+            return dict(obj)
+        if isinstance(obj, list):
+            return list(obj)
+        return obj
+
--- /dev/null
+++ scripts/smoke_csv_csg_v87.py
@@ -0,0 +1,459 @@
+#!/usr/bin/env python3
+from __future__ import annotations
+
+import argparse
+import hashlib
+import json
+import os
+import sys
+from typing import Any, Dict, List, Optional, Tuple
+
+sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))
+
+from atos_core.act import Act, Instruction, canonical_json_dumps, deterministic_iso, sha256_hex
+from atos_core.csg_v87 import (
+    CsvCsgLogsV87,
+    build_csg_concept_def_v87,
+    canonicalize_csg_v87,
+    csg_expand_v87,
+    csg_hash_v87,
+    csg_to_concept_program_v87,
+)
+from atos_core.engine_v80 import EngineV80
+from atos_core.store import ActStore
+
+
+def sha256_file(path: str) -> str:
+    h = hashlib.sha256()
+    with open(path, "rb") as f:
+        while True:
+            b = f.read(1024 * 1024)
+            if not b:
+                break
+            h.update(b)
+    return h.hexdigest()
+
+
+def sha256_text(s: str) -> str:
+    return hashlib.sha256(str(s).encode("utf-8")).hexdigest()
+
+
+def sha256_canon(obj: Any) -> str:
+    return sha256_hex(canonical_json_dumps(obj).encode("utf-8"))
+
+
+def _fail(msg: str, *, code: int = 2) -> None:
+    print(msg, file=sys.stderr)
+    raise SystemExit(code)
+
+
+def ensure_absent(path: str) -> None:
+    if os.path.exists(path):
+        _fail(f"ERROR: path already exists: {path}")
+
+
+def write_json(path: str, obj: Any) -> str:
+    ensure_absent(path)
+    tmp = path + ".tmp"
+    with open(tmp, "w", encoding="utf-8") as f:
+        f.write(json.dumps(obj, ensure_ascii=False, indent=2, sort_keys=True))
+        f.write("\n")
+    os.replace(tmp, path)
+    return sha256_file(path)
+
+
+def make_concept_act(
+    *,
+    act_id: str,
+    input_schema: Dict[str, str],
+    output_schema: Dict[str, str],
+    validator_id: str,
+    program: List[Instruction],
+    match: Optional[Dict[str, Any]] = None,
+    evidence_extra: Optional[Dict[str, Any]] = None,
+) -> Act:
+    ev: Dict[str, Any] = {
+        "interface": {
+            "input_schema": dict(input_schema),
+            "output_schema": dict(output_schema),
+            "validator_id": str(validator_id),
+        }
+    }
+    if isinstance(evidence_extra, dict) and evidence_extra:
+        ev.update(dict(evidence_extra))
+    return Act(
+        id=str(act_id),
+        version=1,
+        created_at=deterministic_iso(step=0),
+        kind="concept_csv",
+        match=dict(match) if isinstance(match, dict) else {},
+        program=list(program),
+        evidence=ev,
+        cost={},
+        deps=[],
+        active=True,
+    )
+
+
+def _execute_inline_replay(
+    *,
+    engine: EngineV80,
+    steps: List[Dict[str, Any]],
+    inputs: Dict[str, Any],
+    goal_kind: str,
+) -> Dict[str, Any]:
+    env: Dict[str, Any] = dict(inputs)
+    replay_calls: List[Dict[str, Any]] = []
+    for st in steps:
+        act_id = str(st.get("act_id") or "")
+        bind = st.get("bind") if isinstance(st.get("bind"), dict) else {}
+        produces = str(st.get("produces") or "")
+        sub_inputs: Dict[str, Any] = {}
+        for slot, var in bind.items():
+            sub_inputs[str(slot)] = env.get(str(var))
+        res = engine.execute_concept_csv(
+            concept_act_id=str(act_id),
+            inputs=sub_inputs,
+            goal_kind=str(goal_kind),
+            expected=None,
+            step=0,
+            max_depth=6,
+            max_events=256,
+            validate_output=False,
+        )
+        replay_calls.append(
+            {
+                "act_id": str(act_id),
+                "inputs_sig": sha256_canon(sub_inputs),
+                "meta_ok": bool((res.get("meta") or {}).get("ok", False)),
+                "meta_reason": str((res.get("meta") or {}).get("reason", "")),
+                "output_sig": str((res.get("meta") or {}).get("output_sig", "")),
+            }
+        )
+        meta = res.get("meta") if isinstance(res.get("meta"), dict) else {}
+        if not bool(meta.get("ok", False)):
+            return {"ok": False, "reason": "inline_step_failed", "failing_act_id": str(act_id), "meta": dict(meta)}
+        env[produces] = res.get("output")
+    last_out = steps[-1].get("produces") if steps else ""
+    return {
+        "ok": True,
+        "output": env.get(str(last_out)),
+        "replay_calls": list(replay_calls),
+    }
+
+
+def _assert_match_disallowed(*, exec_res: Dict[str, Any], want_concept_id: str, want_goal_kind: str) -> None:
+    meta = exec_res.get("meta") if isinstance(exec_res.get("meta"), dict) else {}
+    if bool(meta.get("ok", True)):
+        _fail(f"ERROR: expected ok=false, got meta.ok=true: {meta}")
+    if str(meta.get("reason") or "") != "match_disallowed":
+        _fail(f"ERROR: expected reason=match_disallowed, got={meta.get('reason')}")
+    if str(meta.get("concept_id") or "") != str(want_concept_id):
+        _fail(f"ERROR: expected concept_id={want_concept_id}, got={meta.get('concept_id')}")
+    if str(meta.get("goal_kind") or "") != str(want_goal_kind):
+        _fail(f"ERROR: expected goal_kind={want_goal_kind}, got={meta.get('goal_kind')}")
+    tr = exec_res.get("trace") if isinstance(exec_res.get("trace"), dict) else {}
+    calls = tr.get("concept_calls") if isinstance(tr.get("concept_calls"), list) else []
+    if not calls:
+        _fail("ERROR: expected trace.concept_calls non-empty for blocked call")
+    call0 = calls[0] if isinstance(calls[0], dict) else {}
+    if not bool(call0.get("blocked", False)):
+        _fail(f"ERROR: expected blocked=true in concept_calls[0], got={call0}")
+    if str(call0.get("blocked_reason") or "") != "match_disallowed":
+        _fail(f"ERROR: expected blocked_reason=match_disallowed, got={call0.get('blocked_reason')}")
+
+
+def smoke_try(*, out_dir: str, seed: int) -> Dict[str, Any]:
+    logs = CsvCsgLogsV87.init(str(out_dir))
+    store = ActStore()
+
+    # Base micro-world concepts.
+    normalize_x_id = "concept_v87_normalize_x_v0"
+    normalize_y_id = "concept_v87_normalize_y_v0"
+    add_nx_ny_id = "concept_v87_add_nx_ny_v0"
+
+    store.add(
+        make_concept_act(
+            act_id=normalize_x_id,
+            input_schema={"x": "str"},
+            output_schema={"nx": "str"},
+            validator_id="text_exact",
+            program=[
+                Instruction("CSV_GET_INPUT", {"name": "x", "out": "x"}),
+                Instruction("CSV_PRIMITIVE", {"fn": "scan_digits", "in": ["x"], "out": "d"}),
+                Instruction("CSV_PRIMITIVE", {"fn": "digits_to_int", "in": ["d"], "out": "i"}),
+                Instruction("CSV_PRIMITIVE", {"fn": "int_to_digits", "in": ["i"], "out": "nx"}),
+                Instruction("CSV_RETURN", {"var": "nx"}),
+            ],
+        )
+    )
+    store.add(
+        make_concept_act(
+            act_id=normalize_y_id,
+            input_schema={"y": "str"},
+            output_schema={"ny": "str"},
+            validator_id="text_exact",
+            program=[
+                Instruction("CSV_GET_INPUT", {"name": "y", "out": "y"}),
+                Instruction("CSV_PRIMITIVE", {"fn": "scan_digits", "in": ["y"], "out": "d"}),
+                Instruction("CSV_PRIMITIVE", {"fn": "digits_to_int", "in": ["d"], "out": "i"}),
+                Instruction("CSV_PRIMITIVE", {"fn": "int_to_digits", "in": ["i"], "out": "ny"}),
+                Instruction("CSV_RETURN", {"var": "ny"}),
+            ],
+        )
+    )
+    store.add(
+        make_concept_act(
+            act_id=add_nx_ny_id,
+            input_schema={"nx": "str", "ny": "str"},
+            output_schema={"sum": "str"},
+            validator_id="text_exact",
+            program=[
+                Instruction("CSV_GET_INPUT", {"name": "nx", "out": "nx"}),
+                Instruction("CSV_GET_INPUT", {"name": "ny", "out": "ny"}),
+                Instruction("CSV_PRIMITIVE", {"fn": "scan_digits", "in": ["nx"], "out": "dx"}),
+                Instruction("CSV_PRIMITIVE", {"fn": "scan_digits", "in": ["ny"], "out": "dy"}),
+                Instruction("CSV_PRIMITIVE", {"fn": "digits_to_int", "in": ["dx"], "out": "ix"}),
+                Instruction("CSV_PRIMITIVE", {"fn": "digits_to_int", "in": ["dy"], "out": "iy"}),
+                Instruction("CSV_PRIMITIVE", {"fn": "add_int", "in": ["ix", "iy"], "out": "sum_i"}),
+                Instruction("CSV_PRIMITIVE", {"fn": "int_to_digits", "in": ["sum_i"], "out": "sum"}),
+                Instruction("CSV_RETURN", {"var": "sum"}),
+            ],
+        )
+    )
+
+    allowed_goal_kind = "v87_sum"
+    disallowed_goal_kind = "v87_forbidden"
+
+    # Explicit CSG (Concept SubGraph) definition for a composed concept.
+    csg = {
+        "schema_version": 1,
+        "nodes": [
+            {"act_id": normalize_x_id, "bind": {"x": "x"}, "produces": "nx", "role": "entry"},
+            {"act_id": normalize_y_id, "bind": {"y": "y"}, "produces": "ny", "role": "entry"},
+            {"act_id": add_nx_ny_id, "bind": {"nx": "nx", "ny": "ny"}, "produces": "sum", "role": "exit"},
+        ],
+        "interface": {
+            "inputs": ["x", "y"],
+            "outputs": ["sum"],
+            "validator_ids": ["text_exact"],
+            "match": {"goal_kinds": [allowed_goal_kind]},
+        },
+    }
+    csg_canon = canonicalize_csg_v87(csg)
+    csg_hash = csg_hash_v87(csg_canon)
+    concept_def = build_csg_concept_def_v87(csg=csg_canon, store=store)
+    concept_id = str(concept_def.get("concept_id") or "")
+    if not concept_id:
+        _fail("ERROR: missing concept_id from build_csg_concept_def_v87")
+
+    # Materialize executable concept_csv ACT with embedded CSG.
+    prog = csg_to_concept_program_v87(csg_canon)
+    concept_act = make_concept_act(
+        act_id=concept_id,
+        input_schema={"x": "str", "y": "str"},
+        output_schema={"sum": "str"},
+        validator_id="text_exact",
+        program=prog,
+        match={"goal_kinds": [allowed_goal_kind]},
+        evidence_extra={"csg_v87": {"schema_version": 1, "csg_hash": csg_hash, "csg": csg_canon}},
+    )
+    store.add(concept_act)
+
+    step = 0
+    logs.append_concept_def(step=step, concept=concept_def)
+    step += 1
+
+    engine = EngineV80(store, seed=int(seed))
+
+    inputs = {"x": "0004", "y": "0008"}
+
+    # Allowed execution (must match inline replay).
+    allowed_res = engine.execute_concept_csv(
+        concept_act_id=concept_id,
+        inputs=dict(inputs),
+        goal_kind=allowed_goal_kind,
+        expected="12",
+        step=0,
+        max_depth=6,
+        max_events=256,
+        validate_output=True,
+    )
+    allowed_meta = allowed_res.get("meta") if isinstance(allowed_res.get("meta"), dict) else {}
+    if not bool(allowed_meta.get("ok", False)):
+        _fail(f"ERROR: allowed execution failed: {allowed_meta}")
+    if str(allowed_res.get("output") or "") != "12":
+        _fail(f"ERROR: allowed output mismatch: got={allowed_res.get('output')}")
+    trace = allowed_res.get("trace") if isinstance(allowed_res.get("trace"), dict) else {}
+    trace_sig = sha256_canon(trace)
+
+    logs.append_telemetry(
+        step=step,
+        telemetry={
+            "schema_version": 1,
+            "event": "CALL",
+            "concept_id": concept_id,
+            "csg_hash": csg_hash,
+            "goal_kind": allowed_goal_kind,
+            "ok": True,
+            "reason": str(allowed_meta.get("reason") or ""),
+            "output_sig": str(allowed_meta.get("output_sig") or ""),
+            "trace_sig": str(trace_sig),
+            "call_site": "smoke_v87:allowed",
+        },
+    )
+    step += 1
+
+    # Inline replay from CSG expansion (audit).
+    steps = csg_expand_v87(csg_canon, store)
+    replay_res = _execute_inline_replay(engine=engine, steps=steps, inputs=dict(inputs), goal_kind=allowed_goal_kind)
+    if not bool(replay_res.get("ok", False)):
+        _fail(f"ERROR: inline replay failed: {replay_res}")
+    if str(replay_res.get("output") or "") != str(allowed_res.get("output") or ""):
+        _fail(f"ERROR: replay output mismatch: replay={replay_res.get('output')} concept={allowed_res.get('output')}")
+
+    logs.append_evidence(
+        step=step,
+        evidence={
+            "schema_version": 1,
+            "concept_id": concept_id,
+            "csg_hash": csg_hash,
+            "source": {
+                "kind": "smoke_v87",
+                "trace_sig": str(trace_sig),
+                "concept_calls_total": int(
+                    len((allowed_res.get("trace") or {}).get("concept_calls") or [])
+                    if isinstance(allowed_res.get("trace"), dict)
+                    else 0
+                ),
+                "window": {"start": 0, "end": int(len(steps) - 1)},
+            },
+            "verify": {"ok": True, "reason": "replay_equivalent"},
+        },
+    )
+    step += 1
+
+    # Disallowed execution (match closed, explicit).
+    disallowed_res = engine.execute_concept_csv(
+        concept_act_id=concept_id,
+        inputs=dict(inputs),
+        goal_kind=disallowed_goal_kind,
+        expected="12",
+        step=0,
+        max_depth=6,
+        max_events=256,
+        validate_output=True,
+    )
+    _assert_match_disallowed(
+        exec_res=disallowed_res,
+        want_concept_id=str(concept_id),
+        want_goal_kind=str(disallowed_goal_kind),
+    )
+    dis_meta = disallowed_res.get("meta") if isinstance(disallowed_res.get("meta"), dict) else {}
+    dis_trace = disallowed_res.get("trace") if isinstance(disallowed_res.get("trace"), dict) else {}
+    dis_trace_sig = sha256_canon(dis_trace)
+
+    logs.append_telemetry(
+        step=step,
+        telemetry={
+            "schema_version": 1,
+            "event": "CALL",
+            "concept_id": concept_id,
+            "csg_hash": csg_hash,
+            "goal_kind": disallowed_goal_kind,
+            "ok": False,
+            "reason": str(dis_meta.get("reason") or ""),
+            "trace_sig": str(dis_trace_sig),
+            "call_site": "smoke_v87:disallowed",
+            "blocked": True,
+            "blocked_reason": "match_disallowed",
+        },
+    )
+    step += 1
+
+    chains = logs.verify_chains()
+    if not (bool(chains.get("concepts_chain_ok")) and bool(chains.get("evidence_chain_ok")) and bool(chains.get("telemetry_chain_ok"))):
+        _fail(f"ERROR: hash-chain verification failed: {chains}")
+
+    eval_obj: Dict[str, Any] = {
+        "schema_version": 1,
+        "seed": int(seed),
+        "csg_hash": str(csg_hash),
+        "concept_id": str(concept_id),
+        "allowed_goal_kind": str(allowed_goal_kind),
+        "disallowed_goal_kind": str(disallowed_goal_kind),
+        "allowed_output": str(allowed_res.get("output") or ""),
+        "replay_output": str(replay_res.get("output") or ""),
+        "replay_ok": True,
+        "disallowed_reason": str(dis_meta.get("reason") or ""),
+        "trace_sig_allowed": str(trace_sig),
+        "trace_sig_disallowed": str(dis_trace_sig),
+        "chains": dict(chains),
+    }
+    eval_sha = write_json(os.path.join(out_dir, "eval.json"), eval_obj)
+
+    core = {
+        "schema_version": 1,
+        "seed": int(seed),
+        "csg_hash": str(csg_hash),
+        "concept_id": str(concept_id),
+        "allowed_output": str(eval_obj.get("allowed_output") or ""),
+        "disallowed_reason": str(eval_obj.get("disallowed_reason") or ""),
+        "replay_ok": bool(eval_obj.get("replay_ok", False)),
+        "chains": dict(chains),
+        "sha256_eval_json": str(eval_sha),
+    }
+    summary_sha256 = sha256_text(canonical_json_dumps(core))
+    smoke_summary = dict(core, summary_sha256=str(summary_sha256))
+    write_json(os.path.join(out_dir, "smoke_summary.json"), smoke_summary)
+
+    return {
+        "out_dir": str(out_dir),
+        "eval": dict(eval_obj),
+        "smoke_summary": dict(smoke_summary),
+        "summary_sha256": str(summary_sha256),
+        "sha256": {
+            "eval_json": str(eval_sha),
+            "concepts_jsonl": sha256_file(logs.concepts_path),
+            "concept_evidence_jsonl": sha256_file(logs.evidence_path),
+            "concept_telemetry_jsonl": sha256_file(logs.telemetry_path),
+            "smoke_summary_json": sha256_file(os.path.join(out_dir, "smoke_summary.json")),
+        },
+    }
+
+
+def main() -> None:
+    ap = argparse.ArgumentParser()
+    ap.add_argument("--out_base", required=True)
+    ap.add_argument("--seed", type=int, default=0)
+    args = ap.parse_args()
+
+    out_base = str(args.out_base)
+    seed = int(args.seed)
+
+    out1 = f"{out_base}_try1"
+    out2 = f"{out_base}_try2"
+    ensure_absent(out1)
+    ensure_absent(out2)
+
+    r1 = smoke_try(out_dir=out1, seed=seed)
+    r2 = smoke_try(out_dir=out2, seed=seed)
+
+    if str(r1.get("summary_sha256") or "") != str(r2.get("summary_sha256") or ""):
+        _fail(
+            f"ERROR: determinism failed: try1={r1.get('summary_sha256')} try2={r2.get('summary_sha256')}"
+        )
+
+    out = {
+        "ok": True,
+        "seed": int(seed),
+        "determinism": {"ok": True, "summary_sha256": str(r1.get("summary_sha256") or "")},
+        "try1": {"out_dir": str(out1), "sha256": dict(r1.get("sha256") or {})},
+        "try2": {"out_dir": str(out2), "sha256": dict(r2.get("sha256") or {})},
+    }
+    print(json.dumps(out, ensure_ascii=False, indent=2, sort_keys=True))
+
+
+if __name__ == "__main__":
+    main()
+
